{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28466e97-3122-4465-9c90-18e6be66d74b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import copy\n",
    "import logging\n",
    "import networkx\n",
    "from networkx import convert_matrix\n",
    "from typing import NamedTuple, Optional, Tuple, List, Dict, TypeVar, TypedDict, cast\n",
    "\n",
    "import numpy\n",
    "import gym\n",
    "from gym import spaces\n",
    "from gym.utils import seeding\n",
    "\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "from cyberbattle._env.shared_cyberbattle_env import EnvironmentBounds, AttackerGoal, DefenderGoal, DefenderConstraint\n",
    "from cyberbattle._env.defender import DefenderAgent,ScanAndReimageCompromisedMachines\n",
    "from cyberbattle.simulation.model import PortName, PrivilegeLevel\n",
    "from cyberbattle.simulation import commandcontrol, model, actions\n",
    "from cyberbattle._env.discriminatedunion import DiscriminatedUnion\n",
    "from cyberbattle.agents.baseline import agent_wrapper as w\n",
    "import cyberbattle.agents.baseline.learner as learner\n",
    "from cyberbattle.agents.baseline.learner import Learner\n",
    "import cyberbattle.agents.baseline.agent_tabularqlearning as a\n",
    "import cyberbattle.agents.baseline.agent_ddql as ddqla\n",
    "import cyberbattle.agents.baseline.agent_dql as dqla\n",
    "from cyberbattle.defender_agents.trainedDefender import *\n",
    "import random\n",
    "random.seed(120394016)\n",
    "%matplotlib inline\n",
    "import cyberbattle.agents.baseline.agent_tabularqlearning as a\n",
    "import cyberbattle.agents.baseline.learner as learner\n",
    "from cyberbattle.agents.baseline.agent_wrapper import Verbosity\n",
    "#ql\n",
    "iteration_count = 10\n",
    "training_episode_count = 10\n",
    "eval_episode_count = 20\n",
    "gamma_sweep = [\n",
    "    0.015,  # about right\n",
    "]\n",
    "import gym\n",
    "\n",
    "gym_env = gym.make('CyberBattleChain-v0',\n",
    "    size=12,\n",
    "    attacker_goal=AttackerGoal(\n",
    "        own_atleast=0,\n",
    "        own_atleast_percent=1.0\n",
    "    ),\n",
    "    defender_constraint=DefenderConstraint(\n",
    "        maintain_sla=0.80\n",
    "    )\n",
    ")\n",
    "\n",
    "defender_name = \"DefenderName\"\n",
    "defender_agent = DefenderWrapper(gym_env,defender_name)\n",
    "from cyberbattle.agents.baseline.agent_wrapper import ActionTrackingStateAugmentation, StateAugmentation\n",
    "from cyberbattle.agents.baseline.plotting import PlotTraining, plot_averaged_cummulative_rewards\n",
    "import progressbar\n",
    "import math\n",
    "from cyberbattle.agents.baseline import learner\n",
    "import sys\n",
    "from cyberbattle.simulation.model import FirewallRule\n",
    "ep = w.EnvironmentBounds.of_identifiers(\n",
    "    maximum_total_credentials=22,\n",
    "    maximum_node_count=22,\n",
    "    identifiers=gym_env.identifiers\n",
    ")\n",
    "#cyberbattlechain_defender.render_as_fig()\n",
    "import cyberbattle.agents.baseline.agent_tabularqlearning as a\n",
    "import cyberbattle.agents.baseline.learner as learner\n",
    "from cyberbattle.agents.baseline.agent_wrapper import Verbosity\n",
    "epsilon = 0.66666666666666\n",
    "gamma= 0.06925\n",
    "learning_rate= 0.34444444444444444\n",
    "epsilon_exponential_decay=3198.830407105263\n",
    "epsilon_minimum=0.05708811937931034\n",
    "episode_count=10\n",
    "iteration_count=10\n",
    "epsilon_multdecay=0.7777777777777778\n",
    "statespace = w.HashEncoding(ep, [\n",
    "            # Feature_discovered_node_count(),\n",
    "            # Feature_discovered_credential_count(),\n",
    "            w.Feature_discovered_ports_sliding(ep),\n",
    "            w.Feature_discovered_nodeproperties_sliding(ep),\n",
    "            w.Feature_discovered_notowned_node_count(ep, 3)\n",
    "        ], 5000)  # should not be too small, pick something big to avoid collision\n",
    "\n",
    "actionspace = w.RavelEncoding(ep, [\n",
    "            w.Feature_active_node_properties(ep)])\n",
    "print(statespace)\n",
    "print(actionspace)\n",
    "q_matrix = DefenderQMatrix(statespace, actionspace,gym_env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "576454a6-d366-4c96-ab4a-134d5ed2ecb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class SimulationTracker:\n",
    "    def __init__(self):\n",
    "        self.attacker_rewards = []\n",
    "        self.defender_rewards = []\n",
    "        self.actions_taken = {'attacker': [], 'defender': []}\n",
    "        self.exploration_actions = {'attacker': 0, 'defender': 0}\n",
    "        self.exploitation_actions = {'attacker': 0, 'defender': 0}\n",
    "        self.episode_lengths = []\n",
    "        self.reward_variability = []\n",
    "        self.state_visitation_frequencies = {}\n",
    "        self.temporal_differences = []\n",
    "        self.infected_nodes = []\n",
    "\n",
    "    def log_episode(self, attacker_reward, defender_reward, episode_length, episode_rewards):\n",
    "        self.attacker_rewards.append(attacker_reward)\n",
    "        self.defender_rewards.append(defender_reward)\n",
    "        self.episode_lengths.append(episode_length)\n",
    "        self.reward_variability.append(np.var(episode_rewards))\n",
    "        #self.temporal_differences.append(temporal_difference)\n",
    "\n",
    "    def log_action(self, role, action, exploration):\n",
    "        action_str = str(action)  # Convert the action dictionary to a string\n",
    "        self.actions_taken[role].append(action_str)\n",
    "        if exploration:\n",
    "            self.exploration_actions[role] += 1\n",
    "        else:\n",
    "            self.exploitation_actions[role] += 1\n",
    "\n",
    "\n",
    "    def log_state_visitation(self, state):\n",
    "        if state not in self.state_visitation_frequencies:\n",
    "            self.state_visitation_frequencies[state] = 1\n",
    "        else:\n",
    "            self.state_visitation_frequencies[state] += 1\n",
    "\n",
    "    def log_infected_nodes(self, nodes_count):\n",
    "        self.infected_nodes.append(nodes_count)\n",
    "\n",
    "    def print_summary(self):\n",
    "        # Summary statistics\n",
    "        print(\"Simulation Summary:\")\n",
    "        print(f\"Average Attacker Reward: {np.mean(self.attacker_rewards)}\")\n",
    "        print(f\"Average Defender Reward: {np.mean(self.defender_rewards)}\")\n",
    "        print(f\"Average Episode Length: {np.mean(self.episode_lengths)} steps\")\n",
    "        print(f\"Average Reward Variability: {np.mean(self.reward_variability)}\")\n",
    "        print(f\"Average Temporal Difference: {np.mean(self.temporal_differences)}\")\n",
    "        print(f\"Average Infected Nodes per Episode: {np.mean(self.infected_nodes)}\")\n",
    "\n",
    "        # Exploration vs. Exploitation\n",
    "        print(\"\\nExploration vs. Exploitation:\")\n",
    "        for role in ['attacker', 'defender']:\n",
    "            total_actions = self.exploration_actions[role] + self.exploitation_actions[role]\n",
    "            if total_actions > 0:\n",
    "                print(f\"{role.capitalize()} Exploration: {self.exploration_actions[role] / total_actions:.2f}\")\n",
    "                print(f\"{role.capitalize()} Exploitation: {self.exploitation_actions[role] / total_actions:.2f}\")\n",
    "\n",
    "        # Action Distributions\n",
    "        self._print_action_distributions()\n",
    "\n",
    "        # Visualization\n",
    "        self._plot_rewards()\n",
    "\n",
    "    def _print_action_distributions(self):\n",
    "        print(\"\\nAction Distributions:\")\n",
    "        for role, actions in self.actions_taken.items():\n",
    "            # actions are already converted to strings by log_action\n",
    "            unique_actions, counts = np.unique(actions, return_counts=True)\n",
    "            print(f\"{role.capitalize()} Actions:\")\n",
    "            for action, count in zip(unique_actions, counts):\n",
    "                print(f\"Action {action}: {count} times\")\n",
    "\n",
    "\n",
    "    def _plot_rewards(self):\n",
    "        plt.figure(figsize=(10, 5))\n",
    "        plt.plot(self.attacker_rewards, label='Attacker Rewards')\n",
    "        plt.plot(self.defender_rewards, label='Defender Rewards')\n",
    "        plt.xlabel('Episode')\n",
    "        plt.ylabel('Reward')\n",
    "        plt.title('Rewards per Episode for Attacker and Defender')\n",
    "        plt.legend()\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13f3ffee-4357-4408-bea9-591a80c8e42a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import copy\n",
    "import logging\n",
    "import networkx\n",
    "from networkx import convert_matrix\n",
    "from typing import NamedTuple, Optional, Tuple, List, Dict, TypeVar, TypedDict, cast\n",
    "\n",
    "import numpy\n",
    "import gym\n",
    "from gym import spaces\n",
    "from gym.utils import seeding\n",
    "\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "from cyberbattle._env.shared_cyberbattle_env import EnvironmentBounds, AttackerGoal, DefenderGoal, DefenderConstraint\n",
    "from cyberbattle._env.defender import DefenderAgent,ScanAndReimageCompromisedMachines\n",
    "from cyberbattle.simulation.model import PortName, PrivilegeLevel\n",
    "from cyberbattle.simulation import commandcontrol, model, actions\n",
    "from cyberbattle._env.discriminatedunion import DiscriminatedUnion\n",
    "from cyberbattle.agents.baseline import agent_wrapper as w\n",
    "import cyberbattle.agents.baseline.learner as learner\n",
    "from cyberbattle.agents.baseline.learner import Learner\n",
    "import cyberbattle.agents.baseline.agent_tabularqlearning as a\n",
    "import cyberbattle.agents.baseline.agent_ddql as ddqla\n",
    "import cyberbattle.agents.baseline.agent_dql as dqla\n",
    "from cyberbattle.defender_agents.trainedDefender import *\n",
    "import random\n",
    "random.seed(120394016)\n",
    "%matplotlib inline\n",
    "import cyberbattle.agents.baseline.agent_tabularqlearning as a\n",
    "import cyberbattle.agents.baseline.learner as learner\n",
    "from cyberbattle.agents.baseline.agent_wrapper import Verbosity\n",
    "#ql\n",
    "iteration_count = 10\n",
    "training_episode_count = 10\n",
    "eval_episode_count = 20\n",
    "gamma_sweep = [\n",
    "    0.015,  # about right\n",
    "]\n",
    "import gym\n",
    "\n",
    "gym_env = gym.make('CyberBattleChain-v0',\n",
    "    size=12,\n",
    "    attacker_goal=AttackerGoal(\n",
    "        own_atleast=0,\n",
    "        own_atleast_percent=1.0\n",
    "    ),\n",
    "    defender_constraint=DefenderConstraint(\n",
    "        maintain_sla=0.80\n",
    "    )\n",
    ")\n",
    "\n",
    "defender_name = \"DefenderName\"\n",
    "defender_agent = DefenderWrapper(gym_env,defender_name)\n",
    "from cyberbattle.agents.baseline.agent_wrapper import ActionTrackingStateAugmentation, StateAugmentation\n",
    "from cyberbattle.agents.baseline.plotting import PlotTraining, plot_averaged_cummulative_rewards\n",
    "import progressbar\n",
    "import math\n",
    "from cyberbattle.agents.baseline import learner\n",
    "import sys\n",
    "from cyberbattle.simulation.model import FirewallRule\n",
    "ep = w.EnvironmentBounds.of_identifiers(\n",
    "    maximum_total_credentials=22,\n",
    "    maximum_node_count=22,\n",
    "    identifiers=gym_env.identifiers\n",
    ")\n",
    "#cyberbattlechain_defender.render_as_fig()\n",
    "import cyberbattle.agents.baseline.agent_tabularqlearning as a\n",
    "import cyberbattle.agents.baseline.learner as learner\n",
    "from cyberbattle.agents.baseline.agent_wrapper import Verbosity\n",
    "epsilon = 0.66666666666666\n",
    "gamma= 0.06925\n",
    "learning_rate= 0.34444444444444444\n",
    "epsilon_exponential_decay=3198.830407105263\n",
    "epsilon_minimum=0.05708811937931034\n",
    "episode_count=10\n",
    "iteration_count=10\n",
    "epsilon_multdecay=0.7777777777777778\n",
    "statespace = w.HashEncoding(ep, [\n",
    "            # Feature_discovered_node_count(),\n",
    "            # Feature_discovered_credential_count(),\n",
    "            w.Feature_discovered_ports_sliding(ep),\n",
    "            w.Feature_discovered_nodeproperties_sliding(ep),\n",
    "            w.Feature_discovered_notowned_node_count(ep, 3)\n",
    "        ], 5000)  # should not be too small, pick something big to avoid collision\n",
    "\n",
    "actionspace = w.RavelEncoding(ep, [\n",
    "            w.Feature_active_node_properties(ep)])\n",
    "print(statespace)\n",
    "print(actionspace)\n",
    "q_matrix = DefenderQMatrix(statespace, actionspace,gym_env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c361def8-e18d-4726-be8f-e5e0c402ecb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(120394016)\n",
    "# Parameter ranges\n",
    "epsilon = 0.9629629629629622\n",
    "#gamma=np.linspace(0.05,0.09,10)\n",
    "gamma= 0.05888888888888889\n",
    "#learning_rate=np.linspace(0.3,.35,10)\n",
    "learning_rate=0.3222222222222222\n",
    "#epsilon_decay=np.linspace(2333.33333,5000,20)\n",
    "epsilon_exponential_decay=5000\n",
    "#epsilon_minimum=np.linspace(0.01,.07777777,10)\n",
    "epsilon_minimum=0.0513409942068965\n",
    "episode_count=50\n",
    "iteration_count=200\n",
    "#epsilon_multdecay=np.linspace(0.3,0.875,20)\n",
    "epsilon_multdecay=0.6631578947368421\n",
    "\n",
    "trained_attacker = learner.epsilon_greedy_search(\n",
    "        gym_env,\n",
    "        ep,\n",
    "        a.QTabularLearner(ep, gamma=0.05888888888888889, learning_rate=0.3222222222222222, exploit_percentile=100),\n",
    "        episode_count=episode_count,\n",
    "        iteration_count=iteration_count,\n",
    "        epsilon=epsilon,\n",
    "        render=True,\n",
    "        epsilon_multdecay=epsilon_multdecay,  # 0.999,\n",
    "        epsilon_exponential_decay=epsilon_exponential_decay,\n",
    "        epsilon_minimum=epsilon_minimum,\n",
    "        verbosity=Verbosity.Quiet,\n",
    "        title=\"Q-learning\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12a50aaf-78bf-4578-900c-e8b954fed2bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_training_with_hyperparameters(gym_env, environment_properties, statespace, actionspace, \n",
    "                                      epsilon, gamma, learning_rate, epsilon_exponential_decay, epsilon_minimum, episode_count, iteration_count, epsilon_multdecay):\n",
    "    q_matrix = DefenderQMatrix(statespace, actionspace, gym_env)\n",
    "    learner = DefenderQLearner(gym_env, defender_agent, environment_properties, q_matrix,\n",
    "                               epsilon=epsilon, gamma=gamma, learning_rate=learning_rate)\n",
    "    \n",
    "    trained_defender = epsilon_greedy_defender_training(\n",
    "        cyberbattle_gym_env=gym_env,\n",
    "        environment_properties=environment_properties,\n",
    "        learner=learner,\n",
    "        episode_count=episode_count,  # Set the number of episodes for the training\n",
    "        iteration_count=iteration_count,  # Set the number of iterations per episode\n",
    "        epsilon=epsilon,\n",
    "        epsilon_exponential_decay=epsilon_exponential_decay,\n",
    "        epsilon_minimum=epsilon_minimum,\n",
    "        epsilon_multdecay=epsilon_multdecay,\n",
    "        verbosity=logging.INFO,\n",
    "        render=True,\n",
    "        plot_episodes_length=True,\n",
    "        title=\"Defender Q-Learning Training\"\n",
    "    )\n",
    "    total_reward = sum([sum(rewards) for rewards in trained_defender['all_episodes_rewards']])\n",
    "    return total_reward"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c223253-02af-4a82-b376-c4891d657835",
   "metadata": {},
   "source": [
    "# Epsilon = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce9afdef-2a3c-450a-bdfc-315a6190c5af",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import cyberbattle.agents.baseline.agent_tabularqlearning as a\n",
    "import cyberbattle.agents.baseline.learner as learner\n",
    "from cyberbattle.agents.baseline.agent_wrapper import Verbosity\n",
    "epsilon = 1\n",
    "gamma= 0.06925\n",
    "learning_rate= 0.34444444444444444\n",
    "epsilon_exponential_decay=3198.830407105263\n",
    "epsilon_minimum=0.05708811937931034\n",
    "episode_count=50\n",
    "iteration_count=200\n",
    "epsilon_multdecay=0.7777777777777778\n",
    "statespace = w.HashEncoding(ep, [\n",
    "            # Feature_discovered_node_count(),\n",
    "            # Feature_discovered_credential_count(),\n",
    "            w.Feature_discovered_ports_sliding(ep),\n",
    "            w.Feature_discovered_nodeproperties_sliding(ep),\n",
    "            w.Feature_discovered_notowned_node_count(ep, 3)\n",
    "        ], 5000)  # should not be too small, pick something big to avoid collision\n",
    "\n",
    "actionspace = w.RavelEncoding(ep, [\n",
    "            w.Feature_active_node_properties(ep)])\n",
    "print(statespace)\n",
    "print(actionspace)\n",
    "q_matrix = DefenderQMatrix(statespace, actionspace,gym_env)\n",
    "learner2 = DefenderQLearner(gym_env, defender_agent, ep, q_matrix, epsilon=0.9, gamma=0.015, learning_rate=.07)\n",
    "random.seed(120394016)\n",
    "#%matplotlib inline\n",
    "\n",
    "trained_defender = epsilon_greedy_defender_training(\n",
    "        cyberbattle_gym_env=gym_env,\n",
    "        environment_properties=ep,\n",
    "        learner=learner2,\n",
    "        episode_count=episode_count,  # Set the number of episodes for the training\n",
    "        iteration_count=iteration_count,  # Set the number of iterations per episode\n",
    "        epsilon=epsilon,\n",
    "        epsilon_exponential_decay=epsilon_exponential_decay,\n",
    "        epsilon_minimum=epsilon_minimum,\n",
    "        epsilon_multdecay=epsilon_multdecay,\n",
    "        verbosity=logging.INFO,\n",
    "        render=True,\n",
    "        plot_episodes_length=True,\n",
    "        title=\"Defender Q-Learning Training\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e46a0f0-4a5c-4f83-b81a-20220db5b031",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "\n",
    "gym_env = gym.make('CyberBattleChain-v0',\n",
    "    size=12,\n",
    "    attacker_goal=AttackerGoal(\n",
    "        own_atleast=0,\n",
    "        own_atleast_percent=1.0\n",
    "    ),\n",
    "    defender_constraint=DefenderConstraint(\n",
    "        maintain_sla=0.80\n",
    "    )\n",
    ")\n",
    "\n",
    "defender_name = \"DefenderName\" \n",
    "defender_agent = DefenderWrapper(gym_env,defender_name)\n",
    "\n",
    "gym_environment = gym.make('CyberBattleChain-v0',\n",
    "    size=12,\n",
    "    attacker_goal=AttackerGoal(\n",
    "        own_atleast=0,\n",
    "        own_atleast_percent=1.0\n",
    "    ),\n",
    "    defender_constraint=DefenderConstraint(\n",
    "        maintain_sla=0.80\n",
    "    ),\n",
    "    defender_agent=defender_agent\n",
    ")\n",
    "\n",
    "#this was made due to issues where the attacker q learner needed to use a wrapper environment\n",
    "from cyberbattle.agents.baseline.agent_wrapper import AgentWrapper, StateAugmentation\n",
    "def wrap_environment(env):\n",
    "    initial_observation = env.reset()  \n",
    "    state = StateAugmentation(initial_observation)  \n",
    "    return AgentWrapper(env, state)\n",
    "\n",
    "attacker_learner = trained_attacker['learner']\n",
    "defender_learner = trained_defender['learner']\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Simulation Parameters\n",
    "number_of_episodes = 50\n",
    "max_steps_per_episode = 200\n",
    "attacker_epsilon = 0.1\n",
    "defender_epsilon = 0.1\n",
    "\n",
    "# Reward Tracking\n",
    "attacker_rewards_per_episode = []\n",
    "defender_rewards_per_episode = []\n",
    "win_rates = {'attacker': [], 'defender': []}\n",
    "actions_taken_count = {'attacker': [], 'defender': []}\n",
    "exploration_vs_exploitation = {'exploration': [], 'exploitation': []}\n",
    "episode_lengths = []\n",
    "state_visitation_frequencies = {}  \n",
    "reward_variability = []\n",
    "defender_successful_actions = []\n",
    "defender_failed_actions = []\n",
    "defender_actions_taken = []\n",
    "tracker = SimulationTracker() #instantiate simulation tracker\n",
    "wrapped_env = wrap_environment(gym_environment)\n",
    "win_counts = {'attacker': 0, 'defender': 0}\n",
    "# Main Simulation Loop\n",
    "random.seed(120394016)\n",
    "for episode in range(number_of_episodes):\n",
    "    observation = wrapped_env.reset()\n",
    "    attacker_total_reward = 0\n",
    "    defender_total_reward = 0\n",
    "    steps_this_episode=0\n",
    "    episode_rewards = []  # Track rewards for this episode\n",
    "    for step in range(max_steps_per_episode):\n",
    "        steps_this_episode +=1\n",
    "        # Attacker's Turn\n",
    "        exploration = np.random.random() < attacker_epsilon\n",
    "        if exploration:\n",
    "            _, attacker_action, _ = attacker_learner.explore(wrapped_env)\n",
    "        else:\n",
    "            _, attacker_action, _ = attacker_learner.exploit(wrapped_env, observation)\n",
    "        #print(attacker_action)\n",
    "        if attacker_action is not None:\n",
    "            observation, attacker_reward, done, _ = gym_environment.step(attacker_action)\n",
    "            attacker_total_reward += attacker_reward\n",
    "            print(_)\n",
    "            actions_taken_count['attacker'].append(attacker_action)  # Track action\n",
    "            episode_rewards.append(attacker_reward)  # Track reward variability\n",
    "            tracker.log_action('attacker', attacker_action, exploration)\n",
    "            #actions_taken_count['attacker'][attacker_action] = actions_taken_count['attacker'].get(attacker_action, 0) + 1\n",
    "\n",
    "\n",
    "        # Defender's Turn\n",
    "        exploration = np.random.random() < defender_epsilon\n",
    "        if exploration:\n",
    "            _, defender_action, _ = defender_learner.explore(wrapped_env)\n",
    "        else:\n",
    "            _, defender_action, _ = defender_learner.exploit(wrapped_env, observation)\n",
    "\n",
    "        if defender_action is not None:\n",
    "            observation, defender_reward, done, _ = gym_environment.step(defender_action)\n",
    "            #action= _['defender_info']['action']\n",
    "            defender_total_reward += defender_reward\n",
    "            actions_taken_count['defender'].append(defender_action)  # Track action\n",
    "            episode_rewards.append(defender_reward)  # Track reward variability\n",
    "            tracker.log_action('defender', defender_action, exploration)\n",
    "            #actions_taken_count['defender'][defender_action] = actions_taken_count['defender'].get(defender_action, 0) + 1\n",
    "        #logging\n",
    "        # After each episode\n",
    "        tracker.log_episode(attacker_total_reward, defender_total_reward, steps_this_episode, episode_rewards)\n",
    "        tracker.print_summary()\n",
    "        tracker._plot_rewards()\n",
    "\n",
    "        if done:\n",
    "            break\n",
    "            \n",
    "    defender_info = _['defender_info']  \n",
    "    if defender_info:  \n",
    "        defender_actions_taken.append(defender_info['performance_metrics']['actions_taken'])\n",
    "        defender_successful_actions.append(defender_info['performance_metrics']['successful_actions'])\n",
    "        defender_failed_actions.append(defender_info['performance_metrics']['failed_actions'])\n",
    "\n",
    "    attacker_rewards_per_episode.append(attacker_total_reward)\n",
    "    defender_rewards_per_episode.append(defender_total_reward)\n",
    "    episode_lengths.append(steps_this_episode)\n",
    "    reward_variability.append(np.var(episode_rewards))\n",
    "    print(f\"Episode {episode + 1}: Attacker Reward = {attacker_total_reward}, Defender Reward = {defender_total_reward}\")\n",
    "\n",
    "wrapped_env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "081322d8-d6ea-4250-8119-d5e5ec9a5e3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "epsilon1_defender_reward_per_episode = defender_rewards_per_episode\n",
    "epsilon1_attacker_reward_per_episode = attacker_rewards_per_episode\n",
    "epsilon1sa = defender_successful_actions \n",
    "epsilon1fa = defender_failed_actions \n",
    "epsilon1dat = defender_actions_taken "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12f71fca-bd66-43ef-abc8-f0e944531b36",
   "metadata": {},
   "source": [
    "# Epsilon = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f13725ba-8000-4240-8de7-a49f2f00ef66",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cyberbattle.agents.baseline.agent_tabularqlearning as a\n",
    "import cyberbattle.agents.baseline.learner as learner\n",
    "from cyberbattle.agents.baseline.agent_wrapper import Verbosity\n",
    "epsilon = 0.5\n",
    "gamma= 0.06925\n",
    "learning_rate= 0.34444444444444444\n",
    "epsilon_exponential_decay=3198.830407105263\n",
    "epsilon_minimum=0.05708811937931034\n",
    "episode_count=50\n",
    "iteration_count=200\n",
    "epsilon_multdecay=0.7777777777777778\n",
    "statespace = w.HashEncoding(ep, [\n",
    "            # Feature_discovered_node_count(),\n",
    "            # Feature_discovered_credential_count(),\n",
    "            w.Feature_discovered_ports_sliding(ep),\n",
    "            w.Feature_discovered_nodeproperties_sliding(ep),\n",
    "            w.Feature_discovered_notowned_node_count(ep, 3)\n",
    "        ], 5000)  # should not be too small, pick something big to avoid collision\n",
    "\n",
    "actionspace = w.RavelEncoding(ep, [\n",
    "            w.Feature_active_node_properties(ep)])\n",
    "print(statespace)\n",
    "print(actionspace)\n",
    "q_matrix = DefenderQMatrix(statespace, actionspace,gym_env)\n",
    "learner2 = DefenderQLearner(gym_env, defender_agent, ep, q_matrix, epsilon=0.9, gamma=0.015, learning_rate=.07)\n",
    "random.seed(120394016)\n",
    "#%matplotlib inline\n",
    "\n",
    "trained_defender = epsilon_greedy_defender_training(\n",
    "        cyberbattle_gym_env=gym_env,\n",
    "        environment_properties=ep,\n",
    "        learner=learner2,\n",
    "        episode_count=episode_count,  # Set the number of episodes for the training\n",
    "        iteration_count=iteration_count,  # Set the number of iterations per episode\n",
    "        epsilon=epsilon,\n",
    "        epsilon_exponential_decay=epsilon_exponential_decay,\n",
    "        epsilon_minimum=epsilon_minimum,\n",
    "        epsilon_multdecay=epsilon_multdecay,\n",
    "        verbosity=logging.INFO,\n",
    "        render=True,\n",
    "        plot_episodes_length=True,\n",
    "        title=\"Defender Q-Learning Training\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8e4ccc5-14ca-412d-818f-6051ceea69fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "\n",
    "gym_env = gym.make('CyberBattleChain-v0',\n",
    "    size=12,\n",
    "    attacker_goal=AttackerGoal(\n",
    "        own_atleast=0,\n",
    "        own_atleast_percent=1.0\n",
    "    ),\n",
    "    defender_constraint=DefenderConstraint(\n",
    "        maintain_sla=0.80\n",
    "    )\n",
    ")\n",
    "\n",
    "defender_name = \"DefenderName\" \n",
    "defender_agent = DefenderWrapper(gym_env,defender_name)\n",
    "\n",
    "gym_environment = gym.make('CyberBattleChain-v0',\n",
    "    size=12,\n",
    "    attacker_goal=AttackerGoal(\n",
    "        own_atleast=0,\n",
    "        own_atleast_percent=1.0\n",
    "    ),\n",
    "    defender_constraint=DefenderConstraint(\n",
    "        maintain_sla=0.80\n",
    "    ),\n",
    "    defender_agent=defender_agent\n",
    ")\n",
    "\n",
    "#this was made due to issues where the attacker q learner needed to use a wrapper environment\n",
    "from cyberbattle.agents.baseline.agent_wrapper import AgentWrapper, StateAugmentation\n",
    "def wrap_environment(env):\n",
    "    initial_observation = env.reset()  \n",
    "    state = StateAugmentation(initial_observation)  \n",
    "    return AgentWrapper(env, state)\n",
    "\n",
    "attacker_learner = trained_attacker['learner']\n",
    "defender_learner = trained_defender['learner']\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Simulation Parameters\n",
    "number_of_episodes = 50\n",
    "max_steps_per_episode = 200\n",
    "attacker_epsilon = 0.1\n",
    "defender_epsilon = 0.1\n",
    "\n",
    "# Reward Tracking\n",
    "attacker_rewards_per_episode = []\n",
    "defender_rewards_per_episode = []\n",
    "win_rates = {'attacker': [], 'defender': []}\n",
    "actions_taken_count = {'attacker': [], 'defender': []}\n",
    "exploration_vs_exploitation = {'exploration': [], 'exploitation': []}\n",
    "episode_lengths = []\n",
    "state_visitation_frequencies = {}  \n",
    "reward_variability = []\n",
    "defender_successful_actions = []\n",
    "defender_failed_actions = []\n",
    "defender_actions_taken = []\n",
    "tracker = SimulationTracker() #instantiate simulation tracker\n",
    "wrapped_env = wrap_environment(gym_environment)\n",
    "win_counts = {'attacker': 0, 'defender': 0}\n",
    "# Main Simulation Loop\n",
    "random.seed(120394016)\n",
    "for episode in range(number_of_episodes):\n",
    "    observation = wrapped_env.reset()\n",
    "    attacker_total_reward = 0\n",
    "    defender_total_reward = 0\n",
    "    steps_this_episode=0\n",
    "    episode_rewards = []  # Track rewards for this episode\n",
    "    for step in range(max_steps_per_episode):\n",
    "        steps_this_episode +=1\n",
    "        # Attacker's Turn\n",
    "        exploration = np.random.random() < attacker_epsilon\n",
    "        if exploration:\n",
    "            _, attacker_action, _ = attacker_learner.explore(wrapped_env)\n",
    "        else:\n",
    "            _, attacker_action, _ = attacker_learner.exploit(wrapped_env, observation)\n",
    "        #print(attacker_action)\n",
    "        if attacker_action is not None:\n",
    "            observation, attacker_reward, done, _ = gym_environment.step(attacker_action)\n",
    "            attacker_total_reward += attacker_reward\n",
    "            print(_)\n",
    "            actions_taken_count['attacker'].append(attacker_action)  # Track action\n",
    "            episode_rewards.append(attacker_reward)  # Track reward variability\n",
    "            tracker.log_action('attacker', attacker_action, exploration)\n",
    "            #actions_taken_count['attacker'][attacker_action] = actions_taken_count['attacker'].get(attacker_action, 0) + 1\n",
    "\n",
    "\n",
    "        # Defender's Turn\n",
    "        exploration = np.random.random() < defender_epsilon\n",
    "        if exploration:\n",
    "            _, defender_action, _ = defender_learner.explore(wrapped_env)\n",
    "        else:\n",
    "            _, defender_action, _ = defender_learner.exploit(wrapped_env, observation)\n",
    "\n",
    "        if defender_action is not None:\n",
    "            observation, defender_reward, done, _ = gym_environment.step(defender_action)\n",
    "            #action= _['defender_info']['action']\n",
    "            defender_total_reward += defender_reward\n",
    "            actions_taken_count['defender'].append(defender_action)  # Track action\n",
    "            episode_rewards.append(defender_reward)  # Track reward variability\n",
    "            tracker.log_action('defender', defender_action, exploration)\n",
    "            #actions_taken_count['defender'][defender_action] = actions_taken_count['defender'].get(defender_action, 0) + 1\n",
    "        #logging\n",
    "        # After each episode\n",
    "        tracker.log_episode(attacker_total_reward, defender_total_reward, steps_this_episode, episode_rewards)\n",
    "        tracker.print_summary()\n",
    "        tracker._plot_rewards()\n",
    "\n",
    "        if done:\n",
    "            break\n",
    "            \n",
    "    defender_info = _['defender_info']  \n",
    "    if defender_info:  \n",
    "        defender_actions_taken.append(defender_info['performance_metrics']['actions_taken'])\n",
    "        defender_successful_actions.append(defender_info['performance_metrics']['successful_actions'])\n",
    "        defender_failed_actions.append(defender_info['performance_metrics']['failed_actions'])\n",
    "\n",
    "    attacker_rewards_per_episode.append(attacker_total_reward)\n",
    "    defender_rewards_per_episode.append(defender_total_reward)\n",
    "    episode_lengths.append(steps_this_episode)\n",
    "    reward_variability.append(np.var(episode_rewards))\n",
    "    print(f\"Episode {episode + 1}: Attacker Reward = {attacker_total_reward}, Defender Reward = {defender_total_reward}\")\n",
    "\n",
    "wrapped_env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6391f08c-3b59-4993-8a49-90966efa404e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "epsilon1half_defender_reward_per_episode = defender_rewards_per_episode\n",
    "epsilon1half_attacker_reward_per_episode = attacker_rewards_per_episode\n",
    "epsilonhalfsa = defender_successful_actions \n",
    "epsilonhalffa = defender_failed_actions \n",
    "epsilonhalfdat = defender_actions_taken "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64d3d469-df9d-481f-8978-29561f2e014e",
   "metadata": {},
   "source": [
    "# Epsilon = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4457b821-53e8-4d56-9058-3953dbd2d3bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cyberbattle.agents.baseline.agent_tabularqlearning as a\n",
    "import cyberbattle.agents.baseline.learner as learner\n",
    "from cyberbattle.agents.baseline.agent_wrapper import Verbosity\n",
    "epsilon = 0.1\n",
    "gamma= 0.06925\n",
    "learning_rate= 0.34444444444444444\n",
    "epsilon_exponential_decay=3198.830407105263\n",
    "epsilon_minimum=0.05708811937931034\n",
    "episode_count=50\n",
    "iteration_count=200\n",
    "epsilon_multdecay=0.7777777777777778\n",
    "statespace = w.HashEncoding(ep, [\n",
    "            # Feature_discovered_node_count(),\n",
    "            # Feature_discovered_credential_count(),\n",
    "            w.Feature_discovered_ports_sliding(ep),\n",
    "            w.Feature_discovered_nodeproperties_sliding(ep),\n",
    "            w.Feature_discovered_notowned_node_count(ep, 3)\n",
    "        ], 5000)  # should not be too small, pick something big to avoid collision\n",
    "\n",
    "actionspace = w.RavelEncoding(ep, [\n",
    "            w.Feature_active_node_properties(ep)])\n",
    "print(statespace)\n",
    "print(actionspace)\n",
    "q_matrix = DefenderQMatrix(statespace, actionspace,gym_env)\n",
    "learner2 = DefenderQLearner(gym_env, defender_agent, ep, q_matrix, epsilon=0.9, gamma=0.015, learning_rate=.07)\n",
    "random.seed(120394016)\n",
    "#%matplotlib inline\n",
    "\n",
    "trained_defender = epsilon_greedy_defender_training(\n",
    "        cyberbattle_gym_env=gym_env,\n",
    "        environment_properties=ep,\n",
    "        learner=learner2,\n",
    "        episode_count=episode_count,  # Set the number of episodes for the training\n",
    "        iteration_count=iteration_count,  # Set the number of iterations per episode\n",
    "        epsilon=epsilon,\n",
    "        epsilon_exponential_decay=epsilon_exponential_decay,\n",
    "        epsilon_minimum=epsilon_minimum,\n",
    "        epsilon_multdecay=epsilon_multdecay,\n",
    "        verbosity=logging.INFO,\n",
    "        render=True,\n",
    "        plot_episodes_length=True,\n",
    "        title=\"Defender Q-Learning Training\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b67c23f-3f33-4328-bd6f-949caf7bd069",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "\n",
    "gym_env = gym.make('CyberBattleChain-v0',\n",
    "    size=12,\n",
    "    attacker_goal=AttackerGoal(\n",
    "        own_atleast=0,\n",
    "        own_atleast_percent=1.0\n",
    "    ),\n",
    "    defender_constraint=DefenderConstraint(\n",
    "        maintain_sla=0.80\n",
    "    )\n",
    ")\n",
    "\n",
    "defender_name = \"DefenderName\" \n",
    "defender_agent = DefenderWrapper(gym_env,defender_name)\n",
    "\n",
    "gym_environment = gym.make('CyberBattleChain-v0',\n",
    "    size=12,\n",
    "    attacker_goal=AttackerGoal(\n",
    "        own_atleast=0,\n",
    "        own_atleast_percent=1.0\n",
    "    ),\n",
    "    defender_constraint=DefenderConstraint(\n",
    "        maintain_sla=0.80\n",
    "    ),\n",
    "    defender_agent=defender_agent\n",
    ")\n",
    "\n",
    "#this was made due to issues where the attacker q learner needed to use a wrapper environment\n",
    "from cyberbattle.agents.baseline.agent_wrapper import AgentWrapper, StateAugmentation\n",
    "def wrap_environment(env):\n",
    "    initial_observation = env.reset()  \n",
    "    state = StateAugmentation(initial_observation)  \n",
    "    return AgentWrapper(env, state)\n",
    "\n",
    "attacker_learner = trained_attacker['learner']\n",
    "defender_learner = trained_defender['learner']\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Simulation Parameters\n",
    "number_of_episodes = 50\n",
    "max_steps_per_episode = 200\n",
    "attacker_epsilon = 0.1\n",
    "defender_epsilon = 0.1\n",
    "\n",
    "# Reward Tracking\n",
    "attacker_rewards_per_episode = []\n",
    "defender_rewards_per_episode = []\n",
    "win_rates = {'attacker': [], 'defender': []}\n",
    "actions_taken_count = {'attacker': [], 'defender': []}\n",
    "exploration_vs_exploitation = {'exploration': [], 'exploitation': []}\n",
    "episode_lengths = []\n",
    "state_visitation_frequencies = {}  \n",
    "reward_variability = []\n",
    "defender_successful_actions = []\n",
    "defender_failed_actions = []\n",
    "defender_actions_taken = []\n",
    "tracker = SimulationTracker() #instantiate simulation tracker\n",
    "wrapped_env = wrap_environment(gym_environment)\n",
    "win_counts = {'attacker': 0, 'defender': 0}\n",
    "# Main Simulation Loop\n",
    "random.seed(120394016)\n",
    "for episode in range(number_of_episodes):\n",
    "    observation = wrapped_env.reset()\n",
    "    attacker_total_reward = 0\n",
    "    defender_total_reward = 0\n",
    "    steps_this_episode=0\n",
    "    episode_rewards = []  # Track rewards for this episode\n",
    "    for step in range(max_steps_per_episode):\n",
    "        steps_this_episode +=1\n",
    "        # Attacker's Turn\n",
    "        exploration = np.random.random() < attacker_epsilon\n",
    "        if exploration:\n",
    "            _, attacker_action, _ = attacker_learner.explore(wrapped_env)\n",
    "        else:\n",
    "            _, attacker_action, _ = attacker_learner.exploit(wrapped_env, observation)\n",
    "        #print(attacker_action)\n",
    "        if attacker_action is not None:\n",
    "            observation, attacker_reward, done, _ = gym_environment.step(attacker_action)\n",
    "            attacker_total_reward += attacker_reward\n",
    "            print(_)\n",
    "            actions_taken_count['attacker'].append(attacker_action)  # Track action\n",
    "            episode_rewards.append(attacker_reward)  # Track reward variability\n",
    "            tracker.log_action('attacker', attacker_action, exploration)\n",
    "            #actions_taken_count['attacker'][attacker_action] = actions_taken_count['attacker'].get(attacker_action, 0) + 1\n",
    "\n",
    "\n",
    "        # Defender's Turn\n",
    "        exploration = np.random.random() < defender_epsilon\n",
    "        if exploration:\n",
    "            _, defender_action, _ = defender_learner.explore(wrapped_env)\n",
    "        else:\n",
    "            _, defender_action, _ = defender_learner.exploit(wrapped_env, observation)\n",
    "\n",
    "        if defender_action is not None:\n",
    "            observation, defender_reward, done, _ = gym_environment.step(defender_action)\n",
    "            #action= _['defender_info']['action']\n",
    "            defender_total_reward += defender_reward\n",
    "            actions_taken_count['defender'].append(defender_action)  # Track action\n",
    "            episode_rewards.append(defender_reward)  # Track reward variability\n",
    "            tracker.log_action('defender', defender_action, exploration)\n",
    "            #actions_taken_count['defender'][defender_action] = actions_taken_count['defender'].get(defender_action, 0) + 1\n",
    "        #logging\n",
    "        # After each episode\n",
    "        tracker.log_episode(attacker_total_reward, defender_total_reward, steps_this_episode, episode_rewards)\n",
    "        tracker.print_summary()\n",
    "        tracker._plot_rewards()\n",
    "\n",
    "        if done:\n",
    "            break\n",
    "            \n",
    "    defender_info = _['defender_info']  \n",
    "    if defender_info:  \n",
    "        defender_actions_taken.append(defender_info['performance_metrics']['actions_taken'])\n",
    "        defender_successful_actions.append(defender_info['performance_metrics']['successful_actions'])\n",
    "        defender_failed_actions.append(defender_info['performance_metrics']['failed_actions'])\n",
    "\n",
    "    attacker_rewards_per_episode.append(attacker_total_reward)\n",
    "    defender_rewards_per_episode.append(defender_total_reward)\n",
    "    episode_lengths.append(steps_this_episode)\n",
    "    reward_variability.append(np.var(episode_rewards))\n",
    "    print(f\"Episode {episode + 1}: Attacker Reward = {attacker_total_reward}, Defender Reward = {defender_total_reward}\")\n",
    "\n",
    "wrapped_env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c26450c-9246-4597-8339-d8692d149e81",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "epsilonpoint1_defender_reward_per_episode = defender_rewards_per_episode\n",
    "epsilonpoint1_attacker_reward_per_episode = attacker_rewards_per_episode\n",
    "epsilonpoint1sa = defender_successful_actions \n",
    "epsilonpoint1fa = defender_failed_actions \n",
    "epsilonpoint1dat = defender_actions_taken "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e52b35b1-e998-4dfc-ba3c-331f0d273ba9",
   "metadata": {},
   "source": [
    "# Hyperparameter Tuned Epsilon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "747f6a95-9a84-4a26-96f8-e78b6b5d0b14",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cyberbattle.agents.baseline.agent_tabularqlearning as a\n",
    "import cyberbattle.agents.baseline.learner as learner\n",
    "from cyberbattle.agents.baseline.agent_wrapper import Verbosity\n",
    "epsilon = 0.666666\n",
    "gamma= 0.06925\n",
    "learning_rate= 0.34444444444444444\n",
    "epsilon_exponential_decay=3198.830407105263\n",
    "epsilon_minimum=0.05708811937931034\n",
    "episode_count=50\n",
    "iteration_count=200\n",
    "epsilon_multdecay=0.7777777777777778\n",
    "statespace = w.HashEncoding(ep, [\n",
    "            # Feature_discovered_node_count(),\n",
    "            # Feature_discovered_credential_count(),\n",
    "            w.Feature_discovered_ports_sliding(ep),\n",
    "            w.Feature_discovered_nodeproperties_sliding(ep),\n",
    "            w.Feature_discovered_notowned_node_count(ep, 3)\n",
    "        ], 5000)  # should not be too small, pick something big to avoid collision\n",
    "\n",
    "actionspace = w.RavelEncoding(ep, [\n",
    "            w.Feature_active_node_properties(ep)])\n",
    "print(statespace)\n",
    "print(actionspace)\n",
    "q_matrix = DefenderQMatrix(statespace, actionspace,gym_env)\n",
    "learner2 = DefenderQLearner(gym_env, defender_agent, ep, q_matrix, epsilon=0.9, gamma=0.015, learning_rate=.07)\n",
    "random.seed(120394016)\n",
    "#%matplotlib inline\n",
    "\n",
    "trained_defender = epsilon_greedy_defender_training(\n",
    "        cyberbattle_gym_env=gym_env,\n",
    "        environment_properties=ep,\n",
    "        learner=learner2,\n",
    "        episode_count=episode_count,  # Set the number of episodes for the training\n",
    "        iteration_count=iteration_count,  # Set the number of iterations per episode\n",
    "        epsilon=epsilon,\n",
    "        epsilon_exponential_decay=epsilon_exponential_decay,\n",
    "        epsilon_minimum=epsilon_minimum,\n",
    "        epsilon_multdecay=epsilon_multdecay,\n",
    "        verbosity=logging.INFO,\n",
    "        render=True,\n",
    "        plot_episodes_length=True,\n",
    "        title=\"Defender Q-Learning Training\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a06df610-e63e-4b79-9164-b936ec2cc3c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "\n",
    "gym_env = gym.make('CyberBattleChain-v0',\n",
    "    size=12,\n",
    "    attacker_goal=AttackerGoal(\n",
    "        own_atleast=0,\n",
    "        own_atleast_percent=1.0\n",
    "    ),\n",
    "    defender_constraint=DefenderConstraint(\n",
    "        maintain_sla=0.80\n",
    "    )\n",
    ")\n",
    "\n",
    "defender_name = \"DefenderName\" \n",
    "defender_agent = DefenderWrapper(gym_env,defender_name)\n",
    "\n",
    "gym_environment = gym.make('CyberBattleChain-v0',\n",
    "    size=12,\n",
    "    attacker_goal=AttackerGoal(\n",
    "        own_atleast=0,\n",
    "        own_atleast_percent=1.0\n",
    "    ),\n",
    "    defender_constraint=DefenderConstraint(\n",
    "        maintain_sla=0.80\n",
    "    ),\n",
    "    defender_agent=defender_agent\n",
    ")\n",
    "\n",
    "#this was made due to issues where the attacker q learner needed to use a wrapper environment\n",
    "from cyberbattle.agents.baseline.agent_wrapper import AgentWrapper, StateAugmentation\n",
    "def wrap_environment(env):\n",
    "    initial_observation = env.reset()  \n",
    "    state = StateAugmentation(initial_observation)  \n",
    "    return AgentWrapper(env, state)\n",
    "\n",
    "attacker_learner = trained_attacker['learner']\n",
    "defender_learner = trained_defender['learner']\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Simulation Parameters\n",
    "number_of_episodes = 50\n",
    "max_steps_per_episode = 200\n",
    "attacker_epsilon = 0.1\n",
    "defender_epsilon = 0.1\n",
    "\n",
    "# Reward Tracking\n",
    "attacker_rewards_per_episode = []\n",
    "defender_rewards_per_episode = []\n",
    "win_rates = {'attacker': [], 'defender': []}\n",
    "actions_taken_count = {'attacker': [], 'defender': []}\n",
    "exploration_vs_exploitation = {'exploration': [], 'exploitation': []}\n",
    "episode_lengths = []\n",
    "state_visitation_frequencies = {}  \n",
    "reward_variability = []\n",
    "defender_successful_actions = []\n",
    "defender_failed_actions = []\n",
    "defender_actions_taken = []\n",
    "tracker = SimulationTracker() #instantiate simulation tracker\n",
    "wrapped_env = wrap_environment(gym_environment)\n",
    "win_counts = {'attacker': 0, 'defender': 0}\n",
    "# Main Simulation Loop\n",
    "random.seed(120394016)\n",
    "for episode in range(number_of_episodes):\n",
    "    observation = wrapped_env.reset()\n",
    "    attacker_total_reward = 0\n",
    "    defender_total_reward = 0\n",
    "    steps_this_episode=0\n",
    "    episode_rewards = []  # Track rewards for this episode\n",
    "    for step in range(max_steps_per_episode):\n",
    "        steps_this_episode +=1\n",
    "        # Attacker's Turn\n",
    "        exploration = np.random.random() < attacker_epsilon\n",
    "        if exploration:\n",
    "            _, attacker_action, _ = attacker_learner.explore(wrapped_env)\n",
    "        else:\n",
    "            _, attacker_action, _ = attacker_learner.exploit(wrapped_env, observation)\n",
    "        #print(attacker_action)\n",
    "        if attacker_action is not None:\n",
    "            observation, attacker_reward, done, _ = gym_environment.step(attacker_action)\n",
    "            attacker_total_reward += attacker_reward\n",
    "            print(_)\n",
    "            actions_taken_count['attacker'].append(attacker_action)  # Track action\n",
    "            episode_rewards.append(attacker_reward)  # Track reward variability\n",
    "            tracker.log_action('attacker', attacker_action, exploration)\n",
    "            #actions_taken_count['attacker'][attacker_action] = actions_taken_count['attacker'].get(attacker_action, 0) + 1\n",
    "\n",
    "\n",
    "        # Defender's Turn\n",
    "        exploration = np.random.random() < defender_epsilon\n",
    "        if exploration:\n",
    "            _, defender_action, _ = defender_learner.explore(wrapped_env)\n",
    "        else:\n",
    "            _, defender_action, _ = defender_learner.exploit(wrapped_env, observation)\n",
    "\n",
    "        if defender_action is not None:\n",
    "            observation, defender_reward, done, _ = gym_environment.step(defender_action)\n",
    "            #action= _['defender_info']['action']\n",
    "            defender_total_reward += defender_reward\n",
    "            actions_taken_count['defender'].append(defender_action)  # Track action\n",
    "            episode_rewards.append(defender_reward)  # Track reward variability\n",
    "            tracker.log_action('defender', defender_action, exploration)\n",
    "            #actions_taken_count['defender'][defender_action] = actions_taken_count['defender'].get(defender_action, 0) + 1\n",
    "        #logging\n",
    "        # After each episode\n",
    "        tracker.log_episode(attacker_total_reward, defender_total_reward, steps_this_episode, episode_rewards)\n",
    "        tracker.print_summary()\n",
    "        tracker._plot_rewards()\n",
    "\n",
    "        if done:\n",
    "            break\n",
    "            \n",
    "    defender_info = _['defender_info']  \n",
    "    if defender_info:  \n",
    "        defender_actions_taken.append(defender_info['performance_metrics']['actions_taken'])\n",
    "        defender_successful_actions.append(defender_info['performance_metrics']['successful_actions'])\n",
    "        defender_failed_actions.append(defender_info['performance_metrics']['failed_actions'])\n",
    "\n",
    "    attacker_rewards_per_episode.append(attacker_total_reward)\n",
    "    defender_rewards_per_episode.append(defender_total_reward)\n",
    "    episode_lengths.append(steps_this_episode)\n",
    "    reward_variability.append(np.var(episode_rewards))\n",
    "    print(f\"Episode {episode + 1}: Attacker Reward = {attacker_total_reward}, Defender Reward = {defender_total_reward}\")\n",
    "\n",
    "wrapped_env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8087fe91-259b-47e0-a12a-7422234f08ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "epsilontuned_defender_reward_per_episode = defender_rewards_per_episode\n",
    "epsilontuned_attacker_reward_per_episode = attacker_rewards_per_episode\n",
    "epsilontunedsa = defender_successful_actions \n",
    "epsilontunedfa = defender_failed_actions \n",
    "epsilontuneddat = defender_actions_taken "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "729824c7-241a-42c3-934b-ab544cf07112",
   "metadata": {},
   "source": [
    "# Statistical Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10064b56-1297-4675-bfd5-216c9b61a707",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install statsmodels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e81023ad-a66a-40b7-9b53-068af2ea901b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "import numpy as np\n",
    "\n",
    "# ANOVA test for defender rewards\n",
    "F_statistic, p_value = stats.f_oneway(\n",
    "    epsilon1_defender_reward_per_episode,\n",
    "    epsilon1half_defender_reward_per_episode,\n",
    "    epsilonpoint1_defender_reward_per_episode,\n",
    "    epsilontuned_defender_reward_per_episode\n",
    ")\n",
    "\n",
    "print(f\"F-statistic: {F_statistic}, p-value: {p_value}\")\n",
    "\n",
    "if p_value < 0.05:\n",
    "    print(\"There is a statistically significant difference in defender rewards among the different epsilon values.\")\n",
    "else:\n",
    "    print(\"There is no statistically significant difference in defender rewards among the different epsilon values.\")\n",
    "\n",
    "from statsmodels.stats.multicomp import pairwise_tukeyhsd\n",
    "\n",
    "# Combine the data into one array and create an array of group labels\n",
    "all_rewards = np.concatenate([\n",
    "    epsilon1_defender_reward_per_episode,\n",
    "    epsilon1half_defender_reward_per_episode,\n",
    "    epsilonpoint1_defender_reward_per_episode,\n",
    "    epsilontuned_defender_reward_per_episode\n",
    "])\n",
    "\n",
    "groups = ['epsilon1'] * len(epsilon1_defender_reward_per_episode) \\\n",
    "         + ['epsilon1half'] * len(epsilon1half_defender_reward_per_episode) \\\n",
    "         + ['epsilon0.1'] * len(epsilonpoint1_defender_reward_per_episode) \\\n",
    "         + ['epsilontuned'] * len(epsilontuned_defender_reward_per_episode)\n",
    "\n",
    "tukey_results = pairwise_tukeyhsd(all_rewards, groups, 0.05)\n",
    "print(tukey_results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8b01e24-e5d6-49f2-8a8b-201adbcf3a21",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assume we have the means and confidence intervals for each group\n",
    "means = [\n",
    "    np.mean(epsilon1_defender_reward_per_episode),\n",
    "    np.mean(epsilon1half_defender_reward_per_episode),\n",
    "    np.mean(epsilonpoint1_defender_reward_per_episode),\n",
    "    np.mean(epsilontuned_defender_reward_per_episode)\n",
    "]\n",
    "\n",
    "# Confidence intervals would typically come from the Tukey HSD output\n",
    "# For this example, I'm using a placeholder for the CIs\n",
    "confidence_intervals = [\n",
    "    (1.96 * np.std(epsilon1_defender_reward_per_episode) / np.sqrt(len(epsilon1_defender_reward_per_episode))),\n",
    "    (1.96 * np.std(epsilon1half_defender_reward_per_episode) / np.sqrt(len(epsilon1half_defender_reward_per_episode))),\n",
    "    (1.96 * np.std(epsilonpoint1_defender_reward_per_episode) / np.sqrt(len(epsilonpoint1_defender_reward_per_episode))),\n",
    "    (1.96 * np.std(epsilontuned_defender_reward_per_episode) / np.sqrt(len(epsilontuned_defender_reward_per_episode)))\n",
    "]\n",
    "\n",
    "# Calculate the error bars (which represent the confidence intervals)\n",
    "error_bars = [ci for ci in confidence_intervals]\n",
    "\n",
    "# Groups for the x-axis\n",
    "groups = ['epsilon=1', 'epsilon=0.5', 'epsilon=0.1', 'epsilon=tuned']\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.errorbar(groups, means, yerr=error_bars, fmt='o', ecolor='orangered', capsize=5, capthick=2)\n",
    "\n",
    "# Add the p-value text\n",
    "if p_value < 0.05:\n",
    "    significance_text = \"Statistically significant differences (p < 0.05)\"\n",
    "else:\n",
    "    significance_text = \"No significant differences (p > 0.05)\"\n",
    "\n",
    "plt.title(f\"Defender Rewards by Epsilon Value\\n{significance_text}\")\n",
    "plt.xlabel('Epsilon Values')\n",
    "plt.ylabel('Mean Defender Rewards')\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac69f6d7-956c-4275-9e55-141463509d2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "group_means = [\n",
    "    np.mean(epsilon1_defender_reward_per_episode),\n",
    "    np.mean(epsilon1half_defender_reward_per_episode),\n",
    "    np.mean(epsilonpoint1_defender_reward_per_episode),\n",
    "    np.mean(epsilontuned_defender_reward_per_episode)\n",
    "]\n",
    "\n",
    "group_stds = [\n",
    "    np.std(epsilon1_defender_reward_per_episode, ddof=1),\n",
    "    np.std(epsilon1half_defender_reward_per_episode, ddof=1),\n",
    "    np.std(epsilonpoint1_defender_reward_per_episode, ddof=1),\n",
    "    np.std(epsilontuned_defender_reward_per_episode, ddof=1)\n",
    "]\n",
    "\n",
    "# Calculate the 95% confidence intervals\n",
    "confidence_intervals = [std / np.sqrt(len(epsilon1_defender_reward_per_episode)) * 1.96 for std in group_stds]\n",
    "\n",
    "# Groups for the x-axis\n",
    "groups = ['epsilon=1', 'epsilon=0.5', 'epsilon=0.1', 'epsilon tuned']\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(12, 7))\n",
    "\n",
    "# Bar chart with error bars\n",
    "plt.bar(groups, group_means, yerr=confidence_intervals, capsize=5, color=['skyblue', 'lightgreen', 'salmon', 'violet'])\n",
    "\n",
    "# Adding the p-value on the plot\n",
    "plt.text(2, max(group_means) - max(group_means) / 10, f'ANOVA p-value: {p_value:.2e}', \n",
    "         fontsize=12, ha='center', va='bottom', color='black')\n",
    "\n",
    "# Adding title and labels\n",
    "plt.title('Comparison of Defender Rewards Across Different Epsilons')\n",
    "plt.xlabel('Epsilon Values')\n",
    "plt.ylabel('Average Defender Reward')\n",
    "\n",
    "# Show grid\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d378f10-d87c-4c7e-8962-4b3a7d75cc59",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "210eb1ff-b85a-41be-81ff-25f62f510fe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ANOVA test\n",
    "F_statistic, p_value = stats.f_oneway(\n",
    "    epsilon1sa,\n",
    "    epsilonhalffa,\n",
    "    epsilonpoint1sa,\n",
    "    epsilontunedsa\n",
    ")\n",
    "\n",
    "# Performing Tukey's Range Test\n",
    "from statsmodels.stats.multicomp import pairwise_tukeyhsd\n",
    "\n",
    "# Concatenate the data and create groups\n",
    "data = np.concatenate([epsilon1sa, epsilonhalfsa, epsilonpoint1sa, epsilontunedsa])\n",
    "groups = ['epsilon1'] * len(epsilon1sa) + ['epsilon0.5'] * len(epsilonhalfsa) + \\\n",
    "         ['epsilon0.1'] * len(epsilonpoint1sa) + ['epsilontuned'] * len(epsilontunedsa)\n",
    "\n",
    "# Apply Tukey's range test\n",
    "tukey = pairwise_tukeyhsd(endog=data, groups=groups, alpha=0.05)\n",
    "\n",
    "# Plotting the results\n",
    "tukey.plot_simultaneous(comparison_name='epsilon1')\n",
    "plt.title('Tukey\\'s test for different epsilon values')\n",
    "plt.ylabel('Epsilon values')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Print the Tukey test table\n",
    "print(tukey)\n",
    "\n",
    "# Create bar chart for the means\n",
    "means = [np.mean(epsilon1sa), np.mean(epsilonhalfsa), np.mean(epsilonpoint1sa), np.mean(epsilontunedsa)]\n",
    "stds = [np.std(epsilon1sa), np.std(epsilonhalfsa), np.std(epsilonpoint1sa), np.std(epsilontunedsa)]\n",
    "positions = np.arange(4)\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.bar(positions, means, yerr=stds, align='center', alpha=0.7, ecolor='black', capsize=10)\n",
    "plt.xticks(positions, ['epsilon1', 'epsilon0.5', 'epsilon0.1', 'epsilontuned'])\n",
    "plt.ylabel('Average Successful Actions')\n",
    "plt.xlabel('Epsilon Values')\n",
    "plt.title('Average Successful Actions for Different Epsilons')\n",
    "plt.axhline(y=np.mean(data), color='gray', linestyle='--', label='Overall mean')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90eb6260-c428-4328-bc34-98d79c50f08a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy import stats\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "epsilon1_proportion = np.array(epsilon1sa) / np.array(epsilon1dat)\n",
    "epsilon05_proportion = np.array(epsilonhalfsa) / np.array(epsilonhalfdat)\n",
    "epsilon01_proportion = np.array(epsilonpoint1sa) / np.array(epsilonpoint1dat)\n",
    "epsilontuned_proportion = np.array(epsilontunedsa) / np.array(epsilontuneddat)\n",
    "\n",
    "# Perform ANOVA\n",
    "F_statistic, p_value = stats.f_oneway(\n",
    "    epsilon1_proportion,\n",
    "    epsilon05_proportion,\n",
    "    epsilon01_proportion,\n",
    "    epsilontuned_proportion\n",
    ")\n",
    "\n",
    "# Plotting\n",
    "groups = ['epsilon=1', 'epsilon=0.5', 'epsilon=0.1', 'epsilon tuned']\n",
    "group_proportions = [\n",
    "    np.mean(epsilon1_proportion),\n",
    "    np.mean(epsilon05_proportion),\n",
    "    np.mean(epsilon01_proportion),\n",
    "    np.mean(epsilontuned_proportion)\n",
    "]\n",
    "\n",
    "group_std = [\n",
    "    np.std(epsilon1_proportion, ddof=1),\n",
    "    np.std(epsilon05_proportion, ddof=1),\n",
    "    np.std(epsilon01_proportion, ddof=1),\n",
    "    np.std(epsilontuned_proportion, ddof=1)\n",
    "]\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.bar(groups, group_proportions, yerr=group_std, capsize=5, color=['skyblue', 'lightgreen', 'salmon', 'violet'])\n",
    "plt.ylabel('Proportion of Successful Actions')\n",
    "plt.title('Proportion of Successful Actions for Different Epsilons')\n",
    "plt.axhline(y=np.mean(group_proportions), color='black', linestyle='--', label='Mean Proportion')\n",
    "plt.text(2, np.mean(group_proportions), f'Mean Proportion: {np.mean(group_proportions):.2f}', \n",
    "         fontsize=10, ha='center', va='bottom')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Output the ANOVA results\n",
    "print(f\"ANOVA F-statistic: {F_statistic:.2f}, p-value: {p_value:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4746e4d-d18e-403f-a3de-a3e564890ab3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy import stats\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "epsilon1_proportion = np.array(epsilon1sa) / np.array(epsilon1dat)\n",
    "epsilon05_proportion = np.array(epsilonhalfsa) / np.array(epsilonhalfdat)\n",
    "epsilon01_proportion = np.array(epsilonpoint1sa) / np.array(epsilonpoint1dat)\n",
    "epsilontuned_proportion = np.array(epsilontunedsa) / np.array(epsilontuneddat)\n",
    "\n",
    "# Perform ANOVA\n",
    "F_statistic, p_value = stats.f_oneway(\n",
    "    epsilon1_proportion,\n",
    "    epsilon05_proportion,\n",
    "    epsilon01_proportion,\n",
    "    epsilontuned_proportion\n",
    ")\n",
    "\n",
    "# Plotting\n",
    "groups = ['epsilon=1', 'epsilon=0.5', 'epsilon=0.1', 'epsilon tuned']\n",
    "group_proportions = [\n",
    "    np.mean(epsilon1_proportion),\n",
    "    np.mean(epsilon05_proportion),\n",
    "    np.mean(epsilon01_proportion),\n",
    "    np.mean(epsilontuned_proportion)\n",
    "]\n",
    "\n",
    "group_std = [\n",
    "    np.std(epsilon1_proportion, ddof=1),\n",
    "    np.std(epsilon05_proportion, ddof=1),\n",
    "    np.std(epsilon01_proportion, ddof=1),\n",
    "    np.std(epsilontuned_proportion, ddof=1)\n",
    "]\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.bar(groups, group_proportions, yerr=group_std, capsize=5, color=['skyblue', 'lightgreen', 'salmon', 'violet'])\n",
    "plt.ylabel('Proportion of Successful Actions')\n",
    "plt.title('Proportion of Successful Actions for Different Epsilons')\n",
    "plt.axhline(y=np.mean(group_proportions), color='black', linestyle='--', label='Mean Proportion')\n",
    "plt.text(2, np.mean(group_proportions), f'Mean Proportion: {np.mean(group_proportions):.2f}', \n",
    "         fontsize=10, ha='center', va='bottom')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Output the ANOVA results\n",
    "print(f\"ANOVA F-statistic: {F_statistic:.2f}, p-value: {p_value:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:cybersim] *",
   "language": "python",
   "name": "conda-env-cybersim-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
