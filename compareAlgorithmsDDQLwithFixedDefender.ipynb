{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5f5a5cc7-0140-4728-873e-c886efba3177",
      "metadata": {
        "id": "5f5a5cc7-0140-4728-873e-c886efba3177"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "import logging\n",
        "from typing import cast\n",
        "import gym\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt  # type:ignore\n",
        "from cyberbattle.agents.baseline.learner import TrainedLearner\n",
        "import cyberbattle.agents.baseline.plotting as p\n",
        "import cyberbattle.agents.baseline.agent_wrapper as w\n",
        "import cyberbattle.agents.baseline.agent_tabularqlearning as a\n",
        "from cyberbattle.agents.baseline.agent_wrapper import Verbosity\n",
        "import cyberbattle.agents.baseline.learner as learner\n",
        "import importlib\n",
        "import cyberbattle.agents.baseline.agent_dql as dqla\n",
        "import cyberbattle.agents.baseline.agent_randomcredlookup as rca\n",
        "import cyberbattle.agents.baseline.agent_ppo as ppo\n",
        "from cyberbattle._env.defender import ScanAndReimageCompromisedMachines\n",
        "from cyberbattle._env.cyberbattle_env import AttackerGoal, DefenderConstraint\n",
        "from typing import cast\n",
        "from cyberbattle._env.cyberbattle_env import CyberBattleEnv\n",
        "from cyberbattle._env.cyberbattle_toyctf import CyberBattleToyCtf\n",
        "from stable_baselines3.a2c.a2c import A2C\n",
        "from stable_baselines3.ppo.ppo import PPO\n",
        "from cyberbattle._env.flatten_wrapper import FlattenObservationWrapper, FlattenActionWrapper\n",
        "import os\n",
        "from stable_baselines3 import PPO\n",
        "import cyberbattle.agents.baseline.agent_tabularqlearning as tqa\n",
        "import cyberbattle.agents.baseline.agent_dql as dqla\n",
        "import cyberbattle.agents.baseline.agent_ddql as ddqla\n",
        "import cyberbattle.agents.baseline.agent_dueling_dql as duelingdqla\n",
        "import cyberbattle.agents.baseline.agent_dueling_ddql as dueling_ddqla\n",
        "import random\n",
        "random.seed(120394016)\n",
        "%matplotlib inline\n",
        "logging.basicConfig(stream=sys.stdout, level=logging.ERROR, format=\"%(levelname)s: %(message)s\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "eefd380b-be63-42c7-8b25-66b01c862ff3",
      "metadata": {
        "id": "eefd380b-be63-42c7-8b25-66b01c862ff3"
      },
      "outputs": [],
      "source": [
        "# Parameters\n",
        "gymid = \"CyberBattleChain-v0\"\n",
        "iteration_count = 5000\n",
        "training_episode_count = 50\n",
        "eval_episode_count = 10 #10\n",
        "maximum_node_count = 22\n",
        "maximum_total_credentials = 22\n",
        "# env_size = 20\n",
        "env_size = 12\n",
        "# Load the Gym environment\n",
        "if env_size:\n",
        "    gym_env = gym.make(gymid, size=env_size,attacker_goal=AttackerGoal(\n",
        "                                         own_atleast=0,\n",
        "                                         own_atleast_percent=1.0\n",
        "                                     ),\n",
        "                                     defender_constraint=DefenderConstraint(\n",
        "                                         maintain_sla=0.80\n",
        "                                     ),\n",
        "                                     defender_agent=ScanAndReimageCompromisedMachines(\n",
        "                                         probability=0.6,\n",
        "                                         scan_capacity=2,\n",
        "                                         scan_frequency=5))\n",
        "else:\n",
        "    gym_env = gym.make(gymid)\n",
        "\n",
        "ep = w.EnvironmentBounds.of_identifiers(\n",
        "    maximum_node_count=maximum_node_count,\n",
        "    maximum_total_credentials=maximum_total_credentials,\n",
        "    identifiers=gym_env.identifiers\n",
        ")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "faabde7d-b2e8-4d5f-ab6f-ca9ff8335fa8",
      "metadata": {
        "id": "faabde7d-b2e8-4d5f-ab6f-ca9ff8335fa8"
      },
      "outputs": [],
      "source": [
        "debugging = True\n",
        "if debugging:\n",
        "    print(f\"port_count = {ep.port_count}, property_count = {ep.property_count}\")\n",
        "\n",
        "    gym_env.environment\n",
        "    # training_env.environment.plot_environment_graph()\n",
        "    gym_env.environment.network.nodes\n",
        "    gym_env.action_space\n",
        "    gym_env.action_space.sample()\n",
        "    gym_env.observation_space.sample()\n",
        "    o0 = gym_env.reset()\n",
        "    o_test, r, d, i = gym_env.step(gym_env.sample_valid_action())\n",
        "    o0 = gym_env.reset()\n",
        "\n",
        "    o0.keys()\n",
        "\n",
        "    fe_example = w.RavelEncoding(ep, [w.Feature_active_node_properties(ep), w.Feature_discovered_node_count(ep)])\n",
        "    a = w.StateAugmentation(o0)\n",
        "    w.Feature_discovered_ports(ep).get(a, None)\n",
        "    fe_example.encode_at(a, 0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "99baad28-ab57-4d3a-9ab8-f7ff436d9f25",
      "metadata": {
        "id": "99baad28-ab57-4d3a-9ab8-f7ff436d9f25"
      },
      "outputs": [],
      "source": [
        "random.seed(120394016)\n",
        "%matplotlib inline\n",
        "# Evaluate the Deep Q-learning agent\n",
        "dql_run = learner.epsilon_greedy_search(\n",
        "    cyberbattle_gym_env=gym_env,\n",
        "    environment_properties=ep,\n",
        "    learner=dqla.DeepQLearnerPolicy(\n",
        "        ep=ep,\n",
        "        gamma=0.015,\n",
        "        replay_memory_size=10000,\n",
        "        target_update=10,\n",
        "        batch_size=512,\n",
        "        # torch default learning rate is 1e-2\n",
        "        # a large value helps converge in less episodes\n",
        "        learning_rate=0.01\n",
        "    ),\n",
        "    episode_count=training_episode_count,\n",
        "    iteration_count=iteration_count,\n",
        "    epsilon=0.90,\n",
        "    epsilon_exponential_decay=5000,\n",
        "    epsilon_minimum=0.10,\n",
        "    verbosity=Verbosity.Quiet,\n",
        "    render=True,\n",
        "    plot_episodes_length=True,\n",
        "    title=\"DQL\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6c85e4c3-7421-40bd-b4bf-722ce030d30f",
      "metadata": {
        "id": "6c85e4c3-7421-40bd-b4bf-722ce030d30f"
      },
      "outputs": [],
      "source": [
        "random.seed(120394016)\n",
        "%matplotlib inline\n",
        "dql_exploit_run = learner.epsilon_greedy_search(\n",
        "    gym_env,\n",
        "    ep,\n",
        "    learner=dql_run['learner'],\n",
        "    episode_count=eval_episode_count,\n",
        "    iteration_count=iteration_count,\n",
        "    epsilon=0.0,\n",
        "    epsilon_minimum=0.00,\n",
        "    render=True,\n",
        "    plot_episodes_length=True,\n",
        "    verbosity=Verbosity.Quiet,\n",
        "    title=\"Exploiting DQL\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "83d36d66-b0a5-463f-9067-1248060b60aa",
      "metadata": {
        "id": "83d36d66-b0a5-463f-9067-1248060b60aa"
      },
      "outputs": [],
      "source": [
        "# Evaluate the Double Deep Q-learning agent\n",
        "ddql_run = learner.epsilon_greedy_search(\n",
        "    cyberbattle_gym_env=gym_env,\n",
        "    environment_properties=ep,\n",
        "    learner=ddqla.DeepQLearnerPolicy(\n",
        "        ep=ep,\n",
        "        gamma=0.015,\n",
        "        replay_memory_size=10000,\n",
        "        target_update=10,\n",
        "        batch_size=512,\n",
        "        # torch default learning rate is 1e-2\n",
        "        # a large value helps converge in less episodes\n",
        "        learning_rate=0.01\n",
        "    ),\n",
        "    episode_count=training_episode_count,\n",
        "    iteration_count=iteration_count,\n",
        "    epsilon=0.90,\n",
        "    epsilon_exponential_decay=5000,\n",
        "    epsilon_minimum=0.10,\n",
        "    verbosity=Verbosity.Quiet,\n",
        "    render=True,\n",
        "    plot_episodes_length=True,\n",
        "    title=\"DDQL\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b589e143-be72-4dd8-9c8b-0e6bf71f9a47",
      "metadata": {
        "id": "b589e143-be72-4dd8-9c8b-0e6bf71f9a47"
      },
      "outputs": [],
      "source": [
        "random.seed(120394016)\n",
        "%matplotlib inline\n",
        "# Evaluate an agent that exploits the Double Q-function learnt above\n",
        "ddql_exploit_run = learner.epsilon_greedy_search(\n",
        "    gym_env,\n",
        "    ep,\n",
        "    learner=ddql_run['learner'],\n",
        "    episode_count=eval_episode_count,\n",
        "    iteration_count=iteration_count,\n",
        "    epsilon=0.0,\n",
        "    epsilon_minimum=0.00,\n",
        "    render=True,\n",
        "    plot_episodes_length=True,\n",
        "    verbosity=Verbosity.Quiet,\n",
        "    title=\"Exploiting DDQL\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7ec23e47-e839-42a9-bf19-07200ab6afc7",
      "metadata": {
        "id": "7ec23e47-e839-42a9-bf19-07200ab6afc7"
      },
      "outputs": [],
      "source": [
        "random.seed(120394016)\n",
        "%matplotlib inline\n",
        "# Evaluate the Dueling Deep Q-learning agent\n",
        "dueling_dql_run = learner.epsilon_greedy_search(\n",
        "    cyberbattle_gym_env=gym_env,\n",
        "    environment_properties=ep,\n",
        "    learner=duelingdqla.DeepQLearnerPolicy(\n",
        "        ep=ep,\n",
        "        gamma=0.015,\n",
        "        replay_memory_size=10000,\n",
        "        target_update=10,\n",
        "        batch_size=512,\n",
        "        # torch default learning rate is 1e-2\n",
        "        # a large value helps converge in less episodes\n",
        "        learning_rate=0.01\n",
        "    ),\n",
        "    episode_count=training_episode_count,\n",
        "    iteration_count=iteration_count,\n",
        "    epsilon=0.90,\n",
        "    epsilon_exponential_decay=5000,\n",
        "    epsilon_minimum=0.10,\n",
        "    verbosity=Verbosity.Quiet,\n",
        "    render=True,\n",
        "    plot_episodes_length=True,\n",
        "    title=\"Dueling DQL\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "84798c85-f7f9-4d21-9ab1-21fd6f5e7e7c",
      "metadata": {
        "id": "84798c85-f7f9-4d21-9ab1-21fd6f5e7e7c"
      },
      "outputs": [],
      "source": [
        "random.seed(120394016)\n",
        "%matplotlib inline\n",
        "# Evaluate an agent that exploits the Dueling Q-function learnt above\n",
        "dueling_dql_exploit_run = learner.epsilon_greedy_search(\n",
        "    gym_env,\n",
        "    ep,\n",
        "    learner=dueling_dql_run['learner'],\n",
        "    episode_count=eval_episode_count,\n",
        "    iteration_count=iteration_count,\n",
        "    epsilon=0.0,\n",
        "    epsilon_minimum=0.00,\n",
        "    render=True,\n",
        "    plot_episodes_length=True,\n",
        "    verbosity=Verbosity.Quiet,\n",
        "    title=\"Exploiting Dueling DQL\"\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e1408ff5-d1ee-4af6-8bdd-3e4953e7c847",
      "metadata": {
        "id": "e1408ff5-d1ee-4af6-8bdd-3e4953e7c847"
      },
      "outputs": [],
      "source": [
        "random.seed(120394016)\n",
        "%matplotlib inline\n",
        "# Evaluate the Dueling Double Deep Q-learning agent\n",
        "dueling_ddql_run = learner.epsilon_greedy_search(\n",
        "    cyberbattle_gym_env=gym_env,\n",
        "    environment_properties=ep,\n",
        "    learner=dueling_ddqla.DeepQLearnerPolicy(\n",
        "        ep=ep,\n",
        "        gamma=0.015,\n",
        "        replay_memory_size=10000,\n",
        "        target_update=10,\n",
        "        batch_size=512,\n",
        "        # torch default learning rate is 1e-2\n",
        "        # a large value helps converge in less episodes\n",
        "        learning_rate=0.01\n",
        "    ),\n",
        "    episode_count=training_episode_count,\n",
        "    iteration_count=iteration_count,\n",
        "    epsilon=0.90,\n",
        "    epsilon_exponential_decay=5000,\n",
        "    epsilon_minimum=0.10,\n",
        "    verbosity=Verbosity.Quiet,\n",
        "    render=True,\n",
        "    plot_episodes_length=True,\n",
        "    title=\"Dueling DDQL\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "208fdece-8cf5-4d74-bb27-9597d9e80a1a",
      "metadata": {
        "id": "208fdece-8cf5-4d74-bb27-9597d9e80a1a"
      },
      "outputs": [],
      "source": [
        "random.seed(120394016)\n",
        "%matplotlib inline\n",
        "# Evaluate an agent that exploits the  the Dueling Double Q-function learnt above\n",
        "dueling_ddql_exploit_run = learner.epsilon_greedy_search(\n",
        "    gym_env,\n",
        "    ep,\n",
        "    learner=dueling_ddql_run['learner'],\n",
        "    episode_count=eval_episode_count,\n",
        "    iteration_count=iteration_count,\n",
        "    epsilon=0.0,\n",
        "    epsilon_minimum=0.00,\n",
        "    render=True,\n",
        "    plot_episodes_length=True,\n",
        "    verbosity=Verbosity.Quiet,\n",
        "    title=\"Exploiting Dueling DDQL\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "46f22714-9e58-49bd-a1eb-2d3889ea7fb8",
      "metadata": {
        "id": "46f22714-9e58-49bd-a1eb-2d3889ea7fb8"
      },
      "outputs": [],
      "source": [
        "random.seed(120394016)\n",
        "%matplotlib inline\n",
        "# Evaluate the random agent\n",
        "random_run = learner.epsilon_greedy_search(\n",
        "    gym_env,\n",
        "    ep,\n",
        "    learner=learner.RandomPolicy(),\n",
        "    episode_count=eval_episode_count,\n",
        "    iteration_count=iteration_count,\n",
        "    epsilon=1.0,  # purely random\n",
        "    render=True,\n",
        "    verbosity=Verbosity.Quiet,\n",
        "    plot_episodes_length=True,\n",
        "    title=\"Random search\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8d0cf3da-36f5-469f-9ae3-3a63238915e8",
      "metadata": {
        "id": "8d0cf3da-36f5-469f-9ae3-3a63238915e8"
      },
      "outputs": [],
      "source": [
        "random.seed(120394016)\n",
        "%matplotlib inline\n",
        "# Evaluate a random agent that opportunistically exploits\n",
        "# credentials gathere in its local cache\n",
        "credlookup_run = learner.epsilon_greedy_search(\n",
        "    gym_env,\n",
        "    ep,\n",
        "    learner=rca.CredentialCacheExploiter(),\n",
        "    episode_count=10,\n",
        "    iteration_count=iteration_count,\n",
        "    epsilon=0.90,\n",
        "    render=True,\n",
        "    epsilon_exponential_decay=10000,\n",
        "    epsilon_minimum=0.10,\n",
        "    verbosity=Verbosity.Quiet,\n",
        "    title=\"Credential lookups (ϵ-greedy)\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d91e7321-19d9-40b2-8cb7-168c2f42d9e6",
      "metadata": {
        "id": "d91e7321-19d9-40b2-8cb7-168c2f42d9e6"
      },
      "outputs": [],
      "source": [
        "random.seed(120394016)\n",
        "%matplotlib inline\n",
        "# Evaluate a Tabular Q-learning agent\n",
        "tabularq_run = learner.epsilon_greedy_search(\n",
        "    gym_env,\n",
        "    ep,\n",
        "    learner=tqa.QTabularLearner(\n",
        "        ep,\n",
        "        gamma=0.015, learning_rate=0.01, exploit_percentile=100),\n",
        "    episode_count=training_episode_count,\n",
        "    iteration_count=iteration_count,\n",
        "    epsilon=0.90,\n",
        "    epsilon_exponential_decay=5000,\n",
        "    epsilon_minimum=0.01,\n",
        "    verbosity=Verbosity.Quiet,\n",
        "    render=True,\n",
        "    plot_episodes_length=True,\n",
        "    title=\"Tabular Q-learning\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a9e7631f-f387-405d-9f68-adfe927de248",
      "metadata": {
        "id": "a9e7631f-f387-405d-9f68-adfe927de248"
      },
      "outputs": [],
      "source": [
        "random.seed(120394016)\n",
        "%matplotlib inline\n",
        "# Evaluate an agent that exploits the Q-table learnt above\n",
        "tabularq_exploit_run = learner.epsilon_greedy_search(\n",
        "    gym_env,\n",
        "    ep,\n",
        "    learner=tqa.QTabularLearner(\n",
        "        ep,\n",
        "        trained=tabularq_run['learner'],\n",
        "        gamma=0.0,\n",
        "        learning_rate=0.0,\n",
        "        exploit_percentile=90),\n",
        "    episode_count=eval_episode_count,\n",
        "    iteration_count=iteration_count,\n",
        "    epsilon=0.0,\n",
        "    render=True,\n",
        "    verbosity=Verbosity.Quiet,\n",
        "    title=\"Exploiting Q-matrix\"\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "44e0cd28-2d81-4873-b9de-4ad111bab9eb",
      "metadata": {
        "id": "44e0cd28-2d81-4873-b9de-4ad111bab9eb"
      },
      "outputs": [],
      "source": [
        "# Compare and plot results for all the agents\n",
        "all_runs = [\n",
        "     random_run,\n",
        "     #credlookup_run,\n",
        "     #tabularq_run,\n",
        "     #tabularq_exploit_run,\n",
        "     dql_run,\n",
        "     dql_exploit_run,\n",
        "         #ddql_run,\n",
        "     #ddql_exploit_run\n",
        " ]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b43ff2c3-cdef-4b04-84b4-6f6a569185e4",
      "metadata": {
        "id": "b43ff2c3-cdef-4b04-84b4-6f6a569185e4"
      },
      "outputs": [],
      "source": [
        "# Plot averaged cumulative rewards for DQL vs Random vs DQL-Exploit\n",
        "themodel = dqla.CyberBattleStateActionModel(ep)\n",
        "%matplotlib inline\n",
        "p.plot_averaged_cummulative_rewards(\n",
        "     all_runs=all_runs,\n",
        "     title=f'Benchmark -- max_nodes={ep.maximum_node_count}, episodes={eval_episode_count},\\n'\n",
        "     f'State: {[f.name() for f in themodel.state_space.feature_selection]} '\n",
        "     f'({len(themodel.state_space.feature_selection)}\\n'\n",
        "     f\"Action: abstract_action ({themodel.action_space.flat_size()})\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "97ae0aaa-0547-405c-96de-27b00108c78c",
      "metadata": {
        "id": "97ae0aaa-0547-405c-96de-27b00108c78c"
      },
      "outputs": [],
      "source": [
        "\n",
        " # Compare and plot results for all the agents\n",
        "all_runs = [\n",
        "    random_run,\n",
        " #     credlookup_run,\n",
        " #     tabularq_run,\n",
        " #     tabularq_exploit_run,\n",
        "    dql_run,\n",
        "    dql_exploit_run,\n",
        "    ddql_run,\n",
        "    ddql_exploit_run\n",
        "]\n",
        "#\n",
        " # Plot averaged cumulative rewards for DQL vs Random vs DQL-Exploit vs DDQL vs DDQL-Exploit\n",
        "themodel = dqla.CyberBattleStateActionModel(ep)\n",
        " p.plot_averaged_cummulative_rewards(\n",
        "     all_runs=all_runs,\n",
        "     title=f'Benchmark -- max_nodes={ep.maximum_node_count}, episodes={eval_episode_count},\\n'\n",
        "     f'State: {[f.name() for f in themodel.state_space.feature_selection]} '\n",
        "    f'({len(themodel.state_space.feature_selection)}\\n'\n",
        "     f\"Action: abstract_action ({themodel.action_space.flat_size()})\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c9a3be6e-a165-4f84-a0f8-a041e7826d69",
      "metadata": {
        "id": "c9a3be6e-a165-4f84-a0f8-a041e7826d69"
      },
      "outputs": [],
      "source": [
        "# Compare and plot results for all the agents\n",
        "all_runs = [\n",
        "     random_run,\n",
        " #     credlookup_run,\n",
        " #     tabularq_run,\n",
        " #     tabularq_exploit_run,\n",
        "     dql_run,\n",
        "     dql_exploit_run,\n",
        "         ddql_run,\n",
        "     ddql_exploit_run,\n",
        "     dueling_dql_run,\n",
        "     dueling_dql_exploit_run\n",
        "]\n",
        "#\n",
        " # Plot averaged cumulative rewards for DQL vs Random vs DQL-Exploit vs DDQL vs DDQL-Exploit vs Dueling DQL vs Dueling DQL-Exploit\n",
        "themodel = dqla.CyberBattleStateActionModel(ep)\n",
        "p.plot_averaged_cummulative_rewards(\n",
        "     all_runs=all_runs,\n",
        "     title=f'Benchmark -- max_nodes={ep.maximum_node_count}, episodes={eval_episode_count},\\n'\n",
        "     f'State: {[f.name() for f in themodel.state_space.feature_selection]} '\n",
        "     f'({len(themodel.state_space.feature_selection)}\\n'\n",
        "     f\"Action: abstract_action ({themodel.action_space.flat_size()})\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4addb7d7-ae38-4ed2-ba66-e5141b91d41a",
      "metadata": {
        "id": "4addb7d7-ae38-4ed2-ba66-e5141b91d41a"
      },
      "outputs": [],
      "source": [
        "# Compare and plot results for all the agents\n",
        "all_runs = [\n",
        "    random_run,\n",
        " #     credlookup_run,\n",
        " #     tabularq_run,\n",
        " #     tabularq_exploit_run,\n",
        "    dql_run,\n",
        "    dql_exploit_run,\n",
        "         ddql_run,\n",
        "     ddql_exploit_run,\n",
        " #     dueling_dql_run,\n",
        " #     dueling_dql_exploit_run,\n",
        "     dueling_ddql_run,\n",
        "     dueling_ddql_exploit_run\n",
        "\n",
        "\n",
        "]\n",
        "\n",
        " # Plot averaged cumulative rewards for DQL vs Random vs DQL-Exploit vs DDQL vs DDQL-Exploit vs Dueling DDQL vs Dueling DDQL-Exploit\n",
        "themodel = dqla.CyberBattleStateActionModel(ep)\n",
        "p.plot_averaged_cummulative_rewards(\n",
        "     all_runs=all_runs,\n",
        "     title=f'Benchmark -- max_nodes={ep.maximum_node_count}, episodes={eval_episode_count},\\n'\n",
        "     f'State: {[f.name() for f in themodel.state_space.feature_selection]} '\n",
        "     f'({len(themodel.state_space.feature_selection)}\\n'\n",
        "     f\"Action: abstract_action ({themodel.action_space.flat_size()})\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "55e7ac4a-d08e-4017-ace6-02139e4982bd",
      "metadata": {
        "id": "55e7ac4a-d08e-4017-ace6-02139e4982bd"
      },
      "outputs": [],
      "source": [
        "# Compare and plot results for all the agents\n",
        "all_runs = [\n",
        "    random_run,\n",
        "    credlookup_run,\n",
        "    tabularq_run,\n",
        "    tabularq_exploit_run,\n",
        "    dql_run,\n",
        "    dql_exploit_run,\n",
        "    ddql_run,\n",
        "    ddql_exploit_run,\n",
        "    dueling_dql_run,\n",
        "    dueling_dql_exploit_run,\n",
        "    dueling_ddql_run,\n",
        "    dueling_ddql_exploit_run\n",
        "\n",
        "]\n",
        "\n",
        " # Plot averaged cumulative rewards for DQL vs Random vs DQL-Exploit vs DDQL vs DDQL-Exploit vs Dueling DQL vs Dueling DQL-Exploit vs Dueling DDQL vs Dueling DDQL-Exploit\n",
        "themodel = dqla.CyberBattleStateActionModel(ep)\n",
        "p.plot_averaged_cummulative_rewards(\n",
        "    all_runs=all_runs,\n",
        "    title=f'Benchmark -- max_nodes={ep.maximum_node_count}, episodes={eval_episode_count},\\n'\n",
        "          f'State: {[f.name() for f in themodel.state_space.feature_selection]} '\n",
        "          f'({len(themodel.state_space.feature_selection)}\\n'\n",
        "          f\"Action: abstract_action ({themodel.action_space.flat_size()})\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7708e1a8-f41a-49db-a031-d8c730bc9b31",
      "metadata": {
        "id": "7708e1a8-f41a-49db-a031-d8c730bc9b31"
      },
      "outputs": [],
      "source": [
        "# Compare and plot results for all the agents\n",
        "all_runs = [\n",
        "     random_run,\n",
        "     credlookup_run,\n",
        "     tabularq_run,\n",
        "     tabularq_exploit_run,\n",
        "     dql_run,\n",
        "     dql_exploit_run\n",
        " ]\n",
        "#\n",
        "# Plot averaged cumulative rewards for DQL vs Random vs DQL-Exploit\n",
        "themodel = dqla.CyberBattleStateActionModel(ep)\n",
        "p.plot_averaged_cummulative_rewards(\n",
        "    all_runs=all_runs,\n",
        "     title=f'Benchmark -- max_nodes={ep.maximum_node_count}, episodes={eval_episode_count},\\n'\n",
        "     f'State: {[f.name() for f in themodel.state_space.feature_selection]} '\n",
        "     f'({len(themodel.state_space.feature_selection)}\\n'\n",
        "     f\"Action: abstract_action ({themodel.action_space.flat_size()})\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5c0e024d-6258-415b-a910-ef42fa78f46a",
      "metadata": {
        "id": "5c0e024d-6258-415b-a910-ef42fa78f46a"
      },
      "outputs": [],
      "source": [
        "contenders = [\n",
        "    random_run,\n",
        "    credlookup_run,\n",
        "    tabularq_run,\n",
        "    tabularq_exploit_run,\n",
        "    dql_run,\n",
        "    dql_exploit_run,\n",
        "    ddql_run,\n",
        "    ddql_exploit_run,\n",
        "    dueling_dql_run,\n",
        "    dueling_dql_exploit_run,\n",
        "    dueling_ddql_run,\n",
        "    dueling_ddql_exploit_run\n",
        "]\n",
        "p.plot_episodes_length(contenders)\n",
        "p.plot_averaged_cummulative_rewards(\n",
        "    title=f'Agent Benchmark top contenders\\n'\n",
        "          f'max_nodes:{ep.maximum_node_count}\\n',\n",
        "    all_runs=contenders)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4799cb4e-e8e4-419a-acc9-83c705146ce6",
      "metadata": {
        "id": "4799cb4e-e8e4-419a-acc9-83c705146ce6"
      },
      "outputs": [],
      "source": [
        "# Plot cumulative rewards for all episodes\n",
        "for r in contenders:\n",
        "    p.plot_all_episodes(r)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "050092bd-ea89-4ace-9e3c-fe05b5125c1c",
      "metadata": {
        "id": "050092bd-ea89-4ace-9e3c-fe05b5125c1c"
      },
      "outputs": [],
      "source": [
        "contenders2 = [\n",
        "     random_run,\n",
        "     #     credlookup_run,\n",
        "     #     tabularq_run,\n",
        "     #     tabularq_exploit_run,\n",
        "     dql_run,\n",
        "     # dql_exploit_run,\n",
        "     ddql_run,\n",
        "     # ddql_exploit_run,\n",
        "     dueling_dql_run,\n",
        "     # dueling_dql_exploit_run,\n",
        "     dueling_ddql_run,\n",
        "     # dueling_ddql_exploit_run\n",
        " ]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b235f2a6-989d-46c3-bfb1-0e4dd352677d",
      "metadata": {
        "id": "b235f2a6-989d-46c3-bfb1-0e4dd352677d"
      },
      "outputs": [],
      "source": [
        "p.plot_episodes_length(contenders2)\n",
        "p.plot_averaged_cummulative_rewards(\n",
        "     title=f'Agent Benchmark top contenders\\n'\n",
        "           f'max_nodes:{ep.maximum_node_count}\\n',\n",
        "     all_runs=contenders2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3b9bd4a9-7238-48ab-aa30-a962f3c3085b",
      "metadata": {
        "id": "3b9bd4a9-7238-48ab-aa30-a962f3c3085b"
      },
      "outputs": [],
      "source": [
        "contenders3 = [\n",
        "    # random_run,\n",
        "     #     credlookup_run,\n",
        "     #     tabularq_run,\n",
        "     #     tabularq_exploit_run,\n",
        "     # dql_run,\n",
        "     dql_exploit_run,\n",
        "     # ddql_run,\n",
        "     ddql_exploit_run,\n",
        "     # dueling_dql_run,\n",
        "     dueling_dql_exploit_run,\n",
        "     # dueling_ddql_run,\n",
        "     dueling_ddql_exploit_run\n",
        " ]\n",
        "p.plot_episodes_length(contenders3)\n",
        "p.plot_averaged_cummulative_rewards(\n",
        "     title=f'Agent Benchmark top contenders\\n'\n",
        "           f'max_nodes:{ep.maximum_node_count}\\n',\n",
        "     all_runs=contenders3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5b2a9c7d-38a0-4f79-b444-52f8c0de8782",
      "metadata": {
        "id": "5b2a9c7d-38a0-4f79-b444-52f8c0de8782"
      },
      "outputs": [],
      "source": [
        "contenders1 = [\n",
        "    random_run,\n",
        "    #     credlookup_run,\n",
        "    #     tabularq_run,\n",
        "    #     tabularq_exploit_run,\n",
        "    dql_run,\n",
        "    dql_exploit_run,\n",
        "    ddql_run,\n",
        "    ddql_exploit_run,\n",
        "    dueling_dql_run,\n",
        "    dueling_dql_exploit_run,\n",
        "    dueling_ddql_run,\n",
        "    dueling_ddql_exploit_run\n",
        "]\n",
        "p.plot_episodes_length(contenders1)\n",
        "p.plot_averaged_cummulative_rewards(\n",
        "    title=f'Agent Benchmark top contenders\\n'\n",
        "          f'max_nodes:{ep.maximum_node_count}\\n',\n",
        "    all_runs=contenders1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "46788324-6d47-43bb-a759-c86dde6ecc53",
      "metadata": {
        "id": "46788324-6d47-43bb-a759-c86dde6ecc53"
      },
      "outputs": [],
      "source": [
        "# Plot cumulative rewards for all episodes\n",
        "for r in contenders1:\n",
        "    p.plot_all_episodes(r)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5247f540-a79b-4aab-8f69-6b8bcaa8a210",
      "metadata": {
        "id": "5247f540-a79b-4aab-8f69-6b8bcaa8a210"
      },
      "outputs": [],
      "source": [
        "contenders2 = [\n",
        "    random_run,\n",
        "    credlookup_run,\n",
        "    tabularq_run,\n",
        "    #     tabularq_exploit_run,\n",
        "    dql_run,\n",
        "    # dql_exploit_run,\n",
        "    ddql_run,\n",
        "    # ddql_exploit_run,\n",
        "    dueling_dql_run,\n",
        "    # dueling_dql_exploit_run,\n",
        "    dueling_ddql_run,\n",
        "    # dueling_ddql_exploit_run\n",
        "]\n",
        "p.plot_episodes_length(contenders2)\n",
        "p.plot_averaged_cummulative_rewards(\n",
        "    title=f'Agent Benchmark top contenders\\n'\n",
        "          f'max_nodes:{ep.maximum_node_count}\\n',\n",
        "    all_runs=contenders2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b371c831-6001-409e-9f1b-bf3046e45f54",
      "metadata": {
        "id": "b371c831-6001-409e-9f1b-bf3046e45f54"
      },
      "outputs": [],
      "source": [
        "contenders3 = [\n",
        "    random_run,\n",
        "    #     credlookup_run,\n",
        "    #     tabularq_run,\n",
        "    tabularq_exploit_run,\n",
        "    # dql_run,\n",
        "    dql_exploit_run,\n",
        "    # ddql_run,\n",
        "    ddql_exploit_run,\n",
        "    # dueling_dql_run,\n",
        "    dueling_dql_exploit_run,\n",
        "    # dueling_ddql_run,\n",
        "    dueling_ddql_exploit_run\n",
        "]\n",
        "p.plot_episodes_length(contenders3)\n",
        "p.plot_averaged_cummulative_rewards(\n",
        "    title=f'Agent Benchmark top contenders\\n'\n",
        "          f'max_nodes:{ep.maximum_node_count}\\n',\n",
        "    all_runs=contenders3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4c94858d-5c33-4d93-83b2-c966451da3e5",
      "metadata": {
        "id": "4c94858d-5c33-4d93-83b2-c966451da3e5"
      },
      "outputs": [],
      "source": [
        "contenders3v2    = [\n",
        "    # random_run,\n",
        "    #     credlookup_run,\n",
        "    #     tabularq_run,\n",
        "    tabularq_exploit_run,\n",
        "    # dql_run,\n",
        "    dql_exploit_run,\n",
        "    # ddql_run,\n",
        "    ddql_exploit_run,\n",
        "    # dueling_dql_run,\n",
        "    dueling_dql_exploit_run,\n",
        "    # dueling_ddql_run,\n",
        "    dueling_ddql_exploit_run\n",
        "]\n",
        "p.plot_episodes_length(contenders3v2)\n",
        "p.plot_averaged_cummulative_rewards(\n",
        "    title=f'Agent Benchmark top contenders\\n'\n",
        "          f'max_nodes:{ep.maximum_node_count}\\n',\n",
        "    all_runs=contenders3v2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1eb97552-2531-4391-a85c-4ec1e24b1f95",
      "metadata": {
        "id": "1eb97552-2531-4391-a85c-4ec1e24b1f95"
      },
      "outputs": [],
      "source": [
        "contenders3v3   = [\n",
        "    # random_run,\n",
        "    #     credlookup_run,\n",
        "    #     tabularq_run,\n",
        "    # tabularq_exploit_run,\n",
        "    # dql_run,\n",
        "    dql_exploit_run,\n",
        "    # ddql_run,\n",
        "    ddql_exploit_run,\n",
        "    # dueling_dql_run,\n",
        "    dueling_dql_exploit_run,\n",
        "    # dueling_ddql_run,\n",
        "    dueling_ddql_exploit_run\n",
        "]\n",
        "p.plot_episodes_length(contenders3v3)\n",
        "p.plot_averaged_cummulative_rewards(\n",
        "    title=f'Agent Benchmark top contenders\\n'\n",
        "          f'max_nodes:{ep.maximum_node_count}\\n',\n",
        "    all_runs=contenders3v3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "591014ea-38ba-4b75-9913-125d80554caa",
      "metadata": {
        "id": "591014ea-38ba-4b75-9913-125d80554caa"
      },
      "outputs": [],
      "source": [
        "contenders4 = [\n",
        "    # random_run,\n",
        "    credlookup_run,\n",
        "    tabularq_run,\n",
        "    # tabularq_exploit_run,\n",
        "    # dql_run,\n",
        "    # dql_exploit_run,\n",
        "    ddql_run,\n",
        "    # ddql_exploit_run,\n",
        "    dueling_dql_run,\n",
        "    # dueling_dql_exploit_run,\n",
        "    dueling_ddql_run,\n",
        "    # dueling_ddql_exploit_run\n",
        "]\n",
        "p.plot_episodes_length(contenders4)\n",
        "p.plot_averaged_cummulative_rewards(\n",
        "    title=f'Agent Benchmark top contenders\\n'\n",
        "          f'max_nodes:{ep.maximum_node_count}\\n',\n",
        "    all_runs=contenders4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4a05fa15-48a8-4984-b687-ff241232ba20",
      "metadata": {
        "id": "4a05fa15-48a8-4984-b687-ff241232ba20"
      },
      "outputs": [],
      "source": [
        "contenders5 = [\n",
        "    # random_run,\n",
        "    #     credlookup_run,\n",
        "    #     tabularq_run,\n",
        "    tabularq_exploit_run,\n",
        "    # dql_run,\n",
        "    # dql_exploit_run,\n",
        "    # ddql_run,\n",
        "    ddql_exploit_run,\n",
        "    # dueling_dql_run,\n",
        "    dueling_dql_exploit_run,\n",
        "    # dueling_ddql_run,\n",
        "    dueling_ddql_exploit_run\n",
        "]\n",
        "p.plot_episodes_length(contenders5)\n",
        "p.plot_averaged_cummulative_rewards(\n",
        "    title=f'Agent Benchmark top contenders\\n'\n",
        "          f'max_nodes:{ep.maximum_node_count}\\n',\n",
        "    all_runs=contenders5)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python [conda env:cybersim] *",
      "language": "python",
      "name": "conda-env-cybersim-py"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.18"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}