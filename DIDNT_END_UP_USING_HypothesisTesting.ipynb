{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7a3d0aed-c02e-4060-ae75-2128846b0f2a",
   "metadata": {},
   "source": [
    "# Shared Environment Version 2\n",
    "\n",
    "all the defender code set up has been moved to a file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d279a38-a94f-4f3e-a4c0-c5004bf8d79a",
   "metadata": {},
   "source": [
    "Hypothesis "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d76a8a5b-00e7-4223-94f2-8089618c7ceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import copy\n",
    "import logging\n",
    "import networkx\n",
    "from networkx import convert_matrix\n",
    "from typing import NamedTuple, Optional, Tuple, List, Dict, TypeVar, TypedDict, cast\n",
    "\n",
    "import numpy\n",
    "import gym\n",
    "from gym import spaces\n",
    "from gym.utils import seeding\n",
    "\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "from cyberbattle._env.shared_cyberbattle_env import EnvironmentBounds, AttackerGoal, DefenderGoal, DefenderConstraint\n",
    "from cyberbattle._env.defender import DefenderAgent,ScanAndReimageCompromisedMachines\n",
    "from cyberbattle.simulation.model import PortName, PrivilegeLevel\n",
    "from cyberbattle.simulation import commandcontrol, model, actions\n",
    "from cyberbattle._env.discriminatedunion import DiscriminatedUnion\n",
    "from cyberbattle.agents.baseline import agent_wrapper as w\n",
    "import cyberbattle.agents.baseline.learner as learner\n",
    "from cyberbattle.agents.baseline.learner import Learner\n",
    "import cyberbattle.agents.baseline.agent_tabularqlearning as a\n",
    "import cyberbattle.agents.baseline.agent_ddql as ddqla\n",
    "import cyberbattle.agents.baseline.agent_dql as dqla"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d882dd4-d52a-42ce-85e3-8fb814f8c45d",
   "metadata": {},
   "source": [
    "# DefenderWrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19322fe6-6b9c-42b0-92eb-e9d44b011bf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "from gym import spaces\n",
    "\n",
    "class DefenderWrapper:\n",
    "    def __init__(self, environment, name):\n",
    "        self.env = environment\n",
    "        self.network = environment.network\n",
    "        self.defender_actions = DefenderAgentActions(environment)\n",
    "        self.id_mapping = self.create_id_mapping()\n",
    "        self.action_space = self.define_action_space()\n",
    "        #self.observation_space = self.define_observation_space()\n",
    "        self.last_attacker_reward = 0\n",
    "        self.name = name\n",
    "        self.performance_metrics = {\n",
    "            'actions_taken': 0,\n",
    "            'successful_actions': 0,\n",
    "            'failed_actions': 0,\n",
    "            'unnecessary':0\n",
    "        }\n",
    "\n",
    "    def create_id_mapping(self):\n",
    "        mapping = {}\n",
    "        for i, node in enumerate(self.env.network.nodes):\n",
    "            mapping[i] = node\n",
    "        return mapping\n",
    "\n",
    "    def define_action_space(self):\n",
    "        return spaces.Dict({\n",
    "            'action_type': spaces.Discrete(5),\n",
    "            'node_id': spaces.Discrete(len(self.id_mapping)),\n",
    "            'parameter': spaces.Discrete(10)\n",
    "        })\n",
    "    \n",
    "    def define_observation_space(self):\n",
    "        return spaces.Dict({\n",
    "            'infected_nodes': spaces.Box(low=0, high=1, shape=(len(self.id_mapping),), dtype=np.int32),\n",
    "            'firewall_status': spaces.Box(low=0, high=1, shape=(len(self.id_mapping),), dtype=np.int32),\n",
    "            'service_status': spaces.Box(low=0, high=1, shape=(len(self.id_mapping),), dtype=np.int32)\n",
    "        })\n",
    "    \n",
    "    def validate_action(self, action):\n",
    "        action_type, numerical_node_id, _ = action.values()\n",
    "        print(action_type,numerical_node_id,_)\n",
    "        node_id = self.id_mapping.get(numerical_node_id)\n",
    "        action1 = self.create_action(action_type, node_id)\n",
    "        action_type1, numerical_node_id, _ = action.values()\n",
    "        if node_id is None or action1['action_type'] not in [0, 1, 2, 3, 4]:\n",
    "            return False\n",
    "        return True\n",
    "\n",
    "    def step(self, action):\n",
    "        print(f\"[DEBUG] Received action: {action}\")\n",
    "        \n",
    "        if not self.validate_action(action):\n",
    "            return self.construct_step_response(-10, {'error': 'Invalid action'}, False)\n",
    "    \n",
    "        action_type, numerical_node_id, parameter = action.values()\n",
    "        node_id = self.id_mapping[numerical_node_id]\n",
    "        action1 = self.create_action(action_type, node_id)\n",
    "        action_result = self.perform_action(action_type, node_id, parameter)\n",
    "        \n",
    "        if action_result is None:\n",
    "            return self.construct_step_response(-10, {'error': 'Action result is None'}, False)\n",
    "    \n",
    "        reward = self.calculate_reward(action_result)\n",
    "        done = self.defender_goal_reached()\n",
    "        self.update_performance_metrics(action_result)\n",
    "        \n",
    "        response = self.construct_step_response(reward, action_result, done)\n",
    "        return response\n",
    "\n",
    "    def perform_action(self, action_type, node_id, parameter):\n",
    "        node_info = self.env.get_node(node_id)\n",
    "        if action_type == 0:  # Reimage Node\n",
    "            return self.defender_actions.reimage_node(node_id, self.env)\n",
    "        \n",
    "        elif action_type == 1:  # Block Traffic\n",
    "            return self.defender_actions.block_traffic(node_id, parameter, incoming=True)\n",
    "        \n",
    "        elif action_type == 2:  # Allow Traffic\n",
    "            return self.defender_actions.allow_traffic(node_id, parameter, incoming=True)\n",
    "        \n",
    "        elif action_type == 3:  # Stop Service\n",
    "            if any(service.name == parameter and service.running for service in node_info.services):\n",
    "                return self.defender_actions.stop_service(node_id, parameter)\n",
    "            else:\n",
    "                return {'status': 'unnecessary', 'reason': 'Service not running or not found', 'action': 'stop_service'}\n",
    "        \n",
    "        elif action_type == 4:  # Start Service\n",
    "            if any(service.name == parameter and not service.running for service in node_info.services):\n",
    "                return self.defender_actions.start_service(node_id, parameter)\n",
    "            else:\n",
    "                return {'status': 'unnecessary', 'reason': 'Service already running or not found', 'action': 'start_service'}\n",
    "        \n",
    "        else:\n",
    "            return {'status': 'error', 'reason': 'Unknown action type', 'action': 'unknown'}\n",
    "\n",
    "\n",
    "    def calculate_reward(self, action_result):\n",
    "      if action_result['status'] == 'success':\n",
    "          if action_result['action'] == 'reimage_node':\n",
    "              return 50  #higher reward for re-imaging a compromised node\n",
    "          else:\n",
    "              return 25  #standard reward for other successful actions\n",
    "      elif action_result['status'] == 'failed':\n",
    "          if action_result['action'] == 'reimage_node':\n",
    "              return 0\n",
    "          else:\n",
    "              return -15  #penalty for failed actions\n",
    "      elif action_result['status'] == 'unnecessary':\n",
    "          return -5  #smaller penalty for unnecessary actions\n",
    "      return 0  #default reward for other cases\n",
    "\n",
    "    def get_observation(self):\n",
    "      infected_nodes = [int(self.env.is_node_infected(node)) for node in self.env.network.nodes]\n",
    "      firewall_status = [int(self.env.get_firewall_status(node)) for node in self.env.network.nodes]\n",
    "      service_status = [int(self.env.get_service_status(node)) for node in self.env.network.nodes]\n",
    "      return {\n",
    "          'infected_nodes': np.array(infected_nodes, dtype=np.int32),\n",
    "          'firewall_status': np.array(firewall_status, dtype=np.int32),\n",
    "          'service_status': np.array(service_status, dtype=np.int32)\n",
    "      }\n",
    "\n",
    "\n",
    "    def update_performance_metrics(self, action_result):\n",
    "        self.performance_metrics['actions_taken'] += 1\n",
    "        if action_result['status'] == 'success':\n",
    "            self.performance_metrics['successful_actions'] += 1\n",
    "        elif action_result['status'] == 'failed':\n",
    "            self.performance_metrics['failed_actions'] += 1\n",
    "        else:\n",
    "            self.performance_metrics['unnecessary'] += 1\n",
    "        \n",
    "\n",
    "    def construct_step_response(self, reward, info, done):\n",
    "        observation = self.get_observation()\n",
    "        observation['action_mask'] = self.compute_defender_action_mask()\n",
    "        info['performance_metrics'] = self.performance_metrics\n",
    "        network_availability = self.env.network_availability\n",
    "        info['network_availability'] = network_availability\n",
    "\n",
    "        return observation, reward, done, info\n",
    "\n",
    "\n",
    "    def compute_defender_action_mask(self):\n",
    "        action_mask = {\n",
    "            'Reimage Node': numpy.zeros(len(self.id_mapping), dtype=numpy.int32),\n",
    "            'Block Traffic': numpy.zeros(len(self.id_mapping), dtype=numpy.int32),\n",
    "            'Allow Traffic': numpy.zeros(len(self.id_mapping), dtype=numpy.int32),\n",
    "            'Stop Service': numpy.zeros(len(self.id_mapping), dtype=numpy.int32),\n",
    "            'Start Service': numpy.zeros(len(self.id_mapping), dtype=numpy.int32)\n",
    "        }\n",
    "        #update the action mask based on the current state of the environment\n",
    "        for numerical_id, node_id in self.id_mapping.items():\n",
    "            node_info = self.env.network.nodes[node_id]['data']\n",
    "            #if node is infected or has vulnerabilities, defender can reimage node\n",
    "            if node_info.agent_installed or self.has_vulnerabilities(node_id):\n",
    "                action_mask['Reimage Node'][numerical_id] = 1\n",
    "                #print(f\"Node {node_id} can be reimaged (Infected or Vulnerable)\")\n",
    "    \n",
    "            #defender can block traffic and allow traffic on any node\n",
    "            action_mask['Block Traffic'][numerical_id] = 1\n",
    "            action_mask['Allow Traffic'][numerical_id] = 1\n",
    "            #print(f\"Node {node_id} can have traffic blocked/allowed\")\n",
    "    \n",
    "            #if a service is running on the node, defender can stop service\n",
    "            if any(service.running for service in node_info.services):\n",
    "                action_mask['Stop Service'][numerical_id] = 1\n",
    "                #print(f\"Node {node_id} has running services that can be stopped\")\n",
    "    \n",
    "            #if a service is stopped on the node, defender can start service\n",
    "            if any(not service.running for service in node_info.services):\n",
    "                action_mask['Start Service'][numerical_id] = 1\n",
    "                #print(f\"Node {node_id} has stopped services that can be started\")\n",
    "    \n",
    "        return action_mask\n",
    "    \n",
    "    def update_last_attacker_reward(self, reward):\n",
    "        self.last_attacker_reward = reward\n",
    "\n",
    "\n",
    "    def default_or_preventive_action(self):\n",
    "        random_node_id = random.choice(list(self.id_mapping.keys()))\n",
    "        return self.create_action('Check Firewall', random_node_id)\n",
    "\n",
    "    def defender_goal_reached(self):\n",
    "        for _, node_id in self.id_mapping.items():\n",
    "            node_info = self.env.network.nodes[node_id]['data']\n",
    "            if node_info.agent_installed:  \n",
    "                return False\n",
    "        return True\n",
    "\n",
    "\n",
    "    def reset(self):\n",
    "        initial_observation = self.env.reset()\n",
    "        return initial_observation\n",
    "    \n",
    "    def reimage_node(self, node_id, environment):\n",
    "        #mark the node for re-imaging\n",
    "        self.node_reimaging_progress[node_id] = self.REIMAGING_DURATION\n",
    "        return self.defender_actions.reimage_node(node_id, environment)\n",
    "\n",
    "    \n",
    "    def get_node(self, node_id):\n",
    "        return self.env.get_node(node_id)\n",
    "\n",
    "    def choose_action(self, state):\n",
    "        \"\"\"\n",
    "        Choose an action based on the current state of the environment.\n",
    "        \"\"\"\n",
    "        print(\"Choosing action based on the current state\")\n",
    "\n",
    "        #Detect infected nodes\n",
    "        infected_nodes = self.find_infected_nodes()\n",
    "        #print('infected nodes',infected_nodes)\n",
    "        #print(f\"Infected nodes: {infected_nodes}\")\n",
    "        '''\n",
    "        #If there are infected nodes, reimage the first one\n",
    "        if infected_nodes:\n",
    "            #node_id_to_reimage = infected_nodes[0]\n",
    "            node_id_to_reimage = random.choice(infected_nodes)\n",
    "            #print(f\"Reimaging node: {node_id_to_reimage}\")\n",
    "            return self.create_action('Reimage Node', node_id_to_reimage)\n",
    "        '''\n",
    "        #List of preventive actions\n",
    "        preventive_actions = ['Reimage Node','Check Firewall', 'Block Traffic', 'Allow Traffic', 'Stop Service', 'Start Service']\n",
    "        #print(\"No infected nodes found, selecting a preventive action\")\n",
    "        #Randomly select a preventive action\n",
    "        selected_preventive_action = random.choice(preventive_actions)\n",
    "        #print(f\"Selected preventive action: {selected_preventive_action}\")\n",
    "        #Randomly select a node for the preventive action\n",
    "        node_id_for_preventive_action = self.select_node_for_preventive_action()\n",
    "        return self.create_action(selected_preventive_action, node_id_for_preventive_action)\n",
    "\n",
    "    def find_infected_nodes(self):\n",
    "        \"\"\"\n",
    "        Find and return a list of numerical IDs of infected (or compromised) nodes.\n",
    "        \"\"\"\n",
    "        infected_nodes = []\n",
    "        for numerical_id, node_id in self.id_mapping.items():\n",
    "            node_info = self.env.network.nodes[node_id]['data']\n",
    "            if node_info.agent_installed or self.has_vulnerabilities(node_id):\n",
    "                infected_nodes.append(numerical_id)\n",
    "        return infected_nodes\n",
    "\n",
    "    def has_vulnerabilities(self, node_id):\n",
    "        \"\"\"\n",
    "        Check if a node has vulnerabilities.\n",
    "        \"\"\"\n",
    "        node_info = self.env.network.nodes[node_id]['data']\n",
    "        return bool(node_info.vulnerabilities)\n",
    "\n",
    "    def select_node_for_preventive_action(self):\n",
    "        \"\"\"\n",
    "        Select a node for a preventive action\n",
    "        \"\"\"\n",
    "        return random.choice(list(self.id_mapping.keys()))\n",
    "\n",
    "    def create_action(self, action_type, node_id):\n",
    "        \"\"\"\n",
    "        Create an action dictionary based on the action type and node ID.\n",
    "        \"\"\"\n",
    "        action_type_mapping = {\n",
    "            'Reimage Node': 0,\n",
    "            'Block Traffic': 1,\n",
    "            'Allow Traffic': 2,\n",
    "            'Stop Service': 3,\n",
    "            'Start Service': 4\n",
    "        }\n",
    "        if action_type in action_type_mapping:\n",
    "            action_number = action_type_mapping[action_type]\n",
    "        else:\n",
    "            action_number = random.choice(list(action_type_mapping.values()))\n",
    "        return {'action_type': action_number, 'node_id': node_id, 'parameter': 0}\n",
    "\n",
    "    def sample_valid_action(self):\n",
    "        \"\"\"\n",
    "        Randomly selects a valid action for the defender.\n",
    "        \"\"\"\n",
    "        action_type = random.randint(0, 4) \n",
    "        node_id = random.choice(list(self.id_mapping.keys()))\n",
    "        parameter = random.randint(0, 9)  \n",
    "        action = {\n",
    "            'action_type': action_type,\n",
    "            'node_id': node_id,\n",
    "            'parameter': parameter\n",
    "        }\n",
    "        while not self.validate_action(action):\n",
    "            action['action_type'] = random.randint(0, 4)\n",
    "            action['node_id'] = random.choice(list(self.id_mapping.keys()))\n",
    "            action['parameter'] = random.randint(0, 9)\n",
    "\n",
    "        return action\n",
    "\n",
    "    def render(self):\n",
    "        print(\"Rendering the current state of the environment\")\n",
    "\n",
    "    def close(self):\n",
    "        pass\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d4d2d2d-dcb2-42a9-88f0-78332ad664de",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "from gym import spaces\n",
    "\n",
    "class DefenderWrapper:\n",
    "    def __init__(self, environment, name):\n",
    "        self.env = environment\n",
    "        self.network = environment.network\n",
    "        self.defender_actions = DefenderAgentActions(environment)\n",
    "        self.id_mapping = self.create_id_mapping()\n",
    "        self.action_space = self.define_action_space()\n",
    "        #self.observation_space = self.define_observation_space()\n",
    "        self.last_attacker_reward = 0\n",
    "        self.name = name\n",
    "        self.performance_metrics = {\n",
    "            'actions_taken': 0,\n",
    "            'successful_actions': 0,\n",
    "            'failed_actions': 0\n",
    "        }\n",
    "\n",
    "    def create_id_mapping(self):\n",
    "        mapping = {}\n",
    "        for i, node in enumerate(self.env.network.nodes):\n",
    "            mapping[i] = node\n",
    "        return mapping\n",
    "\n",
    "    def define_action_space(self):\n",
    "        return spaces.Dict({\n",
    "            'action_type': spaces.Discrete(5),\n",
    "            'node_id': spaces.Discrete(len(self.id_mapping)),\n",
    "            'parameter': spaces.Discrete(10)\n",
    "        })\n",
    "    \n",
    "    def define_observation_space(self):\n",
    "        return spaces.Dict({\n",
    "            'infected_nodes': spaces.Box(low=0, high=1, shape=(len(self.id_mapping),), dtype=np.int32),\n",
    "            'firewall_status': spaces.Box(low=0, high=1, shape=(len(self.id_mapping),), dtype=np.int32),\n",
    "            'service_status': spaces.Box(low=0, high=1, shape=(len(self.id_mapping),), dtype=np.int32)\n",
    "        })\n",
    "    \n",
    "    def validate_action(self, action):\n",
    "        action_type, numerical_node_id, _ = action.values()\n",
    "        print(action_type,numerical_node_id,_)\n",
    "        node_id = self.id_mapping.get(numerical_node_id)\n",
    "        action1 = self.create_action(action_type, node_id)\n",
    "        #print('new action1',action1)\n",
    "        action_type1, numerical_node_id, _ = action.values()\n",
    "        #print('action type1',action1['action_type'])\n",
    "        #print('node id',node_id)\n",
    "        #print('numerical_node_id',numerical_node_id)\n",
    "        if node_id is None or action1['action_type'] not in [0, 1, 2, 3, 4]:\n",
    "            #print(node_id)\n",
    "            #print(action_type)\n",
    "            return False\n",
    "        return True\n",
    "\n",
    "    def step(self, action):\n",
    "        print(f\"[DEBUG] Received action: {action}\")\n",
    "        \n",
    "        if not self.validate_action(action):\n",
    "            #print(\"[WARNING] Invalid action\")\n",
    "            return self.construct_step_response(-10, {'error': 'Invalid action'}, False)\n",
    "    \n",
    "        action_type, numerical_node_id, parameter = action.values()\n",
    "        node_id = self.id_mapping[numerical_node_id]\n",
    "        #print(f\"[DEBUG] Processed Node ID: {node_id} (Numerical ID: {numerical_node_id})\")\n",
    "        action1 = self.create_action(action_type, node_id)\n",
    "        action_result = self.perform_action(action_type, node_id, parameter)\n",
    "        \n",
    "        if action_result is None:\n",
    "            #print(\"[ERROR] Action result is None\")\n",
    "            return self.construct_step_response(-10, {'error': 'Action result is None'}, False)\n",
    "    \n",
    "        reward = self.calculate_reward(action_result)\n",
    "        done = self.defender_goal_reached()\n",
    "        self.update_performance_metrics(action_result)\n",
    "        \n",
    "        response = self.construct_step_response(reward, action_result, done)\n",
    "        #print(f\"[DEBUG] Step response: {response}\")\n",
    "        return response\n",
    "\n",
    "    def perform_action(self, action_type, node_id, parameter):\n",
    "        # Get the current state of the node\n",
    "        node_info = self.env.get_node(node_id)\n",
    "    \n",
    "        if action_type == 0:  # Reimage Node\n",
    "            return self.defender_actions.reimage_node(node_id, self.env)\n",
    "        \n",
    "        elif action_type == 1:  # Block Traffic\n",
    "            return self.defender_actions.block_traffic(node_id, parameter, incoming=True)\n",
    "        \n",
    "        elif action_type == 2:  # Allow Traffic\n",
    "            return self.defender_actions.allow_traffic(node_id, parameter, incoming=True)\n",
    "        \n",
    "        elif action_type == 3:  # Stop Service\n",
    "            if any(service.name == parameter and service.running for service in node_info.services):\n",
    "                return self.defender_actions.stop_service(node_id, parameter)\n",
    "            else:\n",
    "                return {'status': 'unnecessary', 'reason': 'Service not running or not found', 'action': 'stop_service'}\n",
    "        \n",
    "        elif action_type == 4:  # Start Service\n",
    "            if any(service.name == parameter and not service.running for service in node_info.services):\n",
    "                return self.defender_actions.start_service(node_id, parameter)\n",
    "            else:\n",
    "                return {'status': 'unnecessary', 'reason': 'Service already running or not found', 'action': 'start_service'}\n",
    "        \n",
    "        else:\n",
    "            return {'status': 'error', 'reason': 'Unknown action type', 'action': 'unknown'}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def calculate_reward(self, action_result):\n",
    "      if action_result['status'] == 'success':\n",
    "          if action_result['action'] == 'reimage_node':\n",
    "              return 50  #Higher reward for re-imaging a compromised node\n",
    "          else:\n",
    "              return 25  #Standard reward for other successful actions\n",
    "      elif action_result['status'] == 'failed':\n",
    "          #print('failed')\n",
    "          #print(action_result)\n",
    "          if action_result['action'] == 'reimage_node':\n",
    "              #print('reimaged')\n",
    "              return 0\n",
    "          else:\n",
    "              #print('not reimaged')\n",
    "              return -15  #Penalty for failed actions\n",
    "      elif action_result['status'] == 'unnecessary':\n",
    "          return -5  #Smaller penalty for unnecessary actions\n",
    "      return 0  #Default reward for other cases\n",
    "\n",
    "\n",
    "    def get_observation(self):\n",
    "      #print('environment nodes',self.env.network.nodes)\n",
    "      infected_nodes = [int(self.env.is_node_infected(node)) for node in self.env.network.nodes]\n",
    "        \n",
    "      #print('infected_nodes',infected_nodes)\n",
    "      firewall_status = [int(self.env.get_firewall_status(node)) for node in self.env.network.nodes]\n",
    "      service_status = [int(self.env.get_service_status(node)) for node in self.env.network.nodes]\n",
    "      #print('firewall_status',firewall_status)\n",
    "      #print('service_status',service_status)\n",
    "      return {\n",
    "          'infected_nodes': np.array(infected_nodes, dtype=np.int32),\n",
    "          'firewall_status': np.array(firewall_status, dtype=np.int32),\n",
    "          'service_status': np.array(service_status, dtype=np.int32)\n",
    "      }\n",
    "\n",
    "\n",
    "    def update_performance_metrics(self, action_result):\n",
    "        self.performance_metrics['actions_taken'] += 1\n",
    "        if action_result['status'] == 'success':\n",
    "            self.performance_metrics['successful_actions'] += 1\n",
    "        else:\n",
    "            self.performance_metrics['failed_actions'] += 1\n",
    "\n",
    "    def construct_step_response(self, reward, info, done):\n",
    "        observation = self.get_observation()\n",
    "        observation['action_mask'] = self.compute_defender_action_mask()\n",
    "        info['performance_metrics'] = self.performance_metrics\n",
    "        network_availability = self.env.network_availability\n",
    "        info['network_availability'] = network_availability\n",
    "\n",
    "        return observation, reward, done, info\n",
    "\n",
    "\n",
    "    def compute_defender_action_mask(self):\n",
    "        #initialise a blank action mask\n",
    "        action_mask = {\n",
    "            'Reimage Node': numpy.zeros(len(self.id_mapping), dtype=numpy.int32),\n",
    "            'Block Traffic': numpy.zeros(len(self.id_mapping), dtype=numpy.int32),\n",
    "            'Allow Traffic': numpy.zeros(len(self.id_mapping), dtype=numpy.int32),\n",
    "            'Stop Service': numpy.zeros(len(self.id_mapping), dtype=numpy.int32),\n",
    "            'Start Service': numpy.zeros(len(self.id_mapping), dtype=numpy.int32)\n",
    "        }\n",
    "\n",
    "        # Update the action mask based on the current state of the environment\n",
    "        for numerical_id, node_id in self.id_mapping.items():\n",
    "            node_info = self.env.network.nodes[node_id]['data']\n",
    "    \n",
    "            #print(f\"Node ID: {node_id}, Info: {node_info}\")\n",
    "    \n",
    "            # If node is infected or has vulnerabilities, defender can reimage node\n",
    "            if node_info.agent_installed or self.has_vulnerabilities(node_id):\n",
    "                action_mask['Reimage Node'][numerical_id] = 1\n",
    "                #print(f\"Node {node_id} can be reimaged (Infected or Vulnerable)\")\n",
    "    \n",
    "            # Defender can block traffic and allow traffic on any node\n",
    "            action_mask['Block Traffic'][numerical_id] = 1\n",
    "            action_mask['Allow Traffic'][numerical_id] = 1\n",
    "            #print(f\"Node {node_id} can have traffic blocked/allowed\")\n",
    "    \n",
    "            # If a service is running on the node, defender can stop service\n",
    "            if any(service.running for service in node_info.services):\n",
    "                action_mask['Stop Service'][numerical_id] = 1\n",
    "                #print(f\"Node {node_id} has running services that can be stopped\")\n",
    "    \n",
    "            # If a service is stopped on the node, defender can start service\n",
    "            if any(not service.running for service in node_info.services):\n",
    "                action_mask['Start Service'][numerical_id] = 1\n",
    "                #print(f\"Node {node_id} has stopped services that can be started\")\n",
    "    \n",
    "        return action_mask\n",
    "    \n",
    "    def update_last_attacker_reward(self, reward):\n",
    "        self.last_attacker_reward = reward\n",
    "\n",
    "\n",
    "    def default_or_preventive_action(self):\n",
    "        random_node_id = random.choice(list(self.id_mapping.keys()))\n",
    "        return self.create_action('Check Firewall', random_node_id)\n",
    "\n",
    "    def defender_goal_reached(self):\n",
    "        for _, node_id in self.id_mapping.items():\n",
    "            node_info = self.env.network.nodes[node_id]['data']\n",
    "            if node_info.agent_installed:  \n",
    "                return False\n",
    "        return True\n",
    "\n",
    "\n",
    "    def reset(self):\n",
    "        initial_observation = self.env.reset()\n",
    "        return initial_observation\n",
    "    \n",
    "    def reimage_node(self, node_id, environment):\n",
    "        # Mark the node for re-imaging\n",
    "        self.node_reimaging_progress[node_id] = self.REIMAGING_DURATION\n",
    "    \n",
    "        # Now, update the node's state in the environment\n",
    "        return self.defender_actions.reimage_node(node_id, environment)\n",
    "\n",
    "    \n",
    "    def get_node(self, node_id):\n",
    "        #implement this method to access node information from the environment\n",
    "        return self.env.get_node(node_id)\n",
    "\n",
    "    def choose_action(self, state):\n",
    "        \"\"\"\n",
    "        Choose an action based on the current state of the environment.\n",
    "        \"\"\"\n",
    "        print(\"Choosing action based on the current state\")\n",
    "\n",
    "        #Detect infected nodes\n",
    "        infected_nodes = self.find_infected_nodes()\n",
    "        #print('infected nodes',infected_nodes)\n",
    "        #print(f\"Infected nodes: {infected_nodes}\")\n",
    "        '''\n",
    "        #If there are infected nodes, reimage the first one\n",
    "        if infected_nodes:\n",
    "            #node_id_to_reimage = infected_nodes[0]\n",
    "            node_id_to_reimage = random.choice(infected_nodes)\n",
    "            #print(f\"Reimaging node: {node_id_to_reimage}\")\n",
    "            return self.create_action('Reimage Node', node_id_to_reimage)\n",
    "        '''\n",
    "        #List of preventive actions\n",
    "        preventive_actions = ['Reimage Node','Check Firewall', 'Block Traffic', 'Allow Traffic', 'Stop Service', 'Start Service']\n",
    "        #print(\"No infected nodes found, selecting a preventive action\")\n",
    "\n",
    "        #Randomly select a preventive action\n",
    "        selected_preventive_action = random.choice(preventive_actions)\n",
    "        #print(f\"Selected preventive action: {selected_preventive_action}\")\n",
    "\n",
    "        #Randomly select a node for the preventive action\n",
    "        node_id_for_preventive_action = self.select_node_for_preventive_action()\n",
    "        #print(f\"Node selected for preventive action: {node_id_for_preventive_action}\")\n",
    "\n",
    "        return self.create_action(selected_preventive_action, node_id_for_preventive_action)\n",
    "\n",
    "    def find_infected_nodes(self):\n",
    "        \"\"\"\n",
    "        Find and return a list of numerical IDs of infected (or compromised) nodes.\n",
    "        \"\"\"\n",
    "        infected_nodes = []\n",
    "        for numerical_id, node_id in self.id_mapping.items():\n",
    "            node_info = self.env.network.nodes[node_id]['data']\n",
    "            if node_info.agent_installed or self.has_vulnerabilities(node_id):\n",
    "                infected_nodes.append(numerical_id)\n",
    "        return infected_nodes\n",
    "\n",
    "    def has_vulnerabilities(self, node_id):\n",
    "        \"\"\"\n",
    "        Check if a node has vulnerabilities.\n",
    "        \"\"\"\n",
    "        node_info = self.env.network.nodes[node_id]['data']\n",
    "        return bool(node_info.vulnerabilities)\n",
    "\n",
    "    def select_node_for_preventive_action(self):\n",
    "        \"\"\"\n",
    "        Select a node for a preventive action\n",
    "        \"\"\"\n",
    "        return random.choice(list(self.id_mapping.keys()))\n",
    "\n",
    "    def create_action(self, action_type, node_id):\n",
    "        \"\"\"\n",
    "        Create an action dictionary based on the action type and node ID.\n",
    "        \"\"\"\n",
    "        action_type_mapping = {\n",
    "            'Reimage Node': 0,\n",
    "            'Block Traffic': 1,\n",
    "            'Allow Traffic': 2,\n",
    "            'Stop Service': 3,\n",
    "            'Start Service': 4\n",
    "        }\n",
    "        if action_type in action_type_mapping:\n",
    "            action_number = action_type_mapping[action_type]\n",
    "        else:\n",
    "            action_number = random.choice(list(action_type_mapping.values()))\n",
    "        return {'action_type': action_number, 'node_id': node_id, 'parameter': 0}\n",
    "\n",
    "    def sample_valid_action(self):\n",
    "        \"\"\"\n",
    "        Randomly selects a valid action for the defender.\n",
    "        \"\"\"\n",
    "        action_type = random.randint(0, 4) \n",
    "        node_id = random.choice(list(self.id_mapping.keys()))\n",
    "        parameter = random.randint(0, 9)  \n",
    "        action = {\n",
    "            'action_type': action_type,\n",
    "            'node_id': node_id,\n",
    "            'parameter': parameter\n",
    "        }\n",
    "        while not self.validate_action(action):\n",
    "            action['action_type'] = random.randint(0, 4)\n",
    "            action['node_id'] = random.choice(list(self.id_mapping.keys()))\n",
    "            action['parameter'] = random.randint(0, 9)\n",
    "\n",
    "        return action\n",
    "\n",
    "    def render(self):\n",
    "        #implements rendering logic here\n",
    "        print(\"Rendering the current state of the environment\")\n",
    "\n",
    "    def close(self):\n",
    "        pass\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "880a196d-7336-403f-b8da-2662c86a3f46",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict\n",
    "import datetime\n",
    "from cyberbattle.simulation import model\n",
    "from cyberbattle.simulation.model import PrivilegeLevel, MachineStatus\n",
    "from cyberbattle.simulation.model import Identifiers, NodeID, NodeInfo, Environment\n",
    "from cyberbattle._env.cyberbattle_chain import CyberBattleChain\n",
    "\n",
    "class DefenderAgentActions:\n",
    "    \"\"\"Actions reserved for defender agents\"\"\"\n",
    "\n",
    "    #number of steps it takes to completely reimage a node\n",
    "    REIMAGING_DURATION = 15\n",
    "\n",
    "    def __init__(self, environment: CyberBattleChain):\n",
    "        #map nodes being reimaged to the remaining number of steps to completion\n",
    "        self.node_reimaging_progress: Dict[model.NodeID, int] = dict()\n",
    "        #last calculated availability of the network\n",
    "        self.__network_availability: float = 1.0\n",
    "        self._environment = environment\n",
    "\n",
    "    @property\n",
    "    def network_availability(self):\n",
    "        return self.__network_availability\n",
    "\n",
    "    def print_initial_node_states(self):\n",
    "        print(\"Initial node states:\")\n",
    "        for node_id in self._environment.network.nodes:\n",
    "            node_data = self._environment.get_node(node_id)\n",
    "            print(f\"Node {node_id}: {node_data}\")\n",
    "\n",
    "    def reimage_node(self, node_id: model.NodeID, environment: model.Environment):\n",
    "        \"\"\"Re-image a computer node\"\"\"\n",
    "        node_info = environment.get_node(node_id)\n",
    "        action_result = {}\n",
    "    \n",
    "        # Check if the agent is installed - Precondition for reimaging\n",
    "        if not node_info.agent_installed:\n",
    "            return {\"action\": \"reimage_node\", \"status\": \"unnecessary\", \"node_id\": node_id, \"reason\": \"Reimaging not required - no agent installed\"}\n",
    "    \n",
    "        # Proceed with reimaging\n",
    "        if node_info.status != MachineStatus.Imaging:\n",
    "            node_info.agent_installed = False\n",
    "            node_info.privilege_level = PrivilegeLevel.NoAccess\n",
    "            node_info.status = MachineStatus.Imaging\n",
    "            node_info.last_reimaging = datetime.datetime.now()\n",
    "            action_result = {\"action\": \"reimage_node\", \"status\": \"success\", \"node_id\": node_id}\n",
    "        else:\n",
    "            action_result = {\"action\": \"reimage_node\", \"status\": \"unnecessary\", \"node_id\": node_id}\n",
    "    \n",
    "        return action_result\n",
    "\n",
    "\n",
    "    def on_attacker_step_taken(self):\n",
    "        \"\"\"Function to be called each time a step is taken in the simulation\"\"\"\n",
    "        for node_id in list(self.node_reimaging_progress.keys()):\n",
    "            remaining_steps = self.node_reimaging_progress[node_id]\n",
    "            if remaining_steps > 0:\n",
    "                self.node_reimaging_progress[node_id] -= 1\n",
    "            else:\n",
    "                print(f\"Machine re-imaging completed: {node_id}\")\n",
    "                node_data = self._environment.get_node(node_id)\n",
    "                node_data.status = MachineStatus.Running\n",
    "                self.node_reimaging_progress.pop(node_id)\n",
    "\n",
    "        #calculate the network availability metric based on machines and services that are running\n",
    "        total_node_weights = 0\n",
    "        network_node_availability = 0\n",
    "        for node_id, node_info in self._environment._cyberbattle_env.__environment.nodes():\n",
    "            total_service_weights = 0\n",
    "            running_service_weights = 0\n",
    "            if isinstance(node_info, ChainNodeInfo):\n",
    "                for service in node_info.services:\n",
    "                    total_service_weights += service.sla_weight\n",
    "                    running_service_weights += service.sla_weight * int(service.running)\n",
    "\n",
    "                if node_info.status == MachineStatus.Running:\n",
    "                    adjusted_node_availability = (1 + running_service_weights) / (\n",
    "                        1 + total_service_weights\n",
    "                    )\n",
    "                else:\n",
    "                    adjusted_node_availability = 0.0\n",
    "\n",
    "                total_node_weights += node_info.sla_weight\n",
    "                network_node_availability += (\n",
    "                    adjusted_node_availability * node_info.sla_weight\n",
    "                )\n",
    "\n",
    "        self.__network_availability = network_node_availability / total_node_weights\n",
    "        assert self.__network_availability <= 1.0 and self.__network_availability >= 0.0\n",
    "\n",
    "    def override_firewall_rule(\n",
    "            self,\n",
    "            node_id: model.NodeID,\n",
    "            port_name: model.PortName,\n",
    "            incoming: bool,\n",
    "            permission: model.RulePermission,\n",
    "        ):\n",
    "            node_data = self._environment.get_node(node_id)\n",
    "\n",
    "            def add_or_patch_rule(rules) -> List[FirewallRule]:\n",
    "                new_rules = []\n",
    "                has_matching_rule = False\n",
    "                for r in rules:\n",
    "                    if r.port == port_name:\n",
    "                        has_matching_rule = True\n",
    "                        new_rules.append(FirewallRule(r.port, permission))\n",
    "                    else:\n",
    "                        new_rules.append(r)\n",
    "\n",
    "                if not has_matching_rule:\n",
    "                    new_rules.append(model.FirewallRule(port_name, permission))\n",
    "                return new_rules\n",
    "\n",
    "            if incoming:\n",
    "                node_data.firewall.incoming = add_or_patch_rule(node_data.firewall.incoming)\n",
    "            else:\n",
    "                node_data.firewall.outgoing = add_or_patch_rule(node_data.firewall.outgoing)\n",
    "\n",
    "\n",
    "    #blocks network traffic on specific port of node, if node is running, the traffic on the node can be blocked\n",
    "    #failure is is machine not running\n",
    "    def block_traffic(self, node_id: model.NodeID, port_name: model.PortName, incoming: bool):\n",
    "        node_data = self._environment.get_node(node_id)\n",
    "        if node_data.status == MachineStatus.Running:\n",
    "            self.override_firewall_rule(node_id, port_name, incoming, model.RulePermission.BLOCK)\n",
    "            action_result = {\"action\": \"block_traffic\", \"status\": \"success\", \"node_id\": node_id, \"port_name\": port_name}\n",
    "        else:\n",
    "            action_result = {\"action\": \"block_traffic\", \"status\": \"failed\", \"node_id\": node_id, \"port_name\": port_name, \"reason\": \"Machine not running\"}\n",
    "        return action_result\n",
    "\n",
    "    #allows network traffic on specific port of node, if node is running, the traffic on the node is allowed\n",
    "    #failure is is machine not running\n",
    "    def allow_traffic(self, node_id: model.NodeID, port_name: model.PortName, incoming: bool):\n",
    "        node_data = self._environment.get_node(node_id)\n",
    "        if node_data.status == MachineStatus.Running:\n",
    "            self.override_firewall_rule(node_id, port_name, incoming, model.RulePermission.ALLOW)\n",
    "            action_result = {\"action\": \"allow_traffic\", \"status\": \"success\", \"node_id\": node_id, \"port_name\": port_name}\n",
    "        else:\n",
    "            action_result = {\"action\": \"allow_traffic\", \"status\": \"failed\", \"node_id\": node_id, \"port_name\": port_name, \"reason\": \"Machine not running\"}\n",
    "        return action_result\n",
    "\n",
    "    #stops service on specific node, if node is running and the specified service is running, it can be stopped successfully\n",
    "    #failure is is if machine is not running and service is not found\n",
    "    def stop_service(self, node_id: model.NodeID, port_name: model.PortName):\n",
    "      \"\"\" Stop a service on a given node \"\"\"\n",
    "      node_data = self._environment.get_node(node_id)\n",
    "      action_result = {}\n",
    "      if node_data.status == MachineStatus.Running:\n",
    "          service_found = False\n",
    "          for service in node_data.services:\n",
    "              if service.name == port_name:\n",
    "                  if service.running:\n",
    "                      service.running = False\n",
    "                      service_found = True\n",
    "                      action_result = {\"action\": \"stop_service\", \"status\": \"success\", \"node_id\": node_id, \"port_name\": port_name}\n",
    "                      break\n",
    "                  else:\n",
    "                      action_result = {\"action\": \"stop_service\", \"status\": \"unnecessary\", \"node_id\": node_id, \"port_name\": port_name, \"reason\": \"Service already stopped\"}\n",
    "                      break\n",
    "          if not service_found:\n",
    "              action_result = {\"action\": \"stop_service\", \"status\": \"failed\", \"node_id\": node_id, \"port_name\": port_name, \"reason\": \"Service not found\"}\n",
    "      else:\n",
    "          action_result = {\"action\": \"stop_service\", \"status\": \"failed\", \"node_id\": node_id, \"port_name\": port_name, \"reason\": \"Machine not running\"}\n",
    "      return action_result\n",
    "\n",
    "    #starts service on specific node\n",
    "    #success if specified service is not running and can be started\n",
    "    #unneccesary = service already running\n",
    "    #failure = machine not running and srvice not running\n",
    "    def start_service(self, node_id: model.NodeID, port_name: model.PortName):\n",
    "        \"\"\" Start a service on a given node \"\"\"\n",
    "        node_data = self._environment.get_node(node_id)\n",
    "        action_result = {}\n",
    "        if node_data.status == MachineStatus.Running:\n",
    "            service_found = False\n",
    "            for service in node_data.services:\n",
    "                if service.name == port_name:\n",
    "                    if not service.running:\n",
    "                        service.running = True\n",
    "                        service_found = True\n",
    "                        action_result = {\"action\": \"start_service\", \"status\": \"success\", \"node_id\": node_id, \"port_name\": port_name}\n",
    "                        break\n",
    "                    else:\n",
    "                        action_result = {\"action\": \"start_service\", \"status\": \"unnecessary\", \"node_id\": node_id, \"port_name\": port_name, \"reason\": \"Service already running\"}\n",
    "                        break\n",
    "            if not service_found:\n",
    "                action_result = {\"action\": \"start_service\", \"status\": \"failed\", \"node_id\": node_id, \"port_name\": port_name, \"reason\": \"Service not found\"}\n",
    "        else:\n",
    "            action_result = {\"action\": \"start_service\", \"status\": \"failed\", \"node_id\": node_id, \"port_name\": port_name, \"reason\": \"Machine not running\"}\n",
    "        return action_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1c942fe-636b-416d-a4fd-559256c7ffe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fa3073f-1a3f-44c2-8288-baed45d33b35",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "\n",
    "# Create an instance of the CyberBattleChain environment without the defender\n",
    "gym_env = gym.make('CyberBattleChain-v0',\n",
    "    size=12,\n",
    "    attacker_goal=AttackerGoal(\n",
    "        own_atleast=0,\n",
    "        own_atleast_percent=1.0\n",
    "    ),\n",
    "    defender_constraint=DefenderConstraint(\n",
    "        maintain_sla=0.80\n",
    "    )\n",
    ")\n",
    "\n",
    "defender_name = \"DefenderName\"  \n",
    "defender_agent = DefenderWrapper(gym_env,defender_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a0d8694-9748-4df3-aee2-c616564c92fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "ep = w.EnvironmentBounds.of_identifiers(\n",
    "    maximum_total_credentials=22,\n",
    "    maximum_node_count=22,\n",
    "    identifiers=gym_env.identifiers\n",
    ")\n",
    "#cyberbattlechain_defender.render_as_fig()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5301519e-1932-4767-892e-aa08870f838e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "random.seed(120394016)\n",
    "%matplotlib inline\n",
    "import cyberbattle.agents.baseline.agent_tabularqlearning as a\n",
    "import cyberbattle.agents.baseline.learner as learner\n",
    "from cyberbattle.agents.baseline.agent_wrapper import Verbosity\n",
    "#ql\n",
    "iteration_count = 10\n",
    "training_episode_count = 10\n",
    "eval_episode_count = 20\n",
    "gamma_sweep = [\n",
    "    0.015,  # about right\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "649df427-7988-4935-a550-3fd01722762d",
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_attacker = learner.epsilon_greedy_search(\n",
    "        gym_env,\n",
    "        ep,\n",
    "        a.QTabularLearner(ep, gamma=.015, learning_rate=0.99, exploit_percentile=100),\n",
    "        episode_count=50,\n",
    "        iteration_count=150,\n",
    "        epsilon=0.95,\n",
    "        render=False,\n",
    "        epsilon_multdecay=0.75,  # 0.999,\n",
    "        epsilon_exponential_decay=5000,\n",
    "        epsilon_minimum=0.1,\n",
    "        verbosity=Verbosity.Quiet,\n",
    "        title=\"Q-learning\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c096027-c0fc-495a-ac30-d680b38ef591",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from cyberbattle._env.cyberbattle_env import Action\n",
    "\n",
    "class EpsilonGreedyLearner:\n",
    "    def __init__(self, env, defender, epsilon=0.1):\n",
    "        self.env = env\n",
    "        self.defender = defender\n",
    "        self.epsilon = epsilon\n",
    "\n",
    "    def choose_action(self, state):\n",
    "        if np.random.rand() <= self.epsilon:\n",
    "            return self.explore()\n",
    "        else:\n",
    "            return self.exploit(state)\n",
    "\n",
    "    def explore(self):\n",
    "        #randomly select an action from the defender's valid actions\n",
    "        return self.defender.sample_valid_action()\n",
    "\n",
    "    def exploit(self, state):\n",
    "        return self.defender.sample_valid_action()\n",
    "\n",
    "    def update_state(self, state, reward):\n",
    "        pass\n",
    "\n",
    "    def end_of_episode(self, episode_number):\n",
    "        pass\n",
    "\n",
    "class TrainedDefender:\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0efaec45-d1e5-4e23-b1a9-e8d74ca63c5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from typing import Tuple, Optional\n",
    "import logging\n",
    "import abc\n",
    "from cyberbattle._env import cyberbattle_env\n",
    "\n",
    "class DefenderEpsilonGreedyLearner(EpsilonGreedyLearner):\n",
    "    def __init__(self, env, defender, ep, gamma, learning_rate, epsilon, state_space_size, action_space_size):\n",
    "        super().__init__(env, defender, epsilon)\n",
    "        self.defender = defender\n",
    "        self.state_space_size = state_space_size\n",
    "        self.action_space_size = action_space_size\n",
    "        self.learning_rate = learning_rate\n",
    "        self.q_matrix = np.zeros((self.state_space_size, self.action_space_size))\n",
    "        self.gamma = gamma\n",
    "\n",
    "\n",
    "    def decide_action(self, wrapped_env, observation) -> Tuple[str, cyberbattle_env.Action, object]:\n",
    "        if np.random.rand() < self.epsilon:\n",
    "            return self.explore(wrapped_env)\n",
    "        else:\n",
    "            return self.exploit(wrapped_env, observation)\n",
    "\n",
    "    def explore(self, wrapped_env) -> Tuple[str, cyberbattle_env.Action, object]:\n",
    "        action = wrapped_env.sample_valid_action()\n",
    "        action_metadata = None\n",
    "        action_style = 'explore'\n",
    "        return action_style, action, action_metadata\n",
    "\n",
    "    def exploit(self, wrapped_env, observation) -> Tuple[str, Optional[cyberbattle_env.Action], object]:\n",
    "        action = wrapped_env.sample_valid_action()\n",
    "        action_metadata = None\n",
    "        action_style = 'exploit'\n",
    "        return action_style, action, action_metadata\n",
    "\n",
    "    def on_step(self, wrapped_env, observation, reward, done, info, action_metadata):\n",
    "        current_state = self.encode_state(observation)\n",
    "        action = self.encode_action(action_metadata)\n",
    "        next_state = self.encode_state(observation) if not done else None\n",
    "        self.update_q_matrix(current_state, action, reward, next_state)\n",
    "\n",
    "    def update_q_matrix(self, state, action, reward, next_state):\n",
    "        max_future_q = 0 if next_state is None else np.max(self.q_matrix[next_state])\n",
    "        current_q = self.q_matrix[state, action]\n",
    "        new_q = (1 - self.learning_rate) * current_q + self.learning_rate * (reward + self.gamma * max_future_q)\n",
    "        self.q_matrix[state, action] = new_q\n",
    "\n",
    "    def parameters_as_string(self):\n",
    "        return \"Defender learner parameters: \"\n",
    "\n",
    "    def encode_state(self, observation):\n",
    "        state_index = 0\n",
    "        return state_index\n",
    "\n",
    "    def encode_action(self, action_metadata):\n",
    "        action_index = 0\n",
    "        return action_index\n",
    "\n",
    "    def new_episode(self):\n",
    "        pass\n",
    "\n",
    "    def end_of_episode(self, i_episode, t):\n",
    "        pass\n",
    "\n",
    "    def end_of_iteration(self, t, done):\n",
    "        pass\n",
    "\n",
    "    def all_parameters_as_string(self):\n",
    "        return ''\n",
    "\n",
    "    def loss_as_string(self):\n",
    "        return ''\n",
    "\n",
    "    def stateaction_as_string(self, action_metadata):\n",
    "        return ''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e01927bf-de07-4ed8-aa30-1c1986c34ff0",
   "metadata": {},
   "outputs": [],
   "source": [
    "Breakdown = TypedDict('Breakdown', {\n",
    "    'local': int,\n",
    "    'remote': int,\n",
    "    'connect': int\n",
    "})\n",
    "\n",
    "Outcomes = TypedDict('Outcomes', {\n",
    "    'reward': Breakdown,\n",
    "    'noreward': Breakdown\n",
    "})\n",
    "\n",
    "Stats = TypedDict('Stats', { \n",
    "    'exploit': Outcomes,\n",
    "    'explore': Outcomes,\n",
    "    'exploit_deflected_to_explore': int\n",
    "})\n",
    "\n",
    "TrainedLearner = TypedDict('TrainedLearner', {\n",
    "    'all_episodes_rewards': List[List[float]],\n",
    "    'all_episodes_availability': List[List[float]],\n",
    "    'learner': Learner,\n",
    "    'trained_on': str,\n",
    "    'title': str\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "058c9ae7-c6d3-401d-9034-b2d4aeaf20f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import sys\n",
    "import math\n",
    "import logging\n",
    "import progressbar\n",
    "from typing import Optional, List\n",
    "from collections import defaultdict\n",
    "def plot_exploration_exploitation(exploration_count, exploitation_count, episode_count):\n",
    "    episodes = np.arange(1, episode_count + 1)\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(episodes, exploration_count, label='Exploration Actions', marker='o')\n",
    "    plt.plot(episodes, exploitation_count, label='Exploitation Actions', marker='x')\n",
    "    \n",
    "    plt.title('Exploration vs Exploitation Actions Over Episodes')\n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Action Count')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "def epsilon_greedy_defender_training(\n",
    "    cyberbattle_gym_env: cyberbattle_env.CyberBattleEnv,\n",
    "    environment_properties: w.EnvironmentBounds,\n",
    "    learner: DefenderEpsilonGreedyLearner,\n",
    "    title: str,\n",
    "    episode_count: int,\n",
    "    iteration_count: int,\n",
    "    epsilon: float,\n",
    "    epsilon_minimum=0.0,\n",
    "    epsilon_multdecay: Optional[float] = None,\n",
    "    epsilon_exponential_decay: Optional[int] = None,\n",
    "    render=True,\n",
    "    render_last_episode_rewards_to: Optional[str] = None,\n",
    "    verbosity: Verbosity = Verbosity.Normal,\n",
    "    plot_episodes_length=True\n",
    ") -> TrainedLearner:\n",
    "    \"\"\"Epsilon greedy search for CyberBattle gym environments\n",
    "\n",
    "    Parameters\n",
    "    ==========\n",
    "\n",
    "    - cyberbattle_gym_env -- the CyberBattle environment to train on\n",
    "\n",
    "    - learner --- the policy learner/exploiter\n",
    "\n",
    "    - episode_count -- Number of training episodes\n",
    "\n",
    "    - iteration_count -- Maximum number of iterations in each episode\n",
    "\n",
    "    - epsilon -- explore vs exploit\n",
    "        - 0.0 to exploit the learnt policy only without exploration\n",
    "        - 1.0 to explore purely randomly\n",
    "\n",
    "    - epsilon_minimum -- epsilon decay clipped at this value.\n",
    "    Setting this value too close to 0 may leed the search to get stuck.\n",
    "\n",
    "    - epsilon_decay -- epsilon gets multiplied by this value after each episode\n",
    "\n",
    "    - epsilon_exponential_decay - if set use exponential decay. The bigger the value\n",
    "    is, the slower it takes to get from the initial `epsilon` to `epsilon_minimum`.\n",
    "\n",
    "    - verbosity -- verbosity of the `print` logging\n",
    "\n",
    "    - render -- render the environment interactively after each episode\n",
    "\n",
    "    - render_last_episode_rewards_to -- render the environment to the specified file path\n",
    "    with an index appended to it each time there is a positive reward\n",
    "    for the last episode only\n",
    "\n",
    "    - plot_episodes_length -- Plot the graph showing total number of steps by episode\n",
    "    at th end of the search.\n",
    "\n",
    "    Note on convergence\n",
    "    ===================\n",
    "\n",
    "    Setting 'minimum_espilon' to 0 with an exponential decay <1\n",
    "    makes the learning converge quickly (loss function getting to 0),\n",
    "    but that's just a forced convergence, however, since when\n",
    "    epsilon approaches 0, only the q-values that were explored so\n",
    "    far get updated and so only that subset of cells from\n",
    "    the Q-matrix converges.\n",
    "\n",
    "    \"\"\"\n",
    "    exploration_count = [] #new\n",
    "    exploitation_count = [] #new\n",
    "    #epsilon = epsilon\n",
    "    epsilon_values = []  # To track epsilon values over episodes\n",
    "    \n",
    "    print(f\"###### {title}\\n\"\n",
    "          f\"Learning with: episode_count={episode_count},\"\n",
    "          f\"iteration_count={iteration_count},\"\n",
    "          f\"={epsilon},\"\n",
    "          f'_min={epsilon_minimum}, '\n",
    "          + (f\"_multdecay={epsilon_multdecay},\" if epsilon_multdecay else '')\n",
    "          + (f\"_expdecay={epsilon_exponential_decay},\" if epsilon_exponential_decay else '') +\n",
    "          f\"{learner.parameters_as_string()}\")\n",
    "\n",
    "    initial_epsilon = epsilon\n",
    "    successful_reimaging_count = 0\n",
    "    starting_services_count = 0\n",
    "    allowing_traffic_count = 0\n",
    "    blocking_traffic_count = 0\n",
    "    stopping_services_count = 0\n",
    "    all_episodes_rewards = []\n",
    "    all_episodes_availability = []\n",
    "    defender_name = 'MyDefenderEnv'\n",
    "    wrapped_env = DefenderWrapper(cyberbattle_gym_env,defender_name)\n",
    "    steps_done = 0\n",
    "    plot_title = f\"{title} (epochs={episode_count}, ={initial_epsilon}, _min={epsilon_minimum},\" \\\n",
    "        + (f\"_multdecay={epsilon_multdecay},\" if epsilon_multdecay else '') \\\n",
    "        + (f\"_expdecay={epsilon_exponential_decay},\" if epsilon_exponential_decay else '') \\\n",
    "        + learner.parameters_as_string()\n",
    "    plottraining = PlotTraining(title=plot_title, render_each_episode=render)\n",
    "\n",
    "    render_file_index = 1\n",
    "\n",
    "    for i_episode in range(1, episode_count + 1):\n",
    "        exploration_actions = 0 #new\n",
    "        exploitation_actions = 0 #new\n",
    "        epsilon_values.append(epsilon)\n",
    "        print(f\"  ## Episode: {i_episode}/{episode_count} '{title}' \"\n",
    "              f\"={epsilon:.4f}, \"\n",
    "              f\"{learner.parameters_as_string()}\")\n",
    "\n",
    "        observation = wrapped_env.reset()\n",
    "        total_reward = 0.0\n",
    "        all_rewards = []\n",
    "        all_availability = []\n",
    "        learner.new_episode()\n",
    "\n",
    "        stats = Stats(exploit=Outcomes(reward=Breakdown(local=0, remote=0, connect=0),\n",
    "                                       noreward=Breakdown(local=0, remote=0, connect=0)),\n",
    "                      explore=Outcomes(reward=Breakdown(local=0, remote=0, connect=0),\n",
    "                                       noreward=Breakdown(local=0, remote=0, connect=0)),\n",
    "                      exploit_deflected_to_explore=0\n",
    "                      )\n",
    "\n",
    "        episode_ended_at = None\n",
    "        sys.stdout.flush()\n",
    "\n",
    "        bar = progressbar.ProgressBar(\n",
    "            widgets=[\n",
    "                'Episode ',\n",
    "                f'{i_episode}',\n",
    "                '|Iteration ',\n",
    "                progressbar.Counter(),\n",
    "                '|',\n",
    "                progressbar.Variable(name='reward', width=6, precision=10),\n",
    "                '|',\n",
    "                progressbar.Variable(name='last_reward_at', width=4),\n",
    "                '|',\n",
    "                progressbar.Timer(),\n",
    "                progressbar.Bar()\n",
    "            ],\n",
    "            redirect_stdout=False)\n",
    "\n",
    "        for t in bar(range(1, 1 + iteration_count)):\n",
    "\n",
    "            if epsilon_exponential_decay:\n",
    "                epsilon = epsilon_minimum + math.exp(-1. * steps_done /\n",
    "                                                     epsilon_exponential_decay) * (initial_epsilon - epsilon_minimum)\n",
    "\n",
    "            steps_done += 1\n",
    "\n",
    "            x = np.random.rand()\n",
    "            if x <= epsilon:\n",
    "                action_style, gym_action, action_metadata = learner.explore(wrapped_env)\n",
    "                exploration_actions += 1 #new\n",
    "            else:\n",
    "                action_style, gym_action, action_metadata = learner.exploit(wrapped_env, observation)\n",
    "                exploitation_actions += 1 #new\n",
    "                if not gym_action:\n",
    "                    stats['exploit_deflected_to_explore'] += 1\n",
    "                    _, gym_action, action_metadata = learner.explore(wrapped_env)\n",
    "                    \n",
    "            logging.debug(f\"gym_action={gym_action}, action_metadata={action_metadata}\")\n",
    "            observation, reward, done, info = wrapped_env.step(gym_action)\n",
    "\n",
    "            # Print the simplified action taken\n",
    "            action_type = gym_action.get('action_type', 'Unknown')\n",
    "            if action_type == 0:  #Reimage Node\n",
    "                action_type = \"Reimage Node\"\n",
    "            elif action_type == 1:  #Block Traffic\n",
    "                action_type = \"Block Traffic\"\n",
    "            elif action_type == 2:  #Allow Traffic\n",
    "                action_type = \"Reimage Node\"\n",
    "            elif action_type == 3:  #Stop Service\n",
    "                action_type = \"Allow Traffic\"\n",
    "            elif action_type == 4:  #Start Service\n",
    "                action_type = \"Start Service\"\n",
    "\n",
    "            print('action type: ',action_type)\n",
    "            node_id = gym_action.get('node_id', 'Unknown')\n",
    "            parameter = gym_action.get('parameter', 'Unknown')\n",
    "            print(f\"Iteration {t}: Action Taken: Type={action_type}, NodeID={node_id}, Parameter={parameter}\")\n",
    "        \n",
    "            action_status = info.get('status', '')\n",
    "\n",
    "            if action_status == 'success':\n",
    "                action_name = info.get('action', '')\n",
    "                if action_name == 'reimage_node':\n",
    "                    successful_reimaging_count += 1\n",
    "                elif action_name == 'start_service':\n",
    "                    starting_services_count += 1\n",
    "                elif action_name == 'allow_traffic':\n",
    "                    allowing_traffic_count += 1\n",
    "                elif action_name == 'block_traffic':\n",
    "                    blocking_traffic_count += 1\n",
    "                elif action_name == 'stop_service':\n",
    "                    stopping_services_count += 1\n",
    "\n",
    "            \n",
    "            print('info',info)\n",
    "\n",
    "            learner.on_step(wrapped_env, observation, reward, done, info, action_metadata)\n",
    "            assert np.shape(reward) == ()\n",
    "\n",
    "            all_rewards.append(reward)\n",
    "            if 'network_availability' in info:\n",
    "                all_availability.append(info['network_availability'])\n",
    "            else:\n",
    "                all_availability.append(0)  \n",
    "            total_reward += reward\n",
    "            bar.update(t, reward=total_reward)\n",
    "            if reward > 0:\n",
    "                bar.update(t, last_reward_at=t)\n",
    "\n",
    "            if verbosity == Verbosity.Verbose or (verbosity == Verbosity.Normal and reward > 0):\n",
    "                sign = ['-', '+'][reward > 0]\n",
    "\n",
    "                print(f\"    {sign} t={t} {action_style} r={reward} cum_reward:{total_reward} \"\n",
    "                      f\"a={action_metadata}-{gym_action} \"\n",
    "                      f\"creds={len(observation['credential_cache_matrix'])} \"\n",
    "                      f\" {learner.stateaction_as_string(action_metadata)}\")\n",
    "\n",
    "            if i_episode == episode_count \\\n",
    "                    and render_last_episode_rewards_to is not None \\\n",
    "                    and reward > 0:\n",
    "                fig = cyberbattle_gym_env.render_as_fig()\n",
    "                fig.write_image(f\"{render_last_episode_rewards_to}-e{i_episode}-{render_file_index}.png\")\n",
    "                render_file_index += 1\n",
    "\n",
    "            learner.end_of_iteration(t, done)\n",
    "            '''\n",
    "            if done:\n",
    "                episode_ended_at = t\n",
    "                bar.finish(dirty=True)\n",
    "                break\n",
    "            '''\n",
    "        sys.stdout.flush()\n",
    "\n",
    "        loss_string = learner.loss_as_string()\n",
    "        if loss_string:\n",
    "            loss_string = \"loss={loss_string}\"\n",
    "        exploration_count.append(exploration_actions) #new\n",
    "        exploitation_count.append(exploitation_actions) #new\n",
    "        if episode_ended_at:\n",
    "            print(f\"  Episode {i_episode} ended at t={episode_ended_at} {loss_string}\")\n",
    "        else:\n",
    "            print(f\"  Episode {i_episode} stopped at t={iteration_count} {loss_string}\")\n",
    "\n",
    "        print_stats(stats)\n",
    "        print(f\"  Episode {i_episode} breakdown:\")\n",
    "        print(f\"    Successful Reimaging Count: {successful_reimaging_count}\")\n",
    "        print(f\"    Starting Services Count: {starting_services_count}\")\n",
    "        print(f\"    Allowing Traffic Count: {allowing_traffic_count}\")\n",
    "        print(f\"    Blocking Traffic Count: {blocking_traffic_count}\")\n",
    "        print(f\"    Stopping Services Count: {stopping_services_count}\")\n",
    "\n",
    "        all_episodes_rewards.append(all_rewards)\n",
    "        all_episodes_availability.append(all_availability)\n",
    "\n",
    "        length = episode_ended_at if episode_ended_at else iteration_count\n",
    "        learner.end_of_episode(i_episode=i_episode, t=length)\n",
    "        if plot_episodes_length:\n",
    "            plottraining.episode_done(length)\n",
    "        if render:\n",
    "            wrapped_env.render()\n",
    "\n",
    "        if epsilon_multdecay:\n",
    "            epsilon = max(epsilon_minimum, epsilon * epsilon_multdecay)\n",
    "\n",
    "    wrapped_env.close()\n",
    "    print(\"simulation ended\")\n",
    "    if plot_episodes_length:\n",
    "        plottraining.plot_end()\n",
    "\n",
    "    plot_exploration_exploitation(exploration_count, exploitation_count, episode_count)\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(range(1, episode_count + 1), epsilon_values, label='Epsilon value')\n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Epsilon value')\n",
    "    plt.title('Epsilon Decay Over Episodes')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "    return TrainedLearner(\n",
    "        all_episodes_rewards=all_episodes_rewards,\n",
    "        all_episodes_availability=all_episodes_availability,\n",
    "        learner=learner,\n",
    "        trained_on=cyberbattle_gym_env.name,\n",
    "        title=plot_title\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b3086f2-1d41-4de4-b67d-f7e80f771942",
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_argtop_percentile(array: np.ndarray, percentile: float):\n",
    "    \"\"\"Just like `argmax` but if there are multiple elements with the max\n",
    "    return a random index to break ties instead of returning the first one.\"\"\"\n",
    "    top_percentile = np.percentile(array, percentile)\n",
    "    indices = np.where(array >= top_percentile)[0]\n",
    "    if len(indices) == 0:\n",
    "        return random_argmax(array)\n",
    "    elif indices.shape[0] > 1:\n",
    "        max_index = int(np.random.choice(indices))\n",
    "    else:\n",
    "        max_index = int(indices)\n",
    "\n",
    "    return top_percentile, max_index\n",
    "\n",
    "\n",
    "class QMatrix:\n",
    "    \"\"\"Q-Learning matrix for a given state and action space\n",
    "        state_space  - Features defining the state space\n",
    "        action_space - Features defining the action space\n",
    "        qm           - Optional: initialization values for the Q matrix\n",
    "    \"\"\"\n",
    "    # The Quality matrix\n",
    "    qm: np.ndarray\n",
    "\n",
    "    def __init__(self, name,\n",
    "                 statespace: w.Feature,\n",
    "                 actionspace: w.Feature,\n",
    "                 qm: Optional[np.ndarray] = None):\n",
    "        \"\"\"Initialize the Q-matrix\"\"\"\n",
    "\n",
    "        self.name = name\n",
    "        self.statespace = statespace\n",
    "        self.actionspace = actionspace\n",
    "        self.statedim = statespace.flat_size()\n",
    "        self.actiondim = actionspace.flat_size()\n",
    "        self.qm = self.clear() if qm is None else qm\n",
    "\n",
    "        # error calculated for the last update to the Q-matrix\n",
    "        self.last_error = 0\n",
    "\n",
    "    def shape(self):\n",
    "        return (self.statedim, self.actiondim)\n",
    "\n",
    "    def clear(self):\n",
    "        \"\"\"Re-initialize the Q-matrix to 0\"\"\"\n",
    "        self.qm = np.zeros(shape=self.shape())\n",
    "        # self.qm = np.random.rand(*self.shape()) / 100\n",
    "        return self.qm\n",
    "\n",
    "    def print(self):\n",
    "        print(f\"[{self.name}]\\n\"\n",
    "              f\"state: {self.statespace}\\n\"\n",
    "              f\"action: {self.actionspace}\\n\"\n",
    "              f\"shape = {self.shape()}\")\n",
    "\n",
    "    def update(self, current_state: int, action: int, next_state: int, reward, gamma, learning_rate):\n",
    "        \"\"\"Update the Q matrix after taking `action` in state 'current_State'\n",
    "        and obtaining reward=R[current_state, action]\"\"\"\n",
    "\n",
    "        maxq_atnext, max_index = random_argmax(self.qm[next_state, ])\n",
    "\n",
    "        # bellman equation for Q-learning\n",
    "        temporal_difference = reward + gamma * maxq_atnext - self.qm[current_state, action]\n",
    "        self.qm[current_state, action] += learning_rate * temporal_difference\n",
    "\n",
    "        # The loss is calculated using the squared difference between\n",
    "        # target Q-Value and predicted Q-Value\n",
    "        square_error = temporal_difference * temporal_difference\n",
    "        self.last_error = square_error\n",
    "\n",
    "        return self.qm[current_state, action]\n",
    "\n",
    "    def exploit(self, features, percentile) -> Tuple[int, float]:\n",
    "        \"\"\"exploit: leverage the Q-matrix.\n",
    "        Returns the expected Q value and the chosen action.\"\"\"\n",
    "        expected_q, action = random_argtop_percentile(self.qm[features, :], percentile)\n",
    "        return int(action), float(expected_q)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdec699d-2825-470b-9cc3-792dbaf9b4f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#new\n",
    "import numpy as np\n",
    "from typing import Tuple\n",
    "\n",
    "class DefenderQMatrix(QMatrix):\n",
    "    def __init__(self, statespace, actionspace, defender):\n",
    "        super().__init__('Defender Q-Matrix', statespace, actionspace)\n",
    "        self.defender = defender\n",
    "\n",
    "\n",
    "    '''\n",
    "    update the Q-matrix based on the temporal difference error. It calculates the temporal difference, updates the Q-value, and returns the updated Q-value.\n",
    "    '''\n",
    "    def update(self, current_state: int, action: int, next_state: int, reward, gamma, learning_rate):\n",
    "        \"\"\"Update the Q matrix for the defender agent\"\"\"\n",
    "        maxq_atnext, max_index = random_argmax(self.qm[next_state, ])\n",
    "\n",
    "        #customise the temporal difference calculation\n",
    "        temporal_difference = reward + gamma * maxq_atnext - self.qm[current_state, action]\n",
    "        self.qm[current_state, action] += learning_rate * temporal_difference\n",
    "\n",
    "        #customise the error calculation as needed\n",
    "        square_error = temporal_difference * temporal_difference\n",
    "        self.last_error = square_error\n",
    "\n",
    "        return self.qm[current_state, action]\n",
    "\n",
    "    '''\n",
    "    #issue here\n",
    "    def exploit(self, wrapped_env, observation):\n",
    "        test_state_index = 0\n",
    "        test_action_index = 0\n",
    "\n",
    "        if test_state_index < self.statedim and test_action_index < self.actiondim:\n",
    "            q_value = self.qm[test_state_index, test_action_index]\n",
    "            return test_action_index, q_value\n",
    "        else:\n",
    "            raise IndexError(\"Test state or action index out of bounds.\")\n",
    "    '''\n",
    "    def exploit(self, state_index):\n",
    "        # Encode the current observation into a state index\n",
    "        #state_index = self.encode_observation_to_state(observation)\n",
    "        \n",
    "        # Check if the state index is valid\n",
    "        if 0 <= state_index < self.qm.shape[0]:\n",
    "            # Find the action with the maximum Q-value for the current state\n",
    "            action_index, q_value = max(enumerate(self.qm[state_index, :]), key=lambda x: x[1])\n",
    "            return action_index, q_value\n",
    "        else:\n",
    "            # Handle invalid state index, perhaps by choosing a random action or default action\n",
    "            print(f\"State index {state_index} out of bounds. Choosing default action.\")\n",
    "            return 1, 1  # Example default action\n",
    "\n",
    "    '''\n",
    "    returns the maximum Q-value for a given state, handling terminal states by returning a default value.\n",
    "    '''\n",
    "    def get_max_q_value(self, state):\n",
    "        \"\"\"Return the maximum Q-value for a given state.\"\"\"\n",
    "        # Handle terminal state\n",
    "        if state == -1:\n",
    "            return 0\n",
    "\n",
    "        if 0 <= state < self.qm.shape[0]:\n",
    "            return np.max(self.qm[state, :])\n",
    "        else:\n",
    "            raise IndexError(f\"State index out of bounds: {state}\")\n",
    "\n",
    "    '''\n",
    "    returns the Q-value for a specific state-action pair, checking if the indices are within bounds.\n",
    "    '''\n",
    "    def get_q_value(self, state, action):\n",
    "        \"\"\"Return the Q-value for a specific state-action pair.\"\"\"\n",
    "        print(f\"Accessing Q-matrix with state={state}, action={action}\")\n",
    "        if state >= 0 and state < self.qm.shape[0] and action >= 0 and action < self.qm.shape[1]:\n",
    "            return self.qm[state, action]\n",
    "        else:\n",
    "            raise IndexError(\"State or action index out of bounds.\")\n",
    "\n",
    "    def encode_observation_to_state(self, observation): #weighting features in state_space\n",
    "        if 'firewall_status' not in observation:\n",
    "            print(\"firewall_status is missing from observation\")\n",
    "        if 'infected_nodes' not in observation:\n",
    "            print(\"infected_nodes is missing from observation\")\n",
    "        if 'service_status' not in observation:\n",
    "            print(\"service_status is missing from observation\")\n",
    "        # Define weights\n",
    "        weight_firewall = 0.5\n",
    "        weight_infected = 2\n",
    "        weight_service = 1\n",
    "        \n",
    "        # Calculate weighted counts\n",
    "        firewall_active = sum(observation['firewall_status']) * weight_firewall\n",
    "        infected_nodes = sum(observation['infected_nodes']) * weight_infected\n",
    "        services_active = sum(observation['service_status']) * weight_service\n",
    "        \n",
    "        # Combine into a single state index with adjusted weights\n",
    "        state = int(firewall_active * 10000 + infected_nodes * 100 + services_active)\n",
    "        return state\n",
    "\n",
    "    '''\n",
    "    allows for manually updating a specific state-action pair's Q-value.\n",
    "    '''\n",
    "    def update_q_value(self, state, action, new_q_value):\n",
    "        \"\"\"Update the Q-value for a specific state-action pair.\"\"\"\n",
    "        if state >= 0 and state < self.qm.shape[0] and action >= 0 and action < self.qm.shape[1]:\n",
    "            self.qm[state, action] = new_q_value\n",
    "        else:\n",
    "            raise IndexError(\"State or action index out of bounds.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02416997-2afe-4a4f-b33e-c408064ba462",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DefenderQLearner(EpsilonGreedyLearner):\n",
    "    def __init__(self, env, defender, ep, q_matrix: DefenderQMatrix, epsilon, gamma, learning_rate):\n",
    "        super().__init__(env, defender, epsilon)\n",
    "        self.defender = defender\n",
    "        self.q_matrix = q_matrix\n",
    "        self.epsilon = epsilon\n",
    "        self.gamma = gamma\n",
    "        self.learning_rate = learning_rate\n",
    "        #self.defender_actions = defender_actions\n",
    "        self.SOME_THRESHOLD = 3\n",
    "\n",
    "    def explore(self, wrapped_env) -> Tuple[str, Action, Optional[object]]:\n",
    "        action = wrapped_env.sample_valid_action()\n",
    "        print(action)\n",
    "        action_style = \"explore\"\n",
    "        action_metadata = None\n",
    "        return action_style, action, action_metadata\n",
    "\n",
    "    def convert_action_index_to_action(self, action_index, node_id, parameter):\n",
    "        \"\"\"\n",
    "        Convert an action index to an action representation.\n",
    "\n",
    "        Args:\n",
    "            action_index (int): The index of the action to be converted.\n",
    "            node_id (int): The ID of the node associated with the action.\n",
    "            parameter (int): The parameter value for the action.\n",
    "\n",
    "        Returns:\n",
    "            dict: An action representation as a dictionary.\n",
    "        \"\"\"\n",
    "        # Define a mapping from action indices to action types\n",
    "        action_mapping = {\n",
    "            0: 'Reimage Node',\n",
    "            1: 'Block Traffic',\n",
    "            2: 'Allow Traffic',\n",
    "            3: 'Stop Service',\n",
    "            4: 'Start Service'\n",
    "        }\n",
    "\n",
    "        #if the action index is valid\n",
    "        if action_index not in action_mapping:\n",
    "            raise ValueError(\"Invalid action index\")\n",
    "\n",
    "        #action dictionary\n",
    "        action_type = action_mapping[action_index]\n",
    "        action = {\n",
    "            'action_type': action_type,\n",
    "            'node_id': node_id,\n",
    "            'parameter': parameter\n",
    "        }\n",
    "\n",
    "        return action\n",
    "\n",
    "    def encode_observation_to_state(self, observation):\n",
    "        # Define weights\n",
    "        weight_firewall = 0.1  # Adjusted weight\n",
    "        weight_infected = 0.2  # Adjusted weight\n",
    "        weight_service = 0.1  # Adjusted weight\n",
    "        \n",
    "        # Normalize counts to range [0, 1] based on expected max values\n",
    "        max_firewall = max(observation['firewall_status'])\n",
    "        max_infected = len(observation['infected_nodes'])\n",
    "        max_service = len(observation['service_status'])  \n",
    "        \n",
    "        # Calculate normalized and weighted counts\n",
    "        firewall_active = (sum(observation['firewall_status']) / max_firewall) * weight_firewall\n",
    "        infected_nodes = (sum(observation['infected_nodes']) / max_infected) * weight_infected\n",
    "        services_active = (sum(observation['service_status']) / max_service) * weight_service\n",
    "        \n",
    "        # Simplified state calculation\n",
    "        state = int((firewall_active + infected_nodes + services_active) * 100)  # Adjust scaling factor\n",
    "        \n",
    "        # Debugging output\n",
    "        print(f\"firewall_active: {firewall_active}, infected_nodes: {infected_nodes}, services_active: {services_active}, calculated state: {state}\")\n",
    "        \n",
    "        # Cap state at max_state_index\n",
    "        max_state_index = 4999  # Example maximum state index\n",
    "        state = min(state, max_state_index)\n",
    "        \n",
    "        return state\n",
    "\n",
    "    def validate_action(self, action):\n",
    "        if 'action_type' not in action or 'node_id' not in action or 'parameter' not in action:\n",
    "            raise ValueError(\"Action format is incorrect\")\n",
    "            # Add any other necessary validation steps here\n",
    "\n",
    "    def exploit(self, state,observation) -> Tuple[str, Action, Optional[object]]:\n",
    "        observation = self.defender.get_observation()\n",
    "        state_index = self.encode_observation_to_state(observation)\n",
    "        \n",
    "        # If no infected nodes are detected, use choose_action to select a preventive action\n",
    "        infected_nodes = observation.get('infected_nodes', [])\n",
    "        if sum(infected_nodes) == 0:\n",
    "            print(\"No infected nodes detected. Choosing a preventive action.\")\n",
    "            preventive_action = self.defender.choose_action(observation)\n",
    "            action_style = \"exploit (preventive)\"\n",
    "            return action_style, preventive_action, None\n",
    "        else:\n",
    "            # Normal exploitation based on Q-matrix\n",
    "            action_index, _ = self.q_matrix.exploit(state_index)\n",
    "            node_id = self.extract_node_id_from_observation(observation)\n",
    "            parameter = self.determine_parameter_based_on_state(observation)\n",
    "            action = self.convert_action_index_to_action(action_index, node_id, parameter)\n",
    "            print(action)\n",
    "            action_style = \"exploit\"\n",
    "            return action_style, action, None\n",
    "    \n",
    "    def extract_node_id_from_observation(self, observation):\n",
    "        infected_nodes = observation.get('infected_nodes', [])\n",
    "        print(infected_nodes)\n",
    "        for index, is_infected in enumerate(infected_nodes):\n",
    "            if is_infected == 1:\n",
    "                return index\n",
    "        return None\n",
    "\n",
    "    def determine_parameter_based_on_state(self, observation):\n",
    "        infected_nodes = observation.get('infected_nodes', [])\n",
    "        number_of_infected_nodes = sum(infected_nodes)\n",
    "        parameter = 1 if number_of_infected_nodes > self.SOME_THRESHOLD else 0\n",
    "        return parameter\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def update_q_matrix(self, state, action, reward, next_state):\n",
    "        print(f\"Updating Q-matrix for state={state}, action={action}, next_state={next_state}\")\n",
    "        max_future_q = self.q_matrix.get_max_q_value(next_state)\n",
    "        current_q = self.q_matrix.get_q_value(state, action)\n",
    "        #calculate the new Q-value\n",
    "        new_q = (1 - self.learning_rate) * current_q + self.learning_rate * (reward + self.gamma * max_future_q)\n",
    "        #update the Q-matrix with the new Q-value\n",
    "        self.q_matrix.update_q_value(state, action, new_q)\n",
    "\n",
    "    def on_step(self, wrapped_env, action, reward, next_observation, done, info):\n",
    "        current_observation = self.defender.get_observation()\n",
    "        current_state = self.encode_observation_to_state(current_observation)\n",
    "        next_state = self.encode_observation_to_state(next_observation) if not done else -1\n",
    "        action_index = self.encode_action(action)\n",
    "        # Update Q-matrix using encoded states\n",
    "        self.update_q_matrix(current_state, action_index, reward, next_state)\n",
    "    '''\n",
    "    def on_step(self, wrapped_env, action, reward, next_observation, done, info):\n",
    "        observation = self.defender.get_observation()\n",
    "        current_state = self.q_matrix.statespace.encode(observation)\n",
    "        next_state = self.q_matrix.statespace.encode(observation) if not done else -1\n",
    "        action_index = self.encode_action(action)\n",
    "        self.update_q_matrix(current_state, action_index, reward, next_state)\n",
    "    '''\n",
    "\n",
    "    def end_of_iteration(self, current_iteration: int, total_iterations: int):\n",
    "        pass\n",
    "\n",
    "    def end_of_episode(self, i_episode=None, t=None):\n",
    "        \"\"\"\n",
    "        Modified end_of_episode method with additional parameters.\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "\n",
    "    def parameters_as_string(self):\n",
    "        return \"Defender learner parameters: []\"\n",
    "\n",
    "    def encode_state(self, observation):\n",
    "        state_index = 0\n",
    "        return state_index\n",
    "\n",
    "    def encode_action(self, action_metadata):\n",
    "        action_index = 0\n",
    "        return action_index\n",
    "\n",
    "    def new_episode(self):\n",
    "        pass\n",
    "\n",
    "    def all_parameters_as_string(self):\n",
    "        return ''\n",
    "\n",
    "    def loss_as_string(self):\n",
    "        return ''\n",
    "\n",
    "    def stateaction_as_string(self, action_metadata):\n",
    "        return ''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8acbcede-30ed-49af-8a14-adfedd131527",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_stats(stats):\n",
    "    \"\"\"Print learning statistics\"\"\"\n",
    "    def print_breakdown(stats, actiontype: str):\n",
    "        def ratio(kind: str) -> str:\n",
    "            x, y = stats[actiontype]['reward'][kind], stats[actiontype]['noreward'][kind]\n",
    "            sum = x + y\n",
    "            if sum == 0:\n",
    "                return 'NaN'\n",
    "            else:\n",
    "                return f\"{(x / sum):.2f}\"\n",
    "\n",
    "        def print_kind(kind: str):\n",
    "            print(\n",
    "                f\"    {actiontype}-{kind}: {stats[actiontype]['reward'][kind]}/{stats[actiontype]['noreward'][kind]} \"\n",
    "                f\"({ratio(kind)})\")\n",
    "        print_kind('local')\n",
    "        print_kind('remote')\n",
    "        print_kind('connect')\n",
    "\n",
    "    print(\"  Breakdown [Reward/NoReward (Success rate)]\")\n",
    "    print_breakdown(stats, 'explore')\n",
    "    print_breakdown(stats, 'exploit')\n",
    "    print(f\"  exploit deflected to exploration: {stats['exploit_deflected_to_explore']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "663463cf-7ca9-4b29-8c9f-347c4cb5a784",
   "metadata": {},
   "outputs": [],
   "source": [
    "from cyberbattle.agents.baseline.agent_wrapper import ActionTrackingStateAugmentation, StateAugmentation\n",
    "from cyberbattle.agents.baseline.plotting import PlotTraining, plot_averaged_cummulative_rewards\n",
    "import progressbar\n",
    "import math\n",
    "from cyberbattle.agents.baseline import learner\n",
    "import sys\n",
    "from cyberbattle.simulation.model import FirewallRule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deb426c6-3962-4b28-875e-cea58b3f2a12",
   "metadata": {},
   "outputs": [],
   "source": [
    "statespace = w.HashEncoding(ep, [\n",
    "            # Feature_discovered_node_count(),\n",
    "            # Feature_discovered_credential_count(),\n",
    "            w.Feature_discovered_ports_sliding(ep),\n",
    "            w.Feature_discovered_nodeproperties_sliding(ep),\n",
    "            w.Feature_discovered_notowned_node_count(ep, 3)\n",
    "        ], 5000)  # should not be too small, pick something big to avoid collision\n",
    "\n",
    "actionspace = w.RavelEncoding(ep, [\n",
    "            w.Feature_active_node_properties(ep)])\n",
    "print(statespace)\n",
    "print(actionspace)\n",
    "q_matrix = DefenderQMatrix(statespace, actionspace,gym_env)\n",
    "learner2 = DefenderQLearner(gym_env, defender_agent, ep, q_matrix, epsilon=0.9, gamma=0.015, learning_rate=.07)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81bdfd99-c480-472b-9d60-599e7b0a32f3",
   "metadata": {},
   "source": [
    "# 50 episodes 200 iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42796cb0-2406-4704-8fba-af52052bb72c",
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(120394016)\n",
    "#%matplotlib inline\n",
    "trained_defender = epsilon_greedy_defender_training(\n",
    "    cyberbattle_gym_env=gym_env,\n",
    "    environment_properties=ep,\n",
    "    learner=learner2,\n",
    "    episode_count=50,\n",
    "    iteration_count=200,\n",
    "    epsilon=0.99,\n",
    "    epsilon_exponential_decay=5000,\n",
    "    epsilon_minimum=0.05,\n",
    "    epsilon_multdecay=0.99999999999999999999,  # 0.999,,\n",
    "    verbosity=logging.INFO,\n",
    "    render=True,\n",
    "    plot_episodes_length=True,\n",
    "    title=\"Defender Q-Learning Training\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fb0b5bf-5eef-49b5-ae01-74e664314f49",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "trained_defender"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ae6b4ac-7d97-4181-9313-100e43bc95d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_attacker"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8fafca6-c2fa-4630-b8d1-c38dc2a8f0a6",
   "metadata": {},
   "source": [
    "## Visualise Reward Matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dd0741a-f314-4e71-8206-d64b92e825dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "def plot_rewards_heatmap(rewards, title='Rewards Heatmap'):\n",
    "    rewards_array = np.array(rewards)\n",
    "    plt.figure(figsize=(20, 20))\n",
    "    sns.heatmap(rewards_array, annot=False, cmap='viridis', cbar=True)\n",
    "    plt.title(title)\n",
    "    plt.xlabel('Step')\n",
    "    plt.ylabel('Episode')\n",
    "    plt.show()\n",
    "plot_rewards_heatmap(trained_defender['all_episodes_rewards'], title='Defender Rewards Heatmap')\n",
    "plot_rewards_heatmap(trained_attacker['all_episodes_rewards'], title='Attacker Rewards Heatmap')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc051dc5-4154-4de6-8bb9-938182ae5d70",
   "metadata": {},
   "source": [
    "Each row represents an episode.\n",
    "\n",
    "Each column represents a step within those episodes.\n",
    "\n",
    "The color indicates the magnitude of the reward at each step of each episode, providing a quick visual insight into how rewards are distributed over time and across episodes.\n",
    "\n",
    "The heatmaps help identify patterns such as consistent reward values across episodes, variability in rewards, or trends over time, which might be less apparent in line plots when dealing with numerous episodes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22cd926e-64db-41a7-ab6e-046fa2c9f5b0",
   "metadata": {},
   "source": [
    "## Visualising Availability Matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cbb9fff-7c5b-4feb-941c-619f96ef44b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_availability(availabilities, title='Availability Over Episodes'):\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    for i, episode_availability in enumerate(availabilities):\n",
    "        plt.plot(episode_availability, label=f'Episode {i+1}')\n",
    "    plt.title(title)\n",
    "    plt.xlabel('Step')\n",
    "    plt.ylabel('Availability')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "# Plotting the availabilities for the defender and attacker\n",
    "plot_availability(trained_defender['all_episodes_availability'], title='Defender Availability Over Episodes')\n",
    "plot_availability(trained_attacker['all_episodes_availability'], title='Attacker Availability Over Episodes')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3cc97e4-0dfa-4dcc-a214-56b8352959c3",
   "metadata": {},
   "source": [
    "# Initialise Both Agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7816264b-0553-4757-a6ce-e75a76b4f95d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "trained_attacker['learner'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5513ea0a-429c-4f9d-ab53-214818643b4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "attacker_learner = trained_attacker['learner']\n",
    "defender_learner = trained_defender['learner']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bac0773e-0ff8-4095-b88b-9c63e9459769",
   "metadata": {},
   "outputs": [],
   "source": [
    "#this was made due to issues where the attacker q learner needed to use a wrapper environment\n",
    "from cyberbattle.agents.baseline.agent_wrapper import AgentWrapper, StateAugmentation\n",
    "def wrap_environment(env):\n",
    "    initial_observation = env.reset()  \n",
    "    state = StateAugmentation(initial_observation)  \n",
    "    return AgentWrapper(env, state)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed4ed3bb-d452-4f77-8763-a59c129266dd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "\n",
    "gym_env = gym.make('CyberBattleChain-v0',\n",
    "    size=12,\n",
    "    attacker_goal=AttackerGoal(\n",
    "        own_atleast=0,\n",
    "        own_atleast_percent=1.0\n",
    "    ),\n",
    "    defender_constraint=DefenderConstraint(\n",
    "        maintain_sla=0.80\n",
    "    )\n",
    ")\n",
    "\n",
    "defender_name = \"DefenderName\" \n",
    "defender_agent = DefenderWrapper(gym_env,defender_name)\n",
    "\n",
    "gym_environment = gym.make('CyberBattleChain-v0',\n",
    "    size=12,\n",
    "    attacker_goal=AttackerGoal(\n",
    "        own_atleast=0,\n",
    "        own_atleast_percent=1.0\n",
    "    ),\n",
    "    defender_constraint=DefenderConstraint(\n",
    "        maintain_sla=0.80\n",
    "    ),\n",
    "    defender_agent=defender_agent\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8817bc9-fc6a-46f8-8d59-9342cb8c7ee3",
   "metadata": {},
   "outputs": [],
   "source": [
    "gym_environment.environment\n",
    "#gym_environment.network.nodes\n",
    "#gym_environment.action_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b06ccf57-cede-47ea-a1cc-969ce37b41a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "gym_environment.observation_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9b9fdf0-eb22-42bf-a7f9-db1d7f2faf2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_attacker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c253b848-1e75-436a-8864-585bf919d621",
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_defender"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26789507-5b31-4c21-9f75-c606d2b5f354",
   "metadata": {},
   "outputs": [],
   "source": [
    "import inspect\n",
    "print(inspect.signature(gym_environment.step))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2fdc66f-a3b6-4ba0-9b5c-ccfccdb9b804",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class SimulationTracker:\n",
    "    def __init__(self):\n",
    "        self.attacker_rewards = []\n",
    "        self.defender_rewards = []\n",
    "        self.actions_taken = {'attacker': [], 'defender': []}\n",
    "        self.exploration_actions = {'attacker': 0, 'defender': 0}\n",
    "        self.exploitation_actions = {'attacker': 0, 'defender': 0}\n",
    "        self.episode_lengths = []\n",
    "        self.reward_variability = []\n",
    "        self.state_visitation_frequencies = {}\n",
    "        self.temporal_differences = []\n",
    "        self.infected_nodes = []\n",
    "\n",
    "    def log_episode(self, attacker_reward, defender_reward, episode_length, episode_rewards):\n",
    "        self.attacker_rewards.append(attacker_reward)\n",
    "        self.defender_rewards.append(defender_reward)\n",
    "        self.episode_lengths.append(episode_length)\n",
    "        self.reward_variability.append(np.var(episode_rewards))\n",
    "        #self.temporal_differences.append(temporal_difference)\n",
    "\n",
    "    def log_action(self, role, action, exploration):\n",
    "        action_str = str(action)  # Convert the action dictionary to a string\n",
    "        self.actions_taken[role].append(action_str)\n",
    "        if exploration:\n",
    "            self.exploration_actions[role] += 1\n",
    "        else:\n",
    "            self.exploitation_actions[role] += 1\n",
    "\n",
    "\n",
    "    def log_state_visitation(self, state):\n",
    "        if state not in self.state_visitation_frequencies:\n",
    "            self.state_visitation_frequencies[state] = 1\n",
    "        else:\n",
    "            self.state_visitation_frequencies[state] += 1\n",
    "\n",
    "    def log_infected_nodes(self, nodes_count):\n",
    "        self.infected_nodes.append(nodes_count)\n",
    "\n",
    "    def print_summary(self):\n",
    "        # Summary statistics\n",
    "        print(\"Simulation Summary:\")\n",
    "        print(f\"Average Attacker Reward: {np.mean(self.attacker_rewards)}\")\n",
    "        print(f\"Average Defender Reward: {np.mean(self.defender_rewards)}\")\n",
    "        print(f\"Average Episode Length: {np.mean(self.episode_lengths)} steps\")\n",
    "        print(f\"Average Reward Variability: {np.mean(self.reward_variability)}\")\n",
    "        print(f\"Average Temporal Difference: {np.mean(self.temporal_differences)}\")\n",
    "        print(f\"Average Infected Nodes per Episode: {np.mean(self.infected_nodes)}\")\n",
    "\n",
    "        # Exploration vs. Exploitation\n",
    "        print(\"\\nExploration vs. Exploitation:\")\n",
    "        for role in ['attacker', 'defender']:\n",
    "            total_actions = self.exploration_actions[role] + self.exploitation_actions[role]\n",
    "            if total_actions > 0:\n",
    "                print(f\"{role.capitalize()} Exploration: {self.exploration_actions[role] / total_actions:.2f}\")\n",
    "                print(f\"{role.capitalize()} Exploitation: {self.exploitation_actions[role] / total_actions:.2f}\")\n",
    "        self._print_action_distributions()\n",
    "        self._plot_rewards()\n",
    "\n",
    "    def _print_action_distributions(self):\n",
    "        print(\"\\nAction Distributions:\")\n",
    "        for role, actions in self.actions_taken.items():\n",
    "            #actions are already converted to strings by log_action\n",
    "            unique_actions, counts = np.unique(actions, return_counts=True)\n",
    "            print(f\"{role.capitalize()} Actions:\")\n",
    "            for action, count in zip(unique_actions, counts):\n",
    "                print(f\"Action {action}: {count} times\")\n",
    "\n",
    "\n",
    "    def _plot_rewards(self):\n",
    "        plt.figure(figsize=(10, 5))\n",
    "        plt.plot(self.attacker_rewards, label='Attacker Rewards')\n",
    "        plt.plot(self.defender_rewards, label='Defender Rewards')\n",
    "        plt.xlabel('Episode')\n",
    "        plt.ylabel('Reward')\n",
    "        plt.title('Rewards per Episode for Attacker and Defender')\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6828401a-1058-43fe-bd3a-709264ccbfaf",
   "metadata": {},
   "source": [
    "## Stackelberg Game Simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14cbbc94-b613-42ee-b128-104d69b33452",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Simulation Parameters\n",
    "number_of_episodes = 300\n",
    "max_steps_per_episode = 100\n",
    "attacker_epsilon = 0.1\n",
    "defender_epsilon = 0.1\n",
    "\n",
    "# Reward Tracking\n",
    "attacker_rewards_per_episode = []\n",
    "defender_rewards_per_episode = []\n",
    "win_rates = {'attacker': [], 'defender': []}\n",
    "actions_taken_count = {'attacker': [], 'defender': []}\n",
    "exploration_vs_exploitation = {'exploration': [], 'exploitation': []}\n",
    "episode_lengths = []\n",
    "state_visitation_frequencies = {}  \n",
    "reward_variability = []\n",
    "defender_successful_actions = []\n",
    "defender_failed_actions = []\n",
    "defender_actions_taken = []\n",
    "tracker = SimulationTracker() #instantiate simulation tracker\n",
    "wrapped_env = wrap_environment(gym_environment)\n",
    "win_counts = {'attacker': 0, 'defender': 0}\n",
    "# Main Simulation Loop\n",
    "random.seed(120394016)\n",
    "for episode in range(number_of_episodes):\n",
    "    observation = wrapped_env.reset()\n",
    "    attacker_total_reward = 0\n",
    "    defender_total_reward = 0\n",
    "    steps_this_episode=0\n",
    "    episode_rewards = []  # Track rewards for this episode\n",
    "    for step in range(max_steps_per_episode):\n",
    "        steps_this_episode +=1\n",
    "        # Attacker's Turn\n",
    "        exploration = np.random.random() < attacker_epsilon\n",
    "        if exploration:\n",
    "            _, attacker_action, _ = attacker_learner.explore(wrapped_env)\n",
    "        else:\n",
    "            _, attacker_action, _ = attacker_learner.exploit(wrapped_env, observation)\n",
    "        #print(attacker_action)\n",
    "        if attacker_action is not None:\n",
    "            observation, attacker_reward, done, _ = gym_environment.step(attacker_action)\n",
    "            attacker_total_reward += attacker_reward\n",
    "            print(_)\n",
    "            actions_taken_count['attacker'].append(attacker_action)  # Track action\n",
    "            episode_rewards.append(attacker_reward)  # Track reward variability\n",
    "            tracker.log_action('attacker', attacker_action, exploration)\n",
    "            #actions_taken_count['attacker'][attacker_action] = actions_taken_count['attacker'].get(attacker_action, 0) + 1\n",
    "\n",
    "\n",
    "        # Defender's Turn\n",
    "        exploration = np.random.random() < defender_epsilon\n",
    "        if exploration:\n",
    "            _, defender_action, _ = defender_learner.explore(wrapped_env)\n",
    "        else:\n",
    "            _, defender_action, _ = defender_learner.exploit(wrapped_env, observation)\n",
    "\n",
    "        if defender_action is not None:\n",
    "            observation, defender_reward, done, _ = gym_environment.step(defender_action)\n",
    "            #action= _['defender_info']['action']\n",
    "            defender_total_reward += defender_reward\n",
    "            actions_taken_count['defender'].append(defender_action)  # Track action\n",
    "            episode_rewards.append(defender_reward)  # Track reward variability\n",
    "            tracker.log_action('defender', defender_action, exploration)\n",
    "            #actions_taken_count['defender'][defender_action] = actions_taken_count['defender'].get(defender_action, 0) + 1\n",
    "        #logging\n",
    "        # After each episode\n",
    "        tracker.log_episode(attacker_total_reward, defender_total_reward, steps_this_episode, episode_rewards)\n",
    "        tracker.print_summary()\n",
    "        tracker._plot_rewards()\n",
    "\n",
    "        if done:\n",
    "            break\n",
    "            \n",
    "    defender_info = _['defender_info']  \n",
    "    if defender_info:  \n",
    "        defender_actions_taken.append(defender_info['performance_metrics']['actions_taken'])\n",
    "        defender_successful_actions.append(defender_info['performance_metrics']['successful_actions'])\n",
    "        defender_failed_actions.append(defender_info['performance_metrics']['failed_actions'])\n",
    "\n",
    "    attacker_rewards_per_episode.append(attacker_total_reward)\n",
    "    defender_rewards_per_episode.append(defender_total_reward)\n",
    "    episode_lengths.append(steps_this_episode)\n",
    "    reward_variability.append(np.var(episode_rewards))\n",
    "    print(f\"Episode {episode + 1}: Attacker Reward = {attacker_total_reward}, Defender Reward = {defender_total_reward}\")\n",
    "\n",
    "wrapped_env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12e1c765-5d3e-4dfc-b645-cb00d77b291b",
   "metadata": {},
   "source": [
    "## Logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34f0b30c-8676-4f7f-8577-4bab6048557f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb86664e-9cf5-48bb-8969-62d83bed1f2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting actions taken\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(defender_actions_taken, label='Actions Taken')\n",
    "plt.plot(defender_successful_actions, label='Successful Actions')\n",
    "plt.plot(defender_failed_actions, label='Failed Actions')\n",
    "plt.title('Defender Performance Metrics Over Episodes')\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Count')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee44dbcd-d846-4df3-ad99-9cf57f347cbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 10))\n",
    "plt.plot(attacker_rewards_per_episode, label='Attacker Rewards')\n",
    "plt.plot(defender_rewards_per_episode, label='Defender Rewards')\n",
    "plt.title('Total Rewards per Episode')\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Total Reward')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9b43c26-94b5-46aa-bed2-66b6ab370bbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(reward_variability, label='Reward Variability')\n",
    "plt.title('Reward Variability per Episode')\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Variability')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10901163-a892-4635-8cd1-766f4ccd5b8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalise_attacker_action(action):\n",
    "    if 'connect' in action:\n",
    "        return 'connect'\n",
    "    elif 'remote_vulnerability' in action:\n",
    "        return 'remote_vulnerability'\n",
    "    elif 'local_vulnerability' in action:\n",
    "        return 'local_vulnerability'\n",
    "    else:\n",
    "        return 'other'  \n",
    "from collections import Counter\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84c5c8c2-ac9b-487b-b0c9-89200fa5fc35",
   "metadata": {},
   "outputs": [],
   "source": [
    "#No changes needed to the plotting function itself\n",
    "def plot_action_frequencies(action_frequencies, title, ax):\n",
    "    actions = list(action_frequencies.keys())\n",
    "    frequencies = list(action_frequencies.values())\n",
    "\n",
    "    ax.bar(actions, frequencies)\n",
    "    ax.set_title(title)\n",
    "    ax.set_xlabel('Actions')\n",
    "    ax.set_ylabel('Frequencies')\n",
    "    ax.tick_params(axis='x', rotation=90)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fde2d07-c11a-4749-b4dd-5b8d48566e61",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "#convert string of dictionaries to dictionaries\n",
    "defender_actions = [ast.literal_eval(action) for action in normalized_actions['defender']]\n",
    "defender_actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be4f723b-1394-43ec-9213-b73fba1978df",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "#Aggregate Actions by Node\n",
    "actions_by_node = defaultdict(list)\n",
    "\n",
    "for action in defender_actions:\n",
    "    node_id = action.get('node_id')\n",
    "    actions_by_node[node_id].append(action)\n",
    "actions_by_node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1576805-5498-4f57-b10b-c386696a210a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Count Actions per Node\n",
    "action_counts_by_node = {}\n",
    "\n",
    "for node_id, actions in actions_by_node.items():\n",
    "    action_counts = defaultdict(int)\n",
    "    for action in actions:\n",
    "        action_type = action.get('action_type')\n",
    "        action_counts[action_type] += 1\n",
    "    action_counts_by_node[node_id] = dict(action_counts)\n",
    "action_counts_by_node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "852be7b8-2515-454f-8e84-17b816588e68",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_actions_per_node(node_id, action_counts):\n",
    "    labels = list(action_counts.keys())\n",
    "    #labels = ['Reimage Node','Block Traffic','Allow Traffic','Stop Service','Start Service']\n",
    "    counts = list(action_counts.values())\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.bar(labels, counts, color='skyblue')\n",
    "    plt.title(f'Action Counts for Node {node_id}')\n",
    "    plt.xlabel('Action Type')\n",
    "    plt.ylabel('Count')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.show()\n",
    "\n",
    "for nodeid in range(0,14):\n",
    "    if nodeid in action_counts_by_node.keys():\n",
    "        plot_actions_per_node(nodeid, action_counts_by_node[nodeid])\n",
    "    else:\n",
    "        print(f\"No actions recorded for node {nodeid}.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35ed71d5-7b51-474c-af6e-37b31941b750",
   "metadata": {},
   "outputs": [],
   "source": [
    "#track actions by node\n",
    "def track_defender_actions(actions):\n",
    "    actions_by_node = {}\n",
    "    for action in actions['defender']:\n",
    "        if 'node_id' in action:\n",
    "            node_id = action['node_id']\n",
    "            action_type = action.get('action_type')\n",
    "\n",
    "            # Initialise the list for the node if it doesn't exist\n",
    "            if node_id not in actions_by_node:\n",
    "                actions_by_node[node_id] = []\n",
    "\n",
    "            actions_by_node[node_id].append(action_type)\n",
    "\n",
    "    return actions_by_node\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "308cbe90-fcde-407f-a960-4f841955ed23",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def plot_action_sequence(actions, title='Action Sequence'):\n",
    "    \"\"\"Plot the sequence of actions over time.\"\"\"\n",
    "    #convert action dictionaries to a string or tuple representation for hashing\n",
    "    action_strings = [str(action) for action in actions]\n",
    "    \n",
    "    #create a unique ID for each unique action string\n",
    "    action_ids = {action: i for i, action in enumerate(set(action_strings))}\n",
    "    action_sequence = [action_ids[action] for action in action_strings]\n",
    "    \n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.plot(action_sequence, marker='o', linestyle='-', markersize=5, label='Action ID')\n",
    "    plt.yticks(np.arange(len(action_ids)), list(action_ids.keys()), rotation=45, ha='right')\n",
    "    plt.ylabel('Actions')\n",
    "    plt.xlabel('Step')\n",
    "    plt.title(title)\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()  \n",
    "    plt.show()\n",
    "\n",
    "plot_action_sequence(actions_taken_count['attacker'], title='Attacker Action Sequence')\n",
    "plot_action_sequence(actions_taken_count['defender'], title='Defender Action Sequence')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11fb7e75-f6a2-4e99-8cae-c35e24e81800",
   "metadata": {},
   "outputs": [],
   "source": [
    "#plotting the Rewards\n",
    "episodes = range(1, number_of_episodes + 1)\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(episodes, attacker_rewards_per_episode, label='Attacker Rewards', marker='o')\n",
    "plt.plot(episodes, defender_rewards_per_episode, label='Defender Rewards', marker='x')\n",
    "plt.title('Attacker vs Defender Rewards per Episode')\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Cumulative Reward')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "#win Rates Over Time:\n",
    "plt.figure()\n",
    "plt.plot(win_rates['attacker'], label='Attacker Win Rate')\n",
    "plt.plot(win_rates['defender'], label='Defender Win Rate')\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Win Rate')\n",
    "plt.legend()\n",
    "plt.title('Win Rates Over Time')\n",
    "plt.show()\n",
    "\n",
    "#exploration vs. exploitation:\n",
    "plt.figure()\n",
    "plt.plot(exploration_vs_exploitation['exploration'], label='Exploration')\n",
    "plt.plot(exploration_vs_exploitation['exploitation'], label='Exploitation')\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Count')\n",
    "plt.legend()\n",
    "plt.title('Exploration vs Exploitation')\n",
    "plt.show()\n",
    "\n",
    "#episode Lengths:\n",
    "plt.figure()\n",
    "plt.plot(episode_lengths)\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Length')\n",
    "plt.title('Episode Lengths Over Time')\n",
    "plt.show()\n",
    "\n",
    "#learning rates\n",
    "\n",
    "#Q-value changes\n",
    "\n",
    "#activity heatmaps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3bb465c-f0af-4fb1-83d0-ee4a52f20871",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6594505-59ac-4f68-a992-cada07a78f33",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9633dc01-6d18-4a6c-92e6-83a706211628",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:cybersim] *",
   "language": "python",
   "name": "conda-env-cybersim-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
