{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "840f50a6-01d4-4450-924f-337db7fc29d7",
   "metadata": {
    "id": "840f50a6-01d4-4450-924f-337db7fc29d7"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import copy\n",
    "import logging\n",
    "import networkx\n",
    "from networkx import convert_matrix\n",
    "from typing import NamedTuple, Optional, Tuple, List, Dict, TypeVar, TypedDict, cast\n",
    "\n",
    "import numpy\n",
    "import gym\n",
    "from gym import spaces\n",
    "from gym.utils import seeding\n",
    "\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "from cyberbattle._env.shared_cyberbattle_env import EnvironmentBounds, AttackerGoal, DefenderGoal, DefenderConstraint\n",
    "from cyberbattle._env.defender import DefenderAgent,ScanAndReimageCompromisedMachines\n",
    "from cyberbattle.simulation.model import PortName, PrivilegeLevel\n",
    "from cyberbattle.simulation import commandcontrol, model, actions\n",
    "from cyberbattle._env.discriminatedunion import DiscriminatedUnion\n",
    "from cyberbattle.agents.baseline import agent_wrapper as w\n",
    "import cyberbattle.agents.baseline.learner as learner\n",
    "from cyberbattle.agents.baseline.learner import Learner\n",
    "import cyberbattle.agents.baseline.agent_tabularqlearning as a\n",
    "import cyberbattle.agents.baseline.agent_ddql as ddqla\n",
    "import cyberbattle.agents.baseline.agent_dql as dqla\n",
    "from cyberbattle.defender_agents.trainedDefender import *\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0fbc576-1dc6-4d84-9f91-131d2b7ed15d",
   "metadata": {
    "id": "c0fbc576-1dc6-4d84-9f91-131d2b7ed15d"
   },
   "outputs": [],
   "source": [
    "random.seed(120394016)\n",
    "%matplotlib inline\n",
    "import cyberbattle.agents.baseline.agent_tabularqlearning as a\n",
    "import cyberbattle.agents.baseline.learner as learner\n",
    "from cyberbattle.agents.baseline.agent_wrapper import Verbosity\n",
    "#ql\n",
    "iteration_count = 10\n",
    "training_episode_count = 10\n",
    "eval_episode_count = 20\n",
    "gamma_sweep = [\n",
    "    0.015,  # about right\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5685ec1-e65c-4f62-8fc4-7ae2f6b4ae07",
   "metadata": {
    "id": "b5685ec1-e65c-4f62-8fc4-7ae2f6b4ae07",
    "outputId": "7049c6a3-dc8e-48b8-8e80-d98e46e5403e"
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "\n",
    "# Create an instance of the CyberBattleChain environment without the defender\n",
    "gym_env = gym.make('CyberBattleChain-v0',\n",
    "    size=12,\n",
    "    attacker_goal=AttackerGoal(\n",
    "        own_atleast=0,\n",
    "        own_atleast_percent=1.0\n",
    "    ),\n",
    "    defender_constraint=DefenderConstraint(\n",
    "        maintain_sla=0.80\n",
    "    )\n",
    ")\n",
    "\n",
    "\n",
    "defender_name = \"DefenderName\" \n",
    "\n",
    "defender_agent = DefenderWrapper(gym_env,defender_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af8db222-4a48-4fc6-8444-8bf184bf83ac",
   "metadata": {
    "id": "af8db222-4a48-4fc6-8444-8bf184bf83ac"
   },
   "source": [
    "Pre-tuning defender"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1614d54-791b-4d89-91d9-a35700d78eba",
   "metadata": {
    "id": "c1614d54-791b-4d89-91d9-a35700d78eba"
   },
   "outputs": [],
   "source": [
    "from cyberbattle.agents.baseline.agent_wrapper import ActionTrackingStateAugmentation, StateAugmentation\n",
    "from cyberbattle.agents.baseline.plotting import PlotTraining, plot_averaged_cummulative_rewards\n",
    "import progressbar\n",
    "import math\n",
    "from cyberbattle.agents.baseline import learner\n",
    "import sys\n",
    "from cyberbattle.simulation.model import FirewallRule\n",
    "\n",
    "ep = w.EnvironmentBounds.of_identifiers(\n",
    "    maximum_total_credentials=22,\n",
    "    maximum_node_count=22,\n",
    "    identifiers=gym_env.identifiers\n",
    ")\n",
    "#cyberbattlechain_defender.render_as_fig()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d85b369-1bb5-41aa-a80d-1b0f6b6dbe18",
   "metadata": {
    "id": "7d85b369-1bb5-41aa-a80d-1b0f6b6dbe18",
    "outputId": "563b8064-c226-4c52-bab0-c0492a0c2cb4"
   },
   "outputs": [],
   "source": [
    "import cyberbattle.agents.baseline.agent_tabularqlearning as a\n",
    "import cyberbattle.agents.baseline.learner as learner\n",
    "from cyberbattle.agents.baseline.agent_wrapper import Verbosity\n",
    "random.seed(120394016)\n",
    "epsilon = 0.66666666666666\n",
    "gamma= 0.06925\n",
    "learning_rate= 0.34444444444444444\n",
    "epsilon_exponential_decay=3198.830407105263\n",
    "epsilon_minimum=0.05708811937931034\n",
    "episode_count=10\n",
    "iteration_count=10\n",
    "epsilon_multdecay=0.7777777777777778\n",
    "statespace = w.HashEncoding(ep, [\n",
    "            # Feature_discovered_node_count(),\n",
    "            # Feature_discovered_credential_count(),\n",
    "            w.Feature_discovered_ports_sliding(ep),\n",
    "            w.Feature_discovered_nodeproperties_sliding(ep),\n",
    "            w.Feature_discovered_notowned_node_count(ep, 3)\n",
    "        ], 5000)  # should not be too small, pick something big to avoid collision\n",
    "\n",
    "actionspace = w.RavelEncoding(ep, [\n",
    "            w.Feature_active_node_properties(ep)])\n",
    "print(statespace)\n",
    "print(actionspace)\n",
    "q_matrix = DefenderQMatrix(statespace, actionspace,gym_env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "039682e7-b937-4bba-b0f4-de0d4ead31af",
   "metadata": {
    "id": "039682e7-b937-4bba-b0f4-de0d4ead31af"
   },
   "outputs": [],
   "source": [
    "def run_training_with_hyperparameters(gym_env, environment_properties, statespace, actionspace,\n",
    "                                      epsilon, gamma, learning_rate, epsilon_exponential_decay, epsilon_minimum, episode_count, iteration_count, epsilon_multdecay):\n",
    "    q_matrix = DefenderQMatrix(statespace, actionspace, gym_env)\n",
    "    learner = DefenderQLearner(gym_env, defender_agent, environment_properties, q_matrix,\n",
    "                               epsilon=epsilon, gamma=gamma, learning_rate=learning_rate)\n",
    "\n",
    "    trained_defender = epsilon_greedy_defender_training(\n",
    "        cyberbattle_gym_env=gym_env,\n",
    "        environment_properties=environment_properties,\n",
    "        learner=learner,\n",
    "        episode_count=episode_count,  # Set the number of episodes for the training\n",
    "        iteration_count=iteration_count,  # Set the number of iterations per episode\n",
    "        epsilon=epsilon,\n",
    "        epsilon_exponential_decay=epsilon_exponential_decay,\n",
    "        epsilon_minimum=epsilon_minimum,\n",
    "        epsilon_multdecay=epsilon_multdecay,\n",
    "        verbosity=logging.INFO,\n",
    "        render=True,\n",
    "        plot_episodes_length=True,\n",
    "        title=\"Defender Q-Learning Training\"\n",
    "    )\n",
    "    total_reward = sum([sum(rewards) for rewards in trained_defender['all_episodes_rewards']])\n",
    "    return total_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61375360-2228-4cb4-a4c2-cacb03c1d025",
   "metadata": {
    "id": "61375360-2228-4cb4-a4c2-cacb03c1d025",
    "outputId": "eed090d5-a012-4c9b-e857-03ec3225b1db"
   },
   "outputs": [],
   "source": [
    "import cyberbattle.agents.baseline.agent_tabularqlearning as a\n",
    "import cyberbattle.agents.baseline.learner as learner\n",
    "from cyberbattle.agents.baseline.agent_wrapper import Verbosity\n",
    "random.seed(120394016)\n",
    "# Parameter ranges\n",
    "epsilon = .5\n",
    "#gamma=np.linspace(0.05,0.09,10)\n",
    "gamma= 0.0\n",
    "#learning_rate=np.linspace(0.3,.35,10)\n",
    "learning_rate=0.0\n",
    "#epsilon_decay=np.linspace(2333.33333,5000,20)\n",
    "epsilon_exponential_decay=1000\n",
    "#epsilon_minimum=np.linspace(0.01,.07777777,10)\n",
    "epsilon_minimum=0.0\n",
    "episode_count=10\n",
    "iteration_count=10\n",
    "#epsilon_multdecay=np.linspace(0.3,0.875,20)\n",
    "epsilon_multdecay=0.0\n",
    "statespace = w.HashEncoding(ep, [\n",
    "            # Feature_discovered_node_count(),\n",
    "            # Feature_discovered_credential_count(),\n",
    "            w.Feature_discovered_ports_sliding(ep),\n",
    "            w.Feature_discovered_nodeproperties_sliding(ep),\n",
    "            w.Feature_discovered_notowned_node_count(ep, 3)\n",
    "        ], 5000)  # should not be too small, pick something big to avoid collision\n",
    "\n",
    "actionspace = w.RavelEncoding(ep, [\n",
    "            w.Feature_active_node_properties(ep)])\n",
    "\n",
    "\n",
    "total_reward=run_training_with_hyperparameters(gym_env, ep, statespace, actionspace,\n",
    "                                      epsilon, gamma, learning_rate, epsilon_exponential_decay, epsilon_minimum, episode_count, iteration_count, epsilon_multdecay)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e765a53b-d5dd-465e-b30b-50f874df8bdd",
   "metadata": {
    "id": "e765a53b-d5dd-465e-b30b-50f874df8bdd",
    "outputId": "883c735c-275d-4646-d2c2-674a7e56c96e"
   },
   "outputs": [],
   "source": [
    "print(total_reward)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41beb453-a4a6-4e6c-bbec-b567a5a1fc3f",
   "metadata": {
    "id": "41beb453-a4a6-4e6c-bbec-b567a5a1fc3f"
   },
   "source": [
    "740, 620,660,460, 630, 540, 610, 970, 660, 690,840,540,440"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bd8e0b3-b77d-4357-a18e-1c9292dfc7a5",
   "metadata": {
    "id": "9bd8e0b3-b77d-4357-a18e-1c9292dfc7a5"
   },
   "source": [
    "Post-tuning defender"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c28b3a73-e1ae-4db2-863c-f8c255ab2ac2",
   "metadata": {
    "id": "c28b3a73-e1ae-4db2-863c-f8c255ab2ac2"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f223551c-1956-4042-8e58-f98fffe0e45e",
   "metadata": {
    "id": "f223551c-1956-4042-8e58-f98fffe0e45e",
    "outputId": "c7758bdf-eb49-4cd2-9fdc-d6d57d7c4be5"
   },
   "outputs": [],
   "source": [
    "import cyberbattle.agents.baseline.agent_tabularqlearning as a\n",
    "import cyberbattle.agents.baseline.learner as learner\n",
    "from cyberbattle.agents.baseline.agent_wrapper import Verbosity\n",
    "random.seed(120394016)\n",
    "epsilon = 0.66666666666666\n",
    "gamma= 0.06925\n",
    "learning_rate= 0.34444444444444444\n",
    "epsilon_exponential_decay=3198.830407105263\n",
    "epsilon_minimum=0.05708811937931034\n",
    "episode_count=10\n",
    "iteration_count=10\n",
    "epsilon_multdecay=0.7777777777777778\n",
    "statespace = w.HashEncoding(ep, [\n",
    "            # Feature_discovered_node_count(),\n",
    "            # Feature_discovered_credential_count(),\n",
    "            w.Feature_discovered_ports_sliding(ep),\n",
    "            w.Feature_discovered_nodeproperties_sliding(ep),\n",
    "            w.Feature_discovered_notowned_node_count(ep, 3)\n",
    "        ], 5000)  # should not be too small, pick something big to avoid collision\n",
    "\n",
    "actionspace = w.RavelEncoding(ep, [\n",
    "            w.Feature_active_node_properties(ep)])\n",
    "print(statespace)\n",
    "print(actionspace)\n",
    "q_matrix = DefenderQMatrix(statespace, actionspace,gym_env)\n",
    "total_reward1=run_training_with_hyperparameters(gym_env, ep, statespace, actionspace,\n",
    "                                      epsilon, gamma, learning_rate, epsilon_exponential_decay, epsilon_minimum, episode_count, iteration_count, epsilon_multdecay)\n",
    "total_reward1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81d1bb26-88e1-4ceb-b1ab-c16a4c78c4d3",
   "metadata": {
    "id": "81d1bb26-88e1-4ceb-b1ab-c16a4c78c4d3"
   },
   "source": [
    "810,840,550,800,690,440,760,670,590,690"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca67cabe-84c3-4b64-9790-272fb386796a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install scipy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b68edb39-4e2d-4fe5-b032-b4aedade5445",
   "metadata": {
    "id": "b68edb39-4e2d-4fe5-b032-b4aedade5445",
    "outputId": "e743a8b7-bea2-49db-8a05-a194d55e6f02"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "\n",
    "pre_tuned_rewards = [620, 660, 460, 630, 540, 610, 660, 690, 540, 440]\n",
    "post_tuned_rewards = [810, 840, 550, 800, 690, 440, 760, 670, 590, 690]\n",
    "\n",
    "#Perform a Shapiro-Wilk test for normality\n",
    "print(\"Normality Test (Shapiro-Wilk):\")\n",
    "print(\"Pre-Tuned:\", stats.shapiro(pre_tuned_rewards))\n",
    "print(\"Post-Tuned:\", stats.shapiro(post_tuned_rewards))\n",
    "\n",
    "#Perform Levene's test for equal variances\n",
    "print(\"\\nEqual Variance Test (Levene's):\")\n",
    "print(stats.levene(pre_tuned_rewards, post_tuned_rewards))\n",
    "\n",
    "#Perform a t-test for the means of two independent samples\n",
    "print(\"\\nT-test:\")\n",
    "t_test_result = stats.ttest_ind(pre_tuned_rewards, post_tuned_rewards, equal_var=True)\n",
    "print(t_test_result)\n",
    "\n",
    "#If the assumptions of the t-test are not met, perform a Mann-Whitney U test\n",
    "print(\"\\nMann-Whitney U Test:\")\n",
    "mwu_test_result = stats.mannwhitneyu(pre_tuned_rewards, post_tuned_rewards)\n",
    "print(mwu_test_result)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.boxplot([pre_tuned_rewards, post_tuned_rewards], labels=['Pre-Tuned', 'Post-Tuned'])\n",
    "plt.title('Comparison of Total Rewards Before and After Tuning')\n",
    "plt.ylabel('Total Rewards')\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f26a293-55c0-45ed-a064-5cb1eaf95852",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 7))\n",
    "\n",
    "#Define colours for each boxplot and scatter\n",
    "#box_colors = ['#1f77b4', '#ff7f0e']\n",
    "box_colors = ['pink', 'lightblue']\n",
    "scatter_colors = ['#3090C7', '#FFA500']\n",
    "\n",
    "bp = ax.boxplot([pre_tuned_rewards, post_tuned_rewards], patch_artist=True,\n",
    "                labels=['Pre-Tuned', 'Post-Tuned'],\n",
    "                medianprops={'color': 'red', 'linewidth': 2},\n",
    "                boxprops=dict(facecolor='lightgray', color='black', linewidth=2),\n",
    "                whiskerprops={'color': 'black', 'linewidth': 1.5},\n",
    "                capprops={'color': 'black', 'linewidth': 1.5},\n",
    "                showfliers=False)  # Optionally hide the fliers\n",
    "\n",
    "for patch, color in zip(bp['boxes'], box_colors):\n",
    "    patch.set_facecolor(color)\n",
    "\n",
    "# Scatter plot for individual data points\n",
    "for i, data in enumerate([pre_tuned_rewards, post_tuned_rewards]):\n",
    "    y = np.random.normal(1 + i, 0.04, size=len(data))  # Slight jitter for clarity\n",
    "    ax.scatter(y, data, alpha=0.7, color=scatter_colors[i], edgecolor='black', zorder=2, s=50)\n",
    "\n",
    "#Calculate and plot overall means for pre-tuned and post-tuned\n",
    "overall_mean_pre = np.mean(pre_tuned_rewards)\n",
    "overall_mean_post = np.mean(post_tuned_rewards)\n",
    "\n",
    "ax.hlines(overall_mean_pre, xmin=0.8, xmax=1.2, colors='blue', linestyles='dashed', label='Mean Pre-Tuned')\n",
    "ax.hlines(overall_mean_post, xmin=1.8, xmax=2.2, colors='green', linestyles='dashed', label='Mean Post-Tuned')\n",
    "\n",
    "# Highlight maximum values with annotation\n",
    "max_pre = max(pre_tuned_rewards)\n",
    "max_post = max(post_tuned_rewards)\n",
    "\n",
    "max_pre_index = pre_tuned_rewards.index(max_pre) + 1\n",
    "max_post_index = post_tuned_rewards.index(max_post) + 1 + len(pre_tuned_rewards)  # Offset for post-tuned\n",
    "\n",
    "ax.annotate('Max Pre-Tuned: {}'.format(max_pre),\n",
    "            xy=(1, max_pre), xytext=(0.8, max_pre + 50),\n",
    "            arrowprops=dict(facecolor='black', shrink=0.05),\n",
    "            fontsize=12, color='black')\n",
    "\n",
    "ax.annotate('Max Post-Tuned: {}'.format(max_post),\n",
    "            xy=(2, max_post), xytext=(2.2, max_post - 50),\n",
    "            arrowprops=dict(facecolor='black', shrink=0.05),\n",
    "            fontsize=12, color='black')\n",
    "\n",
    "ax.set_title('Comparison of Total Rewards Before and After Tuning', fontsize=16)\n",
    "ax.set_ylabel('Total Rewards', fontsize=14)\n",
    "ax.set_xticklabels(['Pre-Tuned', 'Post-Tuned'], fontsize=12)\n",
    "ax.set_yticklabels(np.round(ax.get_yticks(), 2), fontsize=12)\n",
    "\n",
    "ax.yaxis.grid(True, linestyle='--', which='major', color='grey', alpha=0.7)\n",
    "ax.set_axisbelow(True)  # Ensure grid is below plot elements\n",
    "\n",
    "plt.legend(loc='upper right', fontsize=12)\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f041648b-eb8f-4bdf-bd36-a3511546602c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 7))\n",
    "\n",
    "# Define colors for each boxplot and scatter\n",
    "box_colors = ['pink', 'lightblue']\n",
    "scatter_colors = ['#3090C7', '#FFA500']\n",
    "\n",
    "bp = ax.boxplot([pre_tuned_rewards, post_tuned_rewards], patch_artist=True,\n",
    "                labels=['Pre-Tuned', 'Post-Tuned'],\n",
    "                medianprops={'color': 'red', 'linewidth': 2},\n",
    "                boxprops=dict(facecolor='lightgray', color='black', linewidth=2),\n",
    "                whiskerprops={'color': 'black', 'linewidth': 1.5},\n",
    "                capprops={'color': 'black', 'linewidth': 1.5},\n",
    "                showfliers=False)  # Optionally hide the fliers\n",
    "\n",
    "# Coloring the boxes\n",
    "for patch, color in zip(bp['boxes'], box_colors):\n",
    "    patch.set_facecolor(color)\n",
    "\n",
    "# Scatter plot for individual data points\n",
    "for i, data in enumerate([pre_tuned_rewards, post_tuned_rewards]):\n",
    "    y = np.random.normal(1 + i, 0.04, size=len(data))  # Slight jitter for clarity\n",
    "    ax.scatter(y, data, alpha=0.7, color=scatter_colors[i], edgecolor='black', zorder=2, s=50)\n",
    "\n",
    "# Calculate and plot overall means for pre-tuned and post-tuned\n",
    "overall_mean_pre = np.mean(pre_tuned_rewards)\n",
    "overall_mean_post = np.mean(post_tuned_rewards)\n",
    "\n",
    "ax.hlines(overall_mean_pre, xmin=0.8, xmax=1.2, colors='blue', linestyles='dashed', linewidth=2, label='Mean Pre-Tuned')\n",
    "ax.hlines(overall_mean_post, xmin=1.8, xmax=2.2, colors='green', linestyles='dashed', linewidth=2, label='Mean Post-Tuned')\n",
    "\n",
    "# Highlight maximum values with annotation\n",
    "max_pre = max(pre_tuned_rewards)\n",
    "max_post = max(post_tuned_rewards)\n",
    "ax.plot(1, max_pre, '^', markersize=10, label='Max Pre-Tuned', color='black')\n",
    "ax.plot(2, max_post, '^', markersize=10, label='Max Post-Tuned', color='orange')\n",
    "\n",
    "# Enhancing the plot\n",
    "ax.set_title('Comparison of Total Rewards Before and After Tuning', fontsize=16)\n",
    "ax.set_ylabel('Total Rewards', fontsize=14)\n",
    "ax.set_xticklabels(['Pre-Tuned', 'Post-Tuned'], fontsize=12)\n",
    "ax.set_yticklabels(np.round(ax.get_yticks(), 2), fontsize=12)\n",
    "\n",
    "ax.yaxis.grid(True, linestyle='--', which='major', color='grey', alpha=0.7)\n",
    "ax.set_axisbelow(True)  # Ensure grid is below plot elements\n",
    "\n",
    "# Creating custom legend\n",
    "legend_elements = [\n",
    "    plt.Line2D([0], [0], color='lightgray', lw=4, label='Box (Quartiles)'),\n",
    "    plt.Line2D([0], [0], color='red', lw=2, linestyle='-', label='Median'),\n",
    "    plt.Line2D([0], [0], color='blue', lw=2, linestyle='dashed', label='Mean Pre-Tuned'),\n",
    "    plt.Line2D([0], [0], color='green', lw=2, linestyle='dashed', label='Mean Post-Tuned'),\n",
    "    plt.Line2D([0], [0], marker='^', color='w', markerfacecolor='black', markersize=10, label='Max Pre-Tuned'),\n",
    "    plt.Line2D([0], [0], marker='^', color='w', markerfacecolor='orange', markersize=10, label='Max Post-Tuned')\n",
    "]\n",
    "\n",
    "ax.legend(handles=legend_elements, loc='upper left', fontsize=12,frameon=True, facecolor='white', edgecolor='black')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fc92829-6d50-4e3a-b74d-d57df40a2961",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.stats as stats\n",
    "\n",
    "def mean_confidence_interval(data, confidence=0.95):\n",
    "    a = 1.0 * np.array(data)\n",
    "    n = len(a)\n",
    "    m, se = np.mean(a), stats.sem(a)\n",
    "    h = se * stats.t.ppf((1 + confidence) / 2., n-1)\n",
    "    return m, m-h, m+h\n",
    "\n",
    "# Calculate mean and confidence interval for pre-tuned and post-tuned rewards\n",
    "mean_pre, ci_lower_pre, ci_upper_pre = mean_confidence_interval(pre_tuned_rewards)\n",
    "mean_post, ci_lower_post, ci_upper_post = mean_confidence_interval(post_tuned_rewards)\n",
    "\n",
    "# Calculate means for plotting\n",
    "mean_pre_series = [mean_pre] * len(pre_tuned_rewards)\n",
    "mean_post_series = [mean_post] * len(post_tuned_rewards)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53266600-3688-4265-9635-76ea1f2116c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "mean_pre_series = np.mean(pre_tuned_rewards)\n",
    "mean_post_series = np.mean(post_tuned_rewards)\n",
    "ci_lower_pre, ci_upper_pre = np.percentile(pre_tuned_rewards, [2.5, 97.5])\n",
    "ci_lower_post, ci_upper_post = np.percentile(post_tuned_rewards, [2.5, 97.5])\n",
    "\n",
    "\n",
    "plt.style.use('seaborn-darkgrid')\n",
    "\n",
    "plt.figure(figsize=(12, 7))\n",
    "\n",
    "# Plotting mean lines\n",
    "#plt.plot(mean_pre_series, label='Pre-Tuned Mean', color='black', linestyle='--', linewidth=2)\n",
    "#plt.plot(mean_post_series, label='Post-Tuned Mean', color='darkorange', linestyle='--', linewidth=2)\n",
    "\n",
    "# Shaded confidence interval\n",
    "plt.fill_between(range(len(pre_tuned_rewards)), ci_lower_pre, ci_upper_pre, color='skyblue', alpha=0.4)\n",
    "plt.fill_between(range(len(post_tuned_rewards)), ci_lower_post, ci_upper_post, color='navajowhite', alpha=0.4)\n",
    "\n",
    "# Individual data points\n",
    "plt.scatter(range(len(pre_tuned_rewards)), pre_tuned_rewards, color='blue', alpha=0.7, edgecolor='black', s=100, label='Pre-Tuned Data')\n",
    "plt.scatter(range(len(post_tuned_rewards)), post_tuned_rewards, color='red', alpha=0.7, edgecolor='black', s=100, marker='x', label='Post-Tuned Data')\n",
    "\n",
    "# Annotations for maximum and minimum points (as an example)\n",
    "max_pre = max(pre_tuned_rewards)\n",
    "min_post = min(post_tuned_rewards)\n",
    "plt.annotate('Max Pre-Tuned', xy=(pre_tuned_rewards.index(max_pre), max_pre), xytext=(2, max_pre+30),\n",
    "             arrowprops=dict(facecolor='blue', shrink=0.05), fontsize=12)\n",
    "plt.annotate('Min Post-Tuned', xy=(post_tuned_rewards.index(min_post), min_post), xytext=(5, min_post-60),\n",
    "             arrowprops=dict(facecolor='red', shrink=0.05), fontsize=12)\n",
    "\n",
    "# Enhancing typography\n",
    "plt.title('Total Rewards Over Runs Before and After Tuning with Confidence Intervals', fontsize=18)\n",
    "plt.xlabel('Runs', fontsize=16)\n",
    "plt.ylabel('Total Rewards', fontsize=16)\n",
    "plt.xticks(fontsize=14)\n",
    "plt.yticks(fontsize=14)\n",
    "\n",
    "# Customizing the legend\n",
    "plt.legend(loc='upper left', fontsize=14, frameon=True, facecolor='white', edgecolor='black')\n",
    "\n",
    "plt.grid(True, which='both', linestyle='--', linewidth=0.5)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8618ad35-c4b0-4fe7-9fc8-7eab8347c1ff",
   "metadata": {
    "id": "8618ad35-c4b0-4fe7-9fc8-7eab8347c1ff",
    "outputId": "6a099bf2-98b0-48c2-ebcb-abb51aa88bf9"
   },
   "outputs": [],
   "source": [
    "#plt.figure(figsize=(10, 6))\n",
    "#plt.plot(pre_tuned_rewards, label='Pre-Tuned', marker='o')\n",
    "#plt.plot(post_tuned_rewards, label='Post-Tuned', marker='x')\n",
    "#plt.title('Total Rewards Over Runs Before and After Tuning')\n",
    "#plt.xlabel('Runs')\n",
    "#plt.ylabel('Total Rewards')\n",
    "#plt.legend()\n",
    "#plt.grid(True)\n",
    "#plt.show()\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('seaborn-darkgrid')\n",
    "\n",
    "plt.figure(figsize=(12, 7))\n",
    "plt.plot(pre_tuned_rewards, label='Pre-Tuned', marker='o', markersize=8, linestyle='-', linewidth=2)\n",
    "plt.plot(post_tuned_rewards, label='Post-Tuned', marker='x', markersize=8, linestyle='--', linewidth=2)\n",
    "\n",
    "plt.title('Total Rewards Over Runs Before and After Tuning', fontsize=16)\n",
    "plt.xlabel('Runs', fontsize=14)\n",
    "plt.ylabel('Total Rewards', fontsize=14)\n",
    "\n",
    "plt.text(x=max(len(pre_tuned_rewards), len(post_tuned_rewards)) - 2, y=min(min(pre_tuned_rewards), min(post_tuned_rewards)),\n",
    "         s=\"Dashed lines indicate post-tuning performance\", fontsize=12,\n",
    "         bbox=dict(facecolor='white', alpha=0.5))\n",
    "\n",
    "plt.xticks(fontsize=12)\n",
    "plt.yticks(fontsize=12)\n",
    "plt.legend(fontsize=12, loc='upper left', bbox_to_anchor=(1, 1)) # Place legend outside\n",
    "plt.grid(True, which='both', linestyle='--', linewidth=0.5) # Enhanced grid\n",
    "\n",
    "# Show plot\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8c4e87e-760e-421e-a3e7-43c12d875272",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "plt.style.use('seaborn-darkgrid')\n",
    "\n",
    "# Initialize figure\n",
    "fig, ax = plt.subplots(figsize=(12, 8))\n",
    "\n",
    "# Calculate moving averages for smooth trend lines\n",
    "window_size = 3\n",
    "pre_tuned_ma = np.convolve(pre_tuned_rewards, np.ones(window_size)/window_size, mode='valid')\n",
    "post_tuned_ma = np.convolve(post_tuned_rewards, np.ones(window_size)/window_size, mode='valid')\n",
    "\n",
    "# Plot moving averages with lines connecting data points\n",
    "ax.plot(pre_tuned_rewards, 'o-', label='Pre-Tuned', color='blue', markersize=8, alpha=0.75, linewidth=2)\n",
    "ax.plot(post_tuned_rewards, 'x--', label='Post-Tuned', color='orange', markersize=8, alpha=0.75, linewidth=2)\n",
    "\n",
    "# Highlight the maximum reward points and annotate\n",
    "max_pre = np.max(pre_tuned_rewards)\n",
    "max_post = np.max(post_tuned_rewards)\n",
    "max_pre_index = np.argmax(pre_tuned_rewards)\n",
    "max_post_index = np.argmax(post_tuned_rewards)\n",
    "\n",
    "ax.scatter(max_pre_index, max_pre, color='darkblue', s=100, zorder=5, edgecolor='black')\n",
    "ax.scatter(max_post_index, max_post, color='darkorange', s=100, zorder=5, edgecolor='black')\n",
    "ax.annotate(f'Max Pre: {max_pre}', (max_pre_index, max_pre), textcoords=\"offset points\", xytext=(0,10), ha='center', fontsize=12)\n",
    "ax.annotate(f'Max Post: {max_post}', (max_post_index, max_post), textcoords=\"offset points\", xytext=(0,10), ha='center', fontsize=12)\n",
    "\n",
    "# Enhancing the visual elements\n",
    "ax.set_title('Total Rewards Over Runs Before and After Tuning', fontsize=18)\n",
    "ax.set_xlabel('Runs', fontsize=16)\n",
    "ax.set_ylabel('Total Rewards', fontsize=16)\n",
    "ax.tick_params(axis='both', which='major', labelsize=14)\n",
    "\n",
    "ax.legend(fontsize=14, loc='best',frameon=True, facecolor='white', edgecolor='black')\n",
    "ax.grid(True, which='both', linestyle='--', linewidth=0.5)\n",
    "\n",
    "# To avoid clipping of the annotation\n",
    "fig.tight_layout()\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "763f74a6-8534-4e23-b3e5-2a21dda17a41",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "plt.style.use('seaborn-darkgrid')\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "window_size = 3\n",
    "pre_tuned_ma = np.convolve(pre_tuned_rewards, np.ones(window_size)/window_size, mode='valid')\n",
    "post_tuned_ma = np.convolve(post_tuned_rewards, np.ones(window_size)/window_size, mode='valid')\n",
    "\n",
    "plt.plot(pre_tuned_rewards, label='Pre-Tuned', marker='o', linestyle='', color='blue', alpha=0.5, markersize=8)\n",
    "plt.plot(post_tuned_rewards, label='Post-Tuned', marker='x', linestyle='', color='orange', alpha=0.5, markersize=8)\n",
    "\n",
    "plt.plot(range(window_size-1, len(pre_tuned_ma)+window_size-1), pre_tuned_ma, color='blue', linestyle='-', linewidth=2, alpha=0.7, label='Pre-Tuned MA')\n",
    "plt.plot(range(window_size-1, len(post_tuned_ma)+window_size-1), post_tuned_ma, color='orange', linestyle='--', linewidth=2, alpha=0.7, label='Post-Tuned MA')\n",
    "\n",
    "max_pre_index = np.argmax(pre_tuned_rewards)\n",
    "max_post_index = np.argmax(post_tuned_rewards)\n",
    "plt.scatter(max_pre_index, pre_tuned_rewards[max_pre_index], color='darkblue', s=100, zorder=5, edgecolor='black')\n",
    "plt.scatter(max_post_index, post_tuned_rewards[max_post_index], color='darkorange', s=100, zorder=5, edgecolor='black')\n",
    "\n",
    "plt.annotate(f'Max Pre: {pre_tuned_rewards[max_pre_index]}', (max_pre_index, pre_tuned_rewards[max_pre_index]), textcoords=\"offset points\", xytext=(-10,10), ha='center')\n",
    "plt.annotate(f'Max Post: {post_tuned_rewards[max_post_index]}', (max_post_index, post_tuned_rewards[max_post_index]), textcoords=\"offset points\", xytext=(-10,10), ha='center')\n",
    "\n",
    "plt.title('Total Rewards Over Runs Before and After Tuning', fontsize=18)\n",
    "plt.xlabel('Runs', fontsize=16)\n",
    "plt.ylabel('Total Rewards', fontsize=16)\n",
    "plt.xticks(fontsize=14)\n",
    "plt.yticks(fontsize=14)\n",
    "plt.legend(fontsize=14, loc='best',frameon=True, facecolor='white', edgecolor='black')\n",
    "\n",
    "plt.grid(True, which='major', linestyle='--', linewidth=0.75)\n",
    "plt.grid(True, which='minor', linestyle=':', linewidth=0.5)\n",
    "plt.minorticks_on()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f76f7f9b-8f1a-4780-a00d-efaa1f482cac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "plt.style.use('seaborn-darkgrid')\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "window_size = 3\n",
    "pre_tuned_ma = np.convolve(pre_tuned_rewards, np.ones(window_size)/window_size, mode='valid')\n",
    "post_tuned_ma = np.convolve(post_tuned_rewards, np.ones(window_size)/window_size, mode='valid')\n",
    "\n",
    "plt.plot(pre_tuned_rewards, 'o-', label='Pre-Tuned', color='blue', markersize=6, linestyle='-', linewidth=1.5, alpha=0.7)\n",
    "plt.plot(post_tuned_rewards, 'x--', label='Post-Tuned', color='orange', markersize=6, linestyle='--', linewidth=1.5, alpha=0.7)\n",
    "\n",
    "plt.plot(range(window_size-1, len(pre_tuned_ma)+window_size-1), pre_tuned_ma, color='green', linestyle='-', linewidth=2, label='Pre-Tuned MA', alpha=0.7)\n",
    "plt.plot(range(window_size-1, len(post_tuned_ma)+window_size-1), post_tuned_ma, color='red', linestyle='--', linewidth=2, label='Post-Tuned MA', alpha=0.7)\n",
    "\n",
    "\n",
    "max_pre_index = np.argmax(pre_tuned_rewards)\n",
    "max_post_index = np.argmax(post_tuned_rewards)\n",
    "plt.scatter(max_pre_index, pre_tuned_rewards[max_pre_index], color='navy', s=100, zorder=5, edgecolor='black', alpha=0.8)\n",
    "plt.scatter(max_post_index, post_tuned_rewards[max_post_index], color='chocolate', s=100, zorder=5, edgecolor='black', alpha=0.8)\n",
    "plt.annotate(f'Max Pre: {pre_tuned_rewards[max_pre_index]}', (max_pre_index, pre_tuned_rewards[max_pre_index]), textcoords=\"offset points\", xytext=(0,10), ha='center', fontsize=12)\n",
    "plt.annotate(f'Max Post: {post_tuned_rewards[max_post_index]}', (max_post_index, post_tuned_rewards[max_post_index]), textcoords=\"offset points\", xytext=(0,10), ha='center', fontsize=12)\n",
    "\n",
    "plt.title('Total Rewards Over Runs Before and After Tuning', fontsize=18)\n",
    "plt.xlabel('Runs', fontsize=16)\n",
    "plt.ylabel('Total Rewards', fontsize=16)\n",
    "plt.xticks(fontsize=14)\n",
    "plt.yticks(fontsize=14)\n",
    "plt.legend(fontsize=14, loc='best',frameon=True, facecolor='white', edgecolor='black')\n",
    "\n",
    "plt.grid(True, which='major', linestyle='--', linewidth=0.75)\n",
    "plt.grid(True, which='minor', linestyle=':', linewidth=0.5)\n",
    "plt.minorticks_on()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de1580e8-62f6-496e-b4d5-06f86aeba43e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import scipy.stats as stats\n",
    "\n",
    "plt.style.use('seaborn-darkgrid')\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "window_size = 3\n",
    "\n",
    "# Function to calculate moving average and confidence interval\n",
    "def moving_average_with_ci(data, window_size):\n",
    "    ma = np.convolve(data, np.ones(window_size)/window_size, mode='valid')\n",
    "    \n",
    "    # Calculating standard deviation within the window for confidence intervals\n",
    "    std_devs = [np.std(data[max(0, i-window_size+1):i+1]) for i in range(window_size-1, len(data))]\n",
    "    ci = stats.norm.interval(0.95, loc=ma, scale=std_devs/np.sqrt(window_size))\n",
    "    return ma, ci\n",
    "\n",
    "# Applying the function to pre-tuned and post-tuned rewards\n",
    "pre_tuned_ma, pre_tuned_ci = moving_average_with_ci(pre_tuned_rewards, window_size)\n",
    "post_tuned_ma, post_tuned_ci = moving_average_with_ci(post_tuned_rewards, window_size)\n",
    "\n",
    "# Plotting raw data\n",
    "plt.plot(pre_tuned_rewards, 'o-', color='deepskyblue', markersize=6, alpha=0.5, label='Pre-Tuned Raw')\n",
    "plt.plot(post_tuned_rewards, 'x--', color='coral', markersize=6, alpha=0.5, label='Post-Tuned Raw')\n",
    "\n",
    "# Plotting moving averages\n",
    "plt.plot(range(window_size-1, len(pre_tuned_ma)+window_size-1), pre_tuned_ma, color='navy', linestyle='-', linewidth=2, label='Pre-Tuned MA')\n",
    "plt.plot(range(window_size-1, len(post_tuned_ma)+window_size-1), post_tuned_ma, color='darkred', linestyle='--', linewidth=2, label='Post-Tuned MA')\n",
    "\n",
    "# Plotting confidence intervals\n",
    "plt.fill_between(range(window_size-1, len(pre_tuned_ma)+window_size-1), pre_tuned_ci[0], pre_tuned_ci[1], color='deepskyblue', alpha=0.2, label='Pre-Tuned CI')\n",
    "plt.fill_between(range(window_size-1, len(post_tuned_ma)+window_size-1), post_tuned_ci[0], post_tuned_ci[1], color='coral', alpha=0.2, label='Post-Tuned CI')\n",
    "\n",
    "# Highlight and label the maximum points\n",
    "max_pre_index = np.argmax(pre_tuned_rewards)\n",
    "max_post_index = np.argmax(post_tuned_rewards)\n",
    "plt.scatter([max_pre_index], [pre_tuned_rewards[max_pre_index]], color='blue', s=100, label='Max Pre-Tuned', zorder=5)\n",
    "plt.scatter([max_post_index], [post_tuned_rewards[max_post_index]], color='red', s=100, label='Max Post-Tuned', zorder=5)\n",
    "plt.text(max_pre_index, pre_tuned_rewards[max_pre_index], ' Max Pre', verticalalignment='bottom')\n",
    "plt.text(max_post_index, post_tuned_rewards[max_post_index], ' Max Post', verticalalignment='bottom')\n",
    "\n",
    "plt.title('Total Rewards Over Runs Before and After Tuning with Moving Averages and Confidence Intervals', fontsize=14)\n",
    "plt.xlabel('Runs', fontsize=12)\n",
    "plt.ylabel('Total Rewards', fontsize=12)\n",
    "plt.legend(loc='upper left', bbox_to_anchor=(1, 1))  # Move legend outside the plot\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15d2ae32-dae2-4f3e-95db-fc562a3fd419",
   "metadata": {
    "id": "15d2ae32-dae2-4f3e-95db-fc562a3fd419"
   },
   "source": [
    "Pre-tuning Attacker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5de90a73-0947-4cac-9834-150d320986d8",
   "metadata": {
    "id": "5de90a73-0947-4cac-9834-150d320986d8"
   },
   "outputs": [],
   "source": [
    "def run_training_with_hyperparameters(gym_env, environment_properties,\n",
    "                                      epsilon, gamma, learning_rate, epsilon_exponential_decay, epsilon_minimum, episode_count, iteration_count, epsilon_multdecay):\n",
    "\n",
    "    trained_attacker = learner.epsilon_greedy_search(\n",
    "        gym_env,\n",
    "        ep,\n",
    "        a.QTabularLearner(ep, gamma=gamma, learning_rate=learning_rate, exploit_percentile=100),\n",
    "        episode_count=episode_count,\n",
    "        iteration_count=iteration_count,\n",
    "        epsilon=epsilon,\n",
    "        render=True,\n",
    "        epsilon_multdecay=epsilon_multdecay,  # 0.999,\n",
    "        epsilon_exponential_decay=epsilon_exponential_decay,\n",
    "        epsilon_minimum=epsilon_minimum,\n",
    "        verbosity=Verbosity.Quiet,\n",
    "        title=\"Q-learning\"\n",
    "    )\n",
    "    total_reward = sum([sum(rewards) for rewards in trained_attacker['all_episodes_rewards']])\n",
    "    return total_reward\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc72e8a3-3472-46db-af84-ad511b4762d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "\n",
    "# Create an instance of the CyberBattleChain environment without the defender\n",
    "gym_env = gym.make('CyberBattleChain-v0',\n",
    "    size=12,\n",
    "    attacker_goal=AttackerGoal(\n",
    "        own_atleast=0,\n",
    "        own_atleast_percent=1.0\n",
    "    ),\n",
    "    defender_constraint=DefenderConstraint(\n",
    "        maintain_sla=0.80\n",
    "    )\n",
    ")\n",
    "\n",
    "defender_name = \"DefenderName\"  \n",
    "defender_agent = DefenderWrapper(gym_env,defender_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8edd12ad-cb63-4c9d-aaf0-e240d1a2e50a",
   "metadata": {
    "id": "8edd12ad-cb63-4c9d-aaf0-e240d1a2e50a",
    "outputId": "d9111041-a0b2-475c-f58e-32b6e633edfb"
   },
   "outputs": [],
   "source": [
    "random.seed(120394016)\n",
    "# Parameter ranges\n",
    "epsilon = 1\n",
    "#gamma=np.linspace(0.05,0.09,10)\n",
    "gamma= 0.0\n",
    "#learning_rate=np.linspace(0.3,.35,10)\n",
    "learning_rate=0.0\n",
    "#epsilon_decay=np.linspace(2333.33333,5000,20)\n",
    "epsilon_exponential_decay=1000\n",
    "#epsilon_minimum=np.linspace(0.01,.07777777,10)\n",
    "epsilon_minimum=0.0\n",
    "episode_count=10\n",
    "iteration_count=10\n",
    "#epsilon_multdecay=np.linspace(0.3,0.875,20)\n",
    "epsilon_multdecay=0.0\n",
    "rewards=[]\n",
    "for i in range(10):\n",
    "    attackerreward=run_training_with_hyperparameters(gym_env, ep,\n",
    "                                      epsilon, gamma, learning_rate, epsilon_exponential_decay, epsilon_minimum, episode_count, iteration_count, epsilon_multdecay)\n",
    "    rewards.append(attackerreward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79369242-c6f5-41c4-a926-f5aa6d0dfaa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6a256e0-8698-4a70-8a09-6256b0420fa8",
   "metadata": {
    "id": "e6a256e0-8698-4a70-8a09-6256b0420fa8"
   },
   "outputs": [],
   "source": [
    "pre_tuned_rewards=[-224.0,\n",
    " -423.0,\n",
    " -228.0,\n",
    " -160.0,\n",
    " -156.0,\n",
    " -199.0,\n",
    " -287.0,\n",
    " -215.0,\n",
    " -331.0,\n",
    " -314.0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cabcf8e9-1e9f-4a37-ab6d-0dacbd2f7bb5",
   "metadata": {
    "id": "cabcf8e9-1e9f-4a37-ab6d-0dacbd2f7bb5"
   },
   "source": [
    "Post-tuning Attacker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c14bfd6-b6ae-4050-8290-821f48347650",
   "metadata": {
    "id": "1c14bfd6-b6ae-4050-8290-821f48347650",
    "outputId": "a931c026-ab73-4959-c6cf-9efd8bd304ff"
   },
   "outputs": [],
   "source": [
    "random.seed(120394016)\n",
    "# Parameter ranges\n",
    "epsilon = 0.9629629629629622\n",
    "#gamma=np.linspace(0.05,0.09,10)\n",
    "gamma= 0.05888888888888889\n",
    "#learning_rate=np.linspace(0.3,.35,10)\n",
    "learning_rate=0.3222222222222222\n",
    "#epsilon_decay=np.linspace(2333.33333,5000,20)\n",
    "epsilon_exponential_decay=5000\n",
    "#epsilon_minimum=np.linspace(0.01,.07777777,10)\n",
    "epsilon_minimum=0.0513409942068965\n",
    "episode_count=10\n",
    "iteration_count=10\n",
    "#epsilon_multdecay=np.linspace(0.3,0.875,20)\n",
    "epsilon_multdecay=0.6631578947368421\n",
    "rewards1=[]\n",
    "for i in range(10):\n",
    "    attackerreward=run_training_with_hyperparameters(gym_env, ep,\n",
    "                                      epsilon, gamma, learning_rate, epsilon_exponential_decay, epsilon_minimum, episode_count, iteration_count, epsilon_multdecay)\n",
    "    rewards1.append(attackerreward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "923d1ec8-7634-40c8-b323-e8bedefa60b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "post_tuned_rewards=[-140.0,\n",
    " -261.0,\n",
    " -232.0,\n",
    " -158.0,\n",
    " -376.0,\n",
    " -356.0,\n",
    " -290.0,\n",
    " -165.0,\n",
    " -371.0,\n",
    " -410.0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cfac2aa-c766-417f-b936-0e20bed21953",
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(120394016)\n",
    "# Parameter ranges\n",
    "epsilon = 0.9629629629629622\n",
    "#gamma=np.linspace(0.05,0.09,10)\n",
    "gamma= 1-0.05888888888888889\n",
    "#learning_rate=np.linspace(0.3,.35,10)\n",
    "learning_rate=1-0.3222222222222222\n",
    "#epsilon_decay=np.linspace(2333.33333,5000,20)\n",
    "epsilon_exponential_decay=5000\n",
    "#epsilon_minimum=np.linspace(0.01,.07777777,10)\n",
    "epsilon_minimum=0.0513409942068965\n",
    "episode_count=10\n",
    "iteration_count=10\n",
    "#epsilon_multdecay=np.linspace(0.3,0.875,20)\n",
    "epsilon_multdecay=0.6631578947368421\n",
    "rewards2=[]\n",
    "for i in range(10):\n",
    "    attackerreward=run_training_with_hyperparameters(gym_env, ep,\n",
    "                                      epsilon, gamma, learning_rate, epsilon_exponential_decay, epsilon_minimum, episode_count, iteration_count, epsilon_multdecay)\n",
    "    rewards2.append(attackerreward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "275d0c99-2347-467d-b7b2-43832b34a3a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "rewards2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fc837e4-c0ba-410d-9539-1be266204d41",
   "metadata": {
    "id": "2fc837e4-c0ba-410d-9539-1be266204d41"
   },
   "outputs": [],
   "source": [
    "rewards6=[-1357.0,\n",
    " -1298.0,\n",
    " -1246.0,\n",
    " -1328.0,\n",
    " -1099.0,\n",
    " -1805.0,\n",
    " -1233.0,\n",
    " -1531.0,\n",
    " -1257.0,\n",
    " -1027.0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af38315a-4b17-4e75-9199-4f72b822681f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy import stats\n",
    "\n",
    "# Step 1: Check for normality\n",
    "print(\"Normality Check (Shapiro-Wilk Test):\")\n",
    "print(\"Pre-Tuned:\", stats.shapiro(pre_tuned_rewards))\n",
    "print(\"Post-Tuned:\", stats.shapiro(post_tuned_rewards))\n",
    "\n",
    "# Step 1: Check for equal variances\n",
    "print(\"\\nEqual Variances Check (Levene's Test):\")\n",
    "print(stats.levene(pre_tuned_rewards, post_tuned_rewards))\n",
    "\n",
    "# Step 2: Choose and perform the test\n",
    "# If both p-values from Shapiro and Levene's test are > 0.05\n",
    "print(\"\\nT-Test (if assumptions are met):\")\n",
    "print(stats.ttest_ind(pre_tuned_rewards, post_tuned_rewards))\n",
    "\n",
    "# If any assumption is not met, use Mann-Whitney U test instead\n",
    "print(\"\\nMann-Whitney U Test (if assumptions are not met):\")\n",
    "print(stats.mannwhitneyu(pre_tuned_rewards, post_tuned_rewards))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85147d1b-664f-42f1-99bb-2292c8f18a28",
   "metadata": {
    "id": "85147d1b-664f-42f1-99bb-2292c8f18a28",
    "outputId": "e1e121d5-14b9-4dcc-cb74-671b9bf63beb"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "plt.figure(figsize=(12, 7))\n",
    "\n",
    "# Enhanced color scheme for boxes\n",
    "colors = ['#1f77b4', '#ff7f0e']\n",
    "\n",
    "# Custom boxplots\n",
    "box = plt.boxplot([pre_tuned_rewards, post_tuned_rewards], labels=['Pre-Tuned', 'Post-Tuned'],\n",
    "            patch_artist=True,\n",
    "            #notch=True,  # Adds a notch for median confidence interval\n",
    "            meanline=True,\n",
    "            showmeans=True,\n",
    "            showfliers=True,  # Show outliers\n",
    "            flierprops=dict(marker='o', markerfacecolor='red', markersize=6, linestyle='none'),\n",
    "            medianprops=dict(linestyle='-', linewidth=2.5, color='yellow'),\n",
    "            meanprops=dict(linestyle='--', linewidth=2, color='black'),\n",
    "            whiskerprops=dict(color='gray', linewidth=2),\n",
    "            capprops=dict(color='gray', linewidth=2))\n",
    "\n",
    "for patch, color in zip(box['boxes'], colors):\n",
    "    patch.set_facecolor(color)\n",
    "    patch.set_edgecolor('black')  # Box edges\n",
    "    patch.set_linewidth(2)  # Box edge linewidth\n",
    "\n",
    "for i, data in enumerate([pre_tuned_rewards, post_tuned_rewards]):\n",
    "    x = np.random.normal(i + 1, 0.04, size=len(data))\n",
    "    plt.scatter(x, data, alpha=0.7, color='darkgreen', edgecolor='black', zorder=10)\n",
    "\n",
    "plt.title('Comparison of Total Rewards Before and After Tuning', fontsize=18, fontweight='bold')\n",
    "plt.ylabel('Total Rewards', fontsize=16, fontweight='regular')\n",
    "plt.xticks(fontsize=14, fontweight='bold')\n",
    "plt.yticks(fontsize=12, fontweight='regular')\n",
    "\n",
    "# Grid and background\n",
    "plt.gca().set_facecolor('#EAEAF2')  # Light background color\n",
    "plt.grid(True, linestyle='-', linewidth=0.5, color='white', which='major')  # White grid for contrast\n",
    "plt.minorticks_on()\n",
    "plt.grid(True, linestyle=':', linewidth=0.3, color='white', which='minor')  # Minor grid\n",
    "\n",
    "# Legend for custom elements\n",
    "plt.legend([box[\"medians\"][0], box[\"means\"][0], box['fliers'][0]], ['Median', 'Mean'], loc='upper right', fontsize=12)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b39fe8e-aba4-44a5-96e5-9ad184a0d147",
   "metadata": {
    "id": "7b39fe8e-aba4-44a5-96e5-9ad184a0d147",
    "outputId": "153438bb-1354-4fcd-f50c-c697c11afbbf"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(12, 8))  # Enlarge the plot for better visibility\n",
    "\n",
    "# Plotting with enhanced visibility and distinct markers\n",
    "plt.plot(pre_tuned_rewards, label='Pre-Tuned', marker='o', markersize=8, linestyle='-', linewidth=2, color='royalblue')\n",
    "plt.plot(post_tuned_rewards, label='Post-Tuned', marker='X', markersize=8, linestyle='--', linewidth=2, color='darkorange')\n",
    "\n",
    "# Highlight the maximum values for each series\n",
    "max_pre_index, max_pre_value = max(enumerate(pre_tuned_rewards), key=lambda x: x[1])\n",
    "max_post_index, max_post_value = max(enumerate(post_tuned_rewards), key=lambda x: x[1])\n",
    "\n",
    "plt.scatter(max_pre_index, max_pre_value, color='red', s=100, edgecolors='black', zorder=5, label='Max Pre-Tuned')\n",
    "plt.scatter(max_post_index, max_post_value, color='green', s=100, edgecolors='black', zorder=5, label='Max Post-Tuned')\n",
    "\n",
    "# Annotations for maximum values\n",
    "plt.annotate(f'Max: {max_pre_value}', (max_pre_index, max_pre_value), textcoords=\"offset points\", xytext=(0,10), ha='center', fontsize=12)\n",
    "plt.annotate(f'Max: {max_post_value}', (max_post_index, max_post_value), textcoords=\"offset points\", xytext=(0,10), ha='center', fontsize=12)\n",
    "\n",
    "# Enhancing plot aesthetics\n",
    "plt.title('Total Rewards Over Runs Before and After Tuning', fontsize=16)\n",
    "plt.xlabel('Runs', fontsize=14)\n",
    "plt.ylabel('Total Rewards', fontsize=14)\n",
    "plt.xticks(fontsize=12)\n",
    "plt.yticks(fontsize=12)\n",
    "plt.legend(fontsize=12)\n",
    "plt.grid(True, linestyle='--', linewidth=0.5, alpha=0.7)  # Adding a grid for better readability\n",
    "plt.tight_layout() \n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "473fd512-a262-44bb-9ed3-3f225c72ad89",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "mean_pre_tuned = np.mean(pre_tuned_rewards)\n",
    "mean_post_tuned = np.mean(post_tuned_rewards)\n",
    "\n",
    "std_pre_tuned = np.std(pre_tuned_rewards)\n",
    "std_post_tuned = np.std(post_tuned_rewards)\n",
    "\n",
    "plt.plot(pre_tuned_rewards, label='Pre-Tuned', marker='o', markersize=8, linestyle='-', linewidth=2, color='black')\n",
    "plt.plot(post_tuned_rewards, label='Post-Tuned', marker='X', markersize=8, linestyle='--', linewidth=2, color='green')\n",
    "\n",
    "plt.axhline(y=mean_pre_tuned, color='black', linestyle='-', alpha=0.7, label='Mean Pre-Tuned')\n",
    "plt.axhline(y=mean_post_tuned, color='darkgreen', linestyle='--', alpha=0.7, label='Mean Post-Tuned')\n",
    "\n",
    "max_pre_index, max_pre_value = max(enumerate(pre_tuned_rewards), key=lambda x: x[1])\n",
    "max_post_index, max_post_value = max(enumerate(post_tuned_rewards), key=lambda x: x[1])\n",
    "plt.annotate(f'Max Pre: {max_pre_value}', (max_pre_index, max_pre_value), textcoords=\"offset points\", xytext=(0,10), ha='center')\n",
    "plt.annotate(f'Max Post: {max_post_value}', (max_post_index, max_post_value), textcoords=\"offset points\", xytext=(0,10), ha='center')\n",
    "\n",
    "plt.title('Total Rewards Over Runs Before and After Tuning', fontsize=16)\n",
    "plt.xlabel('Runs', fontsize=14)\n",
    "plt.ylabel('Total Rewards', fontsize=14)\n",
    "plt.xticks(fontsize=12)\n",
    "plt.yticks(fontsize=12)\n",
    "plt.legend(fontsize=12, loc='upper right')\n",
    "plt.grid(True, linestyle='--', linewidth=0.5, alpha=0.7)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4af2e476-95bf-48c2-b244-ed94efa12b21",
   "metadata": {
    "id": "4af2e476-95bf-48c2-b244-ed94efa12b21"
   },
   "source": [
    "# compare 10 episodes, 50 episodes, 10 iterations, 200 iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad6f960a-6a60-4b4a-aaa0-5f3136b8a923",
   "metadata": {},
   "outputs": [],
   "source": [
    "from cyberbattle.agents.baseline.agent_wrapper import ActionTrackingStateAugmentation, StateAugmentation\n",
    "from cyberbattle.agents.baseline.plotting import PlotTraining, plot_averaged_cummulative_rewards\n",
    "import progressbar\n",
    "import math\n",
    "from cyberbattle.agents.baseline import learner\n",
    "import sys\n",
    "from cyberbattle.simulation.model import FirewallRule\n",
    "import gym\n",
    "\n",
    "# Create an instance of the CyberBattleChain environment without the defender\n",
    "gym_env = gym.make('CyberBattleChain-v0',\n",
    "    size=12,\n",
    "    attacker_goal=AttackerGoal(\n",
    "        own_atleast=0,\n",
    "        own_atleast_percent=1.0\n",
    "    ),\n",
    "    defender_constraint=DefenderConstraint(\n",
    "        maintain_sla=0.80\n",
    "    )\n",
    ")\n",
    "\n",
    "defender_name = \"DefenderName\" \n",
    "defender_agent = DefenderWrapper(gym_env,defender_name)\n",
    "ep = w.EnvironmentBounds.of_identifiers(\n",
    "    maximum_total_credentials=22,\n",
    "    maximum_node_count=22,\n",
    "    identifiers=gym_env.identifiers\n",
    ")\n",
    "#cyberbattlechain_defender.render_as_fig()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf582cf0-b346-4c9c-9617-48ae02080c15",
   "metadata": {
    "id": "683cc5df-2f51-4333-bb8b-d3b1b3e54965",
    "outputId": "0935a2f1-2687-42e8-8101-e135086fba7d"
   },
   "outputs": [],
   "source": [
    "# Parameter ranges\n",
    "epsilon = 0.9629629629629622\n",
    "#gamma=np.linspace(0.05,0.09,10)\n",
    "gamma= 0.05888888888888889\n",
    "#learning_rate=np.linspace(0.3,.35,10)\n",
    "random.seed(120394016)\n",
    "learning_rate=0.3222222222222222\n",
    "#epsilon_decay=np.linspace(2333.33333,5000,20)\n",
    "epsilon_exponential_decay=5000\n",
    "#epsilon_minimum=np.linspace(0.01,.07777777,10)\n",
    "epsilon_minimum=0.0513409942068965\n",
    "episode_count=10\n",
    "iteration_count=10\n",
    "#epsilon_multdecay=np.linspace(0.3,0.875,20)\n",
    "epsilon_multdecay=0.6631578947368421\n",
    "\n",
    "trained_attacker = learner.epsilon_greedy_search(\n",
    "        gym_env,\n",
    "        ep,\n",
    "        a.QTabularLearner(ep, gamma=0.05888888888888889, learning_rate=0.3222222222222222, exploit_percentile=100),\n",
    "        episode_count=episode_count,\n",
    "        iteration_count=iteration_count,\n",
    "        epsilon=epsilon,\n",
    "        render=True,\n",
    "        epsilon_multdecay=epsilon_multdecay,  # 0.999,\n",
    "        epsilon_exponential_decay=epsilon_exponential_decay,\n",
    "        epsilon_minimum=epsilon_minimum,\n",
    "        verbosity=Verbosity.Quiet,\n",
    "        title=\"Q-learning\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d3de530-6714-4f61-b2f0-08d25f926fc9",
   "metadata": {
    "id": "5d3de530-6714-4f61-b2f0-08d25f926fc9"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d938d119-3e1f-44ad-a035-7cd5db2eb2eb",
   "metadata": {
    "id": "347e5233-be33-4949-8979-42ad5a3b4bde",
    "outputId": "2679c90d-d1eb-400e-cf29-7a266e967c88"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d22ed739-6b30-4aea-9aa0-4042c7905000",
   "metadata": {
    "id": "0217ad25-6c84-4067-83c7-a3564223c2d8",
    "outputId": "b532ee1d-2f57-490b-9c4e-6c3906cba851"
   },
   "outputs": [],
   "source": [
    "import cyberbattle.agents.baseline.agent_tabularqlearning as a\n",
    "import cyberbattle.agents.baseline.learner as learner\n",
    "from cyberbattle.agents.baseline.agent_wrapper import Verbosity\n",
    "random.seed(120394016)\n",
    "epsilon = 0.66666666666666\n",
    "gamma= 0.06925\n",
    "learning_rate= 0.34444444444444444\n",
    "epsilon_exponential_decay=3198.830407105263\n",
    "epsilon_minimum=0.05708811937931034\n",
    "episode_count=10\n",
    "iteration_count=10\n",
    "epsilon_multdecay=0.7777777777777778\n",
    "statespace = w.HashEncoding(ep, [\n",
    "            # Feature_discovered_node_count(),\n",
    "            # Feature_discovered_credential_count(),\n",
    "            w.Feature_discovered_ports_sliding(ep),\n",
    "            w.Feature_discovered_nodeproperties_sliding(ep),\n",
    "            w.Feature_discovered_notowned_node_count(ep, 3)\n",
    "        ], 5000)  # should not be too small, pick something big to avoid collision\n",
    "\n",
    "actionspace = w.RavelEncoding(ep, [\n",
    "            w.Feature_active_node_properties(ep)])\n",
    "print(statespace)\n",
    "print(actionspace)\n",
    "q_matrix = DefenderQMatrix(statespace, actionspace,gym_env)\n",
    "learner2 = DefenderQLearner(gym_env, defender_agent, ep, q_matrix, epsilon=0.9, gamma=0.015, learning_rate=.07)\n",
    "random.seed(120394016)\n",
    "#%matplotlib inline\n",
    "\n",
    "trained_defender = epsilon_greedy_defender_training(\n",
    "        cyberbattle_gym_env=gym_env,\n",
    "        environment_properties=ep,\n",
    "        learner=learner2,\n",
    "        episode_count=episode_count,  # Set the number of episodes for the training\n",
    "        iteration_count=iteration_count,  # Set the number of iterations per episode\n",
    "        epsilon=epsilon,\n",
    "        epsilon_exponential_decay=epsilon_exponential_decay,\n",
    "        epsilon_minimum=epsilon_minimum,\n",
    "        epsilon_multdecay=epsilon_multdecay,\n",
    "        verbosity=logging.INFO,\n",
    "        render=True,\n",
    "        plot_episodes_length=True,\n",
    "        title=\"Defender Q-Learning Training\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0a2acd4-4bd4-4cb2-b99f-f7bbdbf87f2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "\n",
    "gym_env = gym.make('CyberBattleChain-v0',\n",
    "    size=12,\n",
    "    attacker_goal=AttackerGoal(\n",
    "        own_atleast=0,\n",
    "        own_atleast_percent=1.0\n",
    "    ),\n",
    "    defender_constraint=DefenderConstraint(\n",
    "        maintain_sla=0.80\n",
    "    )\n",
    ")\n",
    "\n",
    "defender_name = \"DefenderName\" \n",
    "defender_agent = DefenderWrapper(gym_env,defender_name)\n",
    "\n",
    "gym_environment = gym.make('CyberBattleChain-v0',\n",
    "    size=12,\n",
    "    attacker_goal=AttackerGoal(\n",
    "        own_atleast=0,\n",
    "        own_atleast_percent=1.0\n",
    "    ),\n",
    "    defender_constraint=DefenderConstraint(\n",
    "        maintain_sla=0.80\n",
    "    ),\n",
    "    defender_agent=defender_agent\n",
    ")\n",
    "\n",
    "#this was made due to issues where the attacker q learner needed to use a wrapper environment\n",
    "from cyberbattle.agents.baseline.agent_wrapper import AgentWrapper, StateAugmentation\n",
    "def wrap_environment(env):\n",
    "    initial_observation = env.reset()  \n",
    "    state = StateAugmentation(initial_observation)  \n",
    "    return AgentWrapper(env, state)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdcb1708-2f16-4284-bbb5-539cb1e028df",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class SimulationTracker:\n",
    "    def __init__(self):\n",
    "        self.attacker_rewards = []\n",
    "        self.defender_rewards = []\n",
    "        self.actions_taken = {'attacker': [], 'defender': []}\n",
    "        self.exploration_actions = {'attacker': 0, 'defender': 0}\n",
    "        self.exploitation_actions = {'attacker': 0, 'defender': 0}\n",
    "        self.episode_lengths = []\n",
    "        self.reward_variability = []\n",
    "        self.state_visitation_frequencies = {}\n",
    "        self.temporal_differences = []\n",
    "        self.infected_nodes = []\n",
    "\n",
    "    def log_episode(self, attacker_reward, defender_reward, episode_length, episode_rewards):\n",
    "        self.attacker_rewards.append(attacker_reward)\n",
    "        self.defender_rewards.append(defender_reward)\n",
    "        self.episode_lengths.append(episode_length)\n",
    "        self.reward_variability.append(np.var(episode_rewards))\n",
    "        #self.temporal_differences.append(temporal_difference)\n",
    "\n",
    "    def log_action(self, role, action, exploration):\n",
    "        action_str = str(action)  # Convert the action dictionary to a string\n",
    "        self.actions_taken[role].append(action_str)\n",
    "        if exploration:\n",
    "            self.exploration_actions[role] += 1\n",
    "        else:\n",
    "            self.exploitation_actions[role] += 1\n",
    "\n",
    "\n",
    "    def log_state_visitation(self, state):\n",
    "        if state not in self.state_visitation_frequencies:\n",
    "            self.state_visitation_frequencies[state] = 1\n",
    "        else:\n",
    "            self.state_visitation_frequencies[state] += 1\n",
    "\n",
    "    def log_infected_nodes(self, nodes_count):\n",
    "        self.infected_nodes.append(nodes_count)\n",
    "\n",
    "    def print_summary(self):\n",
    "        print(\"Simulation Summary:\")\n",
    "        print(f\"Average Attacker Reward: {np.mean(self.attacker_rewards)}\")\n",
    "        print(f\"Average Defender Reward: {np.mean(self.defender_rewards)}\")\n",
    "        print(f\"Average Episode Length: {np.mean(self.episode_lengths)} steps\")\n",
    "        print(f\"Average Reward Variability: {np.mean(self.reward_variability)}\")\n",
    "        print(f\"Average Temporal Difference: {np.mean(self.temporal_differences)}\")\n",
    "        print(f\"Average Infected Nodes per Episode: {np.mean(self.infected_nodes)}\")\n",
    "\n",
    "        print(\"\\nExploration vs. Exploitation:\")\n",
    "        for role in ['attacker', 'defender']:\n",
    "            total_actions = self.exploration_actions[role] + self.exploitation_actions[role]\n",
    "            if total_actions > 0:\n",
    "                print(f\"{role.capitalize()} Exploration: {self.exploration_actions[role] / total_actions:.2f}\")\n",
    "                print(f\"{role.capitalize()} Exploitation: {self.exploitation_actions[role] / total_actions:.2f}\")\n",
    "\n",
    "        self._print_action_distributions()\n",
    "\n",
    "        self._plot_rewards()\n",
    "\n",
    "    def _print_action_distributions(self):\n",
    "        print(\"\\nAction Distributions:\")\n",
    "        for role, actions in self.actions_taken.items():\n",
    "            unique_actions, counts = np.unique(actions, return_counts=True)\n",
    "            print(f\"{role.capitalize()} Actions:\")\n",
    "            for action, count in zip(unique_actions, counts):\n",
    "                print(f\"Action {action}: {count} times\")\n",
    "\n",
    "\n",
    "    def _plot_rewards(self):\n",
    "        plt.figure(figsize=(10, 5))\n",
    "        plt.plot(self.attacker_rewards, label='Attacker Rewards')\n",
    "        plt.plot(self.defender_rewards, label='Defender Rewards')\n",
    "        plt.xlabel('Episode')\n",
    "        plt.ylabel('Reward')\n",
    "        plt.title('Rewards per Episode for Attacker and Defender')\n",
    "        plt.legend()\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2620f7de-b930-454d-ac10-675c174d6283",
   "metadata": {},
   "outputs": [],
   "source": [
    "attacker_learner = trained_attacker['learner']\n",
    "defender_learner = trained_defender['learner']"
   ]
  },
  {
   "cell_type": "raw",
   "id": "ff8f2fad-d4be-4d85-80fd-dd357b5497a1",
   "metadata": {
    "id": "f865e871-cfed-418c-af71-1de77cda203e",
    "outputId": "6d8d569c-3051-487a-e74a-b85752c04595"
   },
   "source": [
    "import cyberbattle.agents.baseline.agent_tabularqlearning as a\n",
    "import cyberbattle.agents.baseline.learner as learner\n",
    "from cyberbattle.agents.baseline.agent_wrapper import Verbosity\n",
    "epsilon = 0.66666666666666\n",
    "gamma= 0.06925\n",
    "learning_rate= 0.34444444444444444\n",
    "epsilon_exponential_decay=3198.830407105263\n",
    "epsilon_minimum=0.05708811937931034\n",
    "episode_count=50\n",
    "iteration_count=200\n",
    "epsilon_multdecay=0.7777777777777778\n",
    "statespace = w.HashEncoding(ep, [\n",
    "            # Feature_discovered_node_count(),\n",
    "            # Feature_discovered_credential_count(),\n",
    "            w.Feature_discovered_ports_sliding(ep),\n",
    "            w.Feature_discovered_nodeproperties_sliding(ep),\n",
    "            w.Feature_discovered_notowned_node_count(ep, 3)\n",
    "        ], 5000)  # should not be too small, pick something big to avoid collision\n",
    "\n",
    "actionspace = w.RavelEncoding(ep, [\n",
    "            w.Feature_active_node_properties(ep)])\n",
    "print(statespace)\n",
    "print(actionspace)\n",
    "q_matrix = DefenderQMatrix(statespace, actionspace,gym_env)\n",
    "learner2 = DefenderQLearner(gym_env, defender_agent, ep, q_matrix, epsilon=0.9, gamma=0.015, learning_rate=.07)\n",
    "random.seed(120394016)\n",
    "#%matplotlib inline\n",
    "\n",
    "trained_defender = epsilon_greedy_defender_training(\n",
    "        cyberbattle_gym_env=gym_env,\n",
    "        environment_properties=ep,\n",
    "        learner=learner2,\n",
    "        episode_count=episode_count,  # Set the number of episodes for the training\n",
    "        iteration_count=iteration_count,  # Set the number of iterations per episode\n",
    "        epsilon=epsilon,\n",
    "        epsilon_exponential_decay=epsilon_exponential_decay,\n",
    "        epsilon_minimum=epsilon_minimum,\n",
    "        epsilon_multdecay=epsilon_multdecay,\n",
    "        verbosity=logging.INFO,\n",
    "        render=True,\n",
    "        plot_episodes_length=True,\n",
    "        title=\"Defender Q-Learning Training\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9094c529-c5e5-4a9b-a2f4-c283b888710b",
   "metadata": {
    "id": "fad44241-b29a-4e1d-b292-8471ceda7748",
    "outputId": "882ef6ca-3c98-458a-d4e9-800d724d012d",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "random.seed(120394016)\n",
    "# Simulation Parameters\n",
    "number_of_episodes = 10\n",
    "max_steps_per_episode = 10\n",
    "attacker_epsilon = 0.1\n",
    "defender_epsilon = 0.1\n",
    "\n",
    "# Reward Tracking\n",
    "attacker_rewards_per_episode = []\n",
    "defender_rewards_per_episode = []\n",
    "win_rates = {'attacker': [], 'defender': []}\n",
    "actions_taken_count = {'attacker': [], 'defender': []}\n",
    "exploration_vs_exploitation = {'exploration': [], 'exploitation': []}\n",
    "episode_lengths = []\n",
    "state_visitation_frequencies = {}\n",
    "reward_variability = []\n",
    "defender_successful_actions = []\n",
    "defender_failed_actions = []\n",
    "defender_actions_taken = []\n",
    "tracker = SimulationTracker() #instantiate simulation tracker\n",
    "wrapped_env = wrap_environment(gym_environment)\n",
    "win_counts = {'attacker': 0, 'defender': 0}\n",
    "#main simulation loop\n",
    "random.seed(120394016)\n",
    "for episode in range(number_of_episodes):\n",
    "    observation = wrapped_env.reset()\n",
    "    attacker_total_reward = 0\n",
    "    defender_total_reward = 0\n",
    "    steps_this_episode=0\n",
    "    episode_rewards = []  #track rewards for this episode\n",
    "    for step in range(max_steps_per_episode):\n",
    "        steps_this_episode +=1\n",
    "        #attacker's Turn\n",
    "        exploration = np.random.random() < attacker_epsilon\n",
    "        if exploration:\n",
    "            _, attacker_action, _ = attacker_learner.explore(wrapped_env)\n",
    "        else:\n",
    "            _, attacker_action, _ = attacker_learner.exploit(wrapped_env, observation)\n",
    "        #print(attacker_action)\n",
    "        if attacker_action is not None:\n",
    "            observation, attacker_reward, done, _ = gym_environment.step(attacker_action)\n",
    "            attacker_total_reward += attacker_reward\n",
    "            print(_)\n",
    "            actions_taken_count['attacker'].append(attacker_action)  # Track action\n",
    "            episode_rewards.append(attacker_reward)  # Track reward variability\n",
    "            tracker.log_action('attacker', attacker_action, exploration)\n",
    "            #actions_taken_count['attacker'][attacker_action] = actions_taken_count['attacker'].get(attacker_action, 0) + 1\n",
    "\n",
    "\n",
    "        # Defender's Turn\n",
    "        exploration = np.random.random() < defender_epsilon\n",
    "        if exploration:\n",
    "            _, defender_action, _ = defender_learner.explore(wrapped_env)\n",
    "        else:\n",
    "            _, defender_action, _ = defender_learner.exploit(wrapped_env, observation)\n",
    "\n",
    "        if defender_action is not None:\n",
    "            observation, defender_reward, done, _ = gym_environment.step(defender_action)\n",
    "            #action= _['defender_info']['action']\n",
    "            defender_total_reward += defender_reward\n",
    "            actions_taken_count['defender'].append(defender_action)  # Track action\n",
    "            episode_rewards.append(defender_reward)  # Track reward variability\n",
    "            tracker.log_action('defender', defender_action, exploration)\n",
    "            #actions_taken_count['defender'][defender_action] = actions_taken_count['defender'].get(defender_action, 0) + 1\n",
    "        #logging\n",
    "        # After each episode\n",
    "        tracker.log_episode(attacker_total_reward, defender_total_reward, steps_this_episode, episode_rewards)\n",
    "        tracker.print_summary()\n",
    "        tracker._plot_rewards()\n",
    "\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "    defender_info = _['defender_info']\n",
    "    if defender_info:\n",
    "        defender_actions_taken.append(defender_info['performance_metrics']['actions_taken'])\n",
    "        defender_successful_actions.append(defender_info['performance_metrics']['successful_actions'])\n",
    "        defender_failed_actions.append(defender_info['performance_metrics']['failed_actions'])\n",
    "\n",
    "    attacker_rewards_per_episode.append(attacker_total_reward)\n",
    "    defender_rewards_per_episode.append(defender_total_reward)\n",
    "    episode_lengths.append(steps_this_episode)\n",
    "    reward_variability.append(np.var(episode_rewards))\n",
    "    print(f\"Episode {episode + 1}: Attacker Reward = {attacker_total_reward}, Defender Reward = {defender_total_reward}\")\n",
    "\n",
    "wrapped_env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eea20f7f-738a-4342-967d-5c088ee028e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "attacker10_rewards_per_episode = attacker_rewards_per_episode\n",
    "defender10_rewards_per_episode = defender_rewards_per_episode\n",
    "win_rates10=win_rates\n",
    "defender_successful_actions10 = defender_successful_actions\n",
    "defender_failed_actions10 = defender_failed_actions\n",
    "defender_actions_taken10 = defender_actions_taken"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b176305c-cea2-44bc-9520-71a64ef8ce02",
   "metadata": {},
   "source": [
    "epsiode 50x200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30d68310-c180-4954-8bdd-7e938f7c6ac5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#this was made due to issues where the attacker q learner needed to use a wrapper environment\n",
    "from cyberbattle.agents.baseline.agent_wrapper import AgentWrapper, StateAugmentation\n",
    "def wrap_environment(env):\n",
    "    initial_observation = env.reset()\n",
    "    state = StateAugmentation(initial_observation)\n",
    "    return AgentWrapper(env, state)\n",
    "\n",
    "import gym\n",
    "\n",
    "gym_env = gym.make('CyberBattleChain-v0',\n",
    "    size=12,\n",
    "    attacker_goal=AttackerGoal(\n",
    "        own_atleast=0,\n",
    "        own_atleast_percent=1.0\n",
    "    ),\n",
    "    defender_constraint=DefenderConstraint(\n",
    "        maintain_sla=0.80\n",
    "    )\n",
    ")\n",
    "\n",
    "defender_name = \"DefenderName\"\n",
    "defender_agent = DefenderWrapper(gym_env,defender_name)\n",
    "\n",
    "gym_environment = gym.make('CyberBattleChain-v0',\n",
    "    size=12,\n",
    "    attacker_goal=AttackerGoal(\n",
    "        own_atleast=0,\n",
    "        own_atleast_percent=1.0\n",
    "    ),\n",
    "    defender_constraint=DefenderConstraint(\n",
    "        maintain_sla=0.80\n",
    "    ),\n",
    "    defender_agent=defender_agent\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a6dddf5-1c2b-4728-8bec-c035597c16e3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#parameter ranges\n",
    "epsilon = 0.9629629629629622\n",
    "#gamma=np.linspace(0.05,0.09,10)\n",
    "gamma= 0.05888888888888889\n",
    "#learning_rate=np.linspace(0.3,.35,10)\n",
    "learning_rate=0.3222222222222222\n",
    "#epsilon_decay=np.linspace(2333.33333,5000,20)\n",
    "epsilon_exponential_decay=5000\n",
    "#epsilon_minimum=np.linspace(0.01,.07777777,10)\n",
    "epsilon_minimum=0.0513409942068965\n",
    "episode_count=50\n",
    "iteration_count=200\n",
    "#epsilon_multdecay=np.linspace(0.3,0.875,20)\n",
    "epsilon_multdecay=0.6631578947368421\n",
    "\n",
    "trained_attacker = learner.epsilon_greedy_search(\n",
    "        gym_env,\n",
    "        ep,\n",
    "        a.QTabularLearner(ep, gamma=0.05888888888888889, learning_rate=0.3222222222222222, exploit_percentile=100),\n",
    "        episode_count=episode_count,\n",
    "        iteration_count=iteration_count,\n",
    "        epsilon=epsilon,\n",
    "        render=True,\n",
    "        epsilon_multdecay=epsilon_multdecay,  # 0.999,\n",
    "        epsilon_exponential_decay=epsilon_exponential_decay,\n",
    "        epsilon_minimum=epsilon_minimum,\n",
    "        verbosity=Verbosity.Quiet,\n",
    "        title=\"Q-learning\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0537f7fc-28ea-4037-9c13-565b2df7b119",
   "metadata": {
    "id": "193f8fd9-d6bf-43db-a177-181526286cd5",
    "outputId": "fd9e9bf7-b7d6-45a1-a1eb-3c7b4f121071"
   },
   "outputs": [],
   "source": [
    "import cyberbattle.agents.baseline.agent_tabularqlearning as a\n",
    "import cyberbattle.agents.baseline.learner as learner\n",
    "from cyberbattle.agents.baseline.agent_wrapper import Verbosity\n",
    "epsilon = 0.66666666666666\n",
    "gamma= 0.06925\n",
    "learning_rate= 0.34444444444444444\n",
    "epsilon_exponential_decay=3198.830407105263\n",
    "epsilon_minimum=0.05708811937931034\n",
    "episode_count=50\n",
    "iteration_count=200\n",
    "epsilon_multdecay=0.7777777777777778\n",
    "statespace = w.HashEncoding(ep, [\n",
    "            # Feature_discovered_node_count(),\n",
    "            # Feature_discovered_credential_count(),\n",
    "            w.Feature_discovered_ports_sliding(ep),\n",
    "            w.Feature_discovered_nodeproperties_sliding(ep),\n",
    "            w.Feature_discovered_notowned_node_count(ep, 3)\n",
    "        ], 5000)  # should not be too small, pick something big to avoid collision\n",
    "\n",
    "actionspace = w.RavelEncoding(ep, [\n",
    "            w.Feature_active_node_properties(ep)])\n",
    "print(statespace)\n",
    "print(actionspace)\n",
    "q_matrix = DefenderQMatrix(statespace, actionspace,gym_env)\n",
    "learner2 = DefenderQLearner(gym_env, defender_agent, ep, q_matrix, epsilon=0.9, gamma=0.015, learning_rate=.07)\n",
    "random.seed(120394016)\n",
    "#%matplotlib inline\n",
    "\n",
    "trained_defender = epsilon_greedy_defender_training(\n",
    "        cyberbattle_gym_env=gym_env,\n",
    "        environment_properties=ep,\n",
    "        learner=learner2,\n",
    "        episode_count=episode_count, \n",
    "        iteration_count=iteration_count, \n",
    "        epsilon=epsilon,\n",
    "        epsilon_exponential_decay=epsilon_exponential_decay,\n",
    "        epsilon_minimum=epsilon_minimum,\n",
    "        epsilon_multdecay=epsilon_multdecay,\n",
    "        verbosity=logging.INFO,\n",
    "        render=True,\n",
    "        plot_episodes_length=True,\n",
    "        title=\"Defender Q-Learning Training\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c72e7df-8ad8-4e3b-af72-63a4dc6a8c19",
   "metadata": {},
   "outputs": [],
   "source": [
    "#this was made due to issues where the attacker q learner needed to use a wrapper environment\n",
    "from cyberbattle.agents.baseline.agent_wrapper import AgentWrapper, StateAugmentation\n",
    "def wrap_environment(env):\n",
    "    initial_observation = env.reset()\n",
    "    state = StateAugmentation(initial_observation)\n",
    "    return AgentWrapper(env, state)\n",
    "\n",
    "import gym\n",
    "\n",
    "gym_env = gym.make('CyberBattleChain-v0',\n",
    "    size=12,\n",
    "    attacker_goal=AttackerGoal(\n",
    "        own_atleast=0,\n",
    "        own_atleast_percent=1.0\n",
    "    ),\n",
    "    defender_constraint=DefenderConstraint(\n",
    "        maintain_sla=0.80\n",
    "    )\n",
    ")\n",
    "\n",
    "defender_name = \"DefenderName\"\n",
    "defender_agent = DefenderWrapper(gym_env,defender_name)\n",
    "\n",
    "gym_environment = gym.make('CyberBattleChain-v0',\n",
    "    size=12,\n",
    "    attacker_goal=AttackerGoal(\n",
    "        own_atleast=0,\n",
    "        own_atleast_percent=1.0\n",
    "    ),\n",
    "    defender_constraint=DefenderConstraint(\n",
    "        maintain_sla=0.80\n",
    "    ),\n",
    "    defender_agent=defender_agent\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db7f094d-d3e7-4f28-b592-327b7c14a9c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "attacker_learner = trained_attacker['learner']\n",
    "defender_learner = trained_defender['learner']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42505350-78d6-4555-860f-8b7a42d63b7a",
   "metadata": {
    "id": "42505350-78d6-4555-860f-8b7a42d63b7a"
   },
   "source": [
    "10x10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "851e6ea9-dcab-4668-94b3-703fe8369150",
   "metadata": {
    "id": "19ee6d89-ff9e-49ba-98b7-2ee5dbe58991",
    "outputId": "117dad20-a2f3-4ef6-d0a4-a21e13b1ccee",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#simulation parameters\n",
    "number_of_episodes = 50\n",
    "max_steps_per_episode = 200\n",
    "attacker_epsilon = 0.1\n",
    "defender_epsilon = 0.1\n",
    "\n",
    "#reward tracking\n",
    "attacker_rewards_per_episode = []\n",
    "defender_rewards_per_episode = []\n",
    "win_rates = {'attacker': [], 'defender': []}\n",
    "actions_taken_count = {'attacker': [], 'defender': []}\n",
    "exploration_vs_exploitation = {'exploration': [], 'exploitation': []}\n",
    "episode_lengths = []\n",
    "state_visitation_frequencies = {}\n",
    "reward_variability = []\n",
    "defender_successful_actions = []\n",
    "defender_failed_actions = []\n",
    "defender_actions_taken = []\n",
    "tracker = SimulationTracker() #instantiate simulation tracker\n",
    "wrapped_env = wrap_environment(gym_environment)\n",
    "win_counts = {'attacker': 0, 'defender': 0}\n",
    "# Main Simulation Loop\n",
    "random.seed(120394016)\n",
    "for episode in range(number_of_episodes):\n",
    "    observation = wrapped_env.reset()\n",
    "    attacker_total_reward = 0\n",
    "    defender_total_reward = 0\n",
    "    steps_this_episode=0\n",
    "    episode_rewards = []  # Track rewards for this episode\n",
    "    for step in range(max_steps_per_episode):\n",
    "        steps_this_episode +=1\n",
    "        # Attacker's Turn\n",
    "        exploration = np.random.random() < attacker_epsilon\n",
    "        if exploration:\n",
    "            _, attacker_action, _ = attacker_learner.explore(wrapped_env)\n",
    "        else:\n",
    "            _, attacker_action, _ = attacker_learner.exploit(wrapped_env, observation)\n",
    "        #print(attacker_action)\n",
    "        if attacker_action is not None:\n",
    "            observation, attacker_reward, done, _ = gym_environment.step(attacker_action)\n",
    "            attacker_total_reward += attacker_reward\n",
    "            print(_)\n",
    "            actions_taken_count['attacker'].append(attacker_action)  # Track action\n",
    "            episode_rewards.append(attacker_reward)  # Track reward variability\n",
    "            tracker.log_action('attacker', attacker_action, exploration)\n",
    "            #actions_taken_count['attacker'][attacker_action] = actions_taken_count['attacker'].get(attacker_action, 0) + 1\n",
    "\n",
    "\n",
    "        # Defender's Turn\n",
    "        exploration = np.random.random() < defender_epsilon\n",
    "        if exploration:\n",
    "            _, defender_action, _ = defender_learner.explore(wrapped_env)\n",
    "        else:\n",
    "            _, defender_action, _ = defender_learner.exploit(wrapped_env, observation)\n",
    "\n",
    "        if defender_action is not None:\n",
    "            observation, defender_reward, done, _ = gym_environment.step(defender_action)\n",
    "            #action= _['defender_info']['action']\n",
    "            defender_total_reward += defender_reward\n",
    "            actions_taken_count['defender'].append(defender_action)  # Track action\n",
    "            episode_rewards.append(defender_reward)  # Track reward variability\n",
    "            tracker.log_action('defender', defender_action, exploration)\n",
    "            #actions_taken_count['defender'][defender_action] = actions_taken_count['defender'].get(defender_action, 0) + 1\n",
    "        #logging\n",
    "        # After each episode\n",
    "        tracker.log_episode(attacker_total_reward, defender_total_reward, steps_this_episode, episode_rewards)\n",
    "        tracker.print_summary()\n",
    "        tracker._plot_rewards()\n",
    "\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "    defender_info = _['defender_info']\n",
    "    if defender_info:\n",
    "        defender_actions_taken.append(defender_info['performance_metrics']['actions_taken'])\n",
    "        defender_successful_actions.append(defender_info['performance_metrics']['successful_actions'])\n",
    "        defender_failed_actions.append(defender_info['performance_metrics']['failed_actions'])\n",
    "\n",
    "    attacker_rewards_per_episode.append(attacker_total_reward)\n",
    "    defender_rewards_per_episode.append(defender_total_reward)\n",
    "    episode_lengths.append(steps_this_episode)\n",
    "    reward_variability.append(np.var(episode_rewards))\n",
    "    print(f\"Episode {episode + 1}: Attacker Reward = {attacker_total_reward}, Defender Reward = {defender_total_reward}\")\n",
    "\n",
    "wrapped_env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4674bd9b-c6d0-48b4-9f5b-21949dd8c069",
   "metadata": {},
   "outputs": [],
   "source": [
    "attacker50_rewards_per_episode = attacker_rewards_per_episode\n",
    "defender50_rewards_per_episode = defender_rewards_per_episode\n",
    "win_rates50=win_rates\n",
    "defender_successful_actions50 = defender_successful_actions\n",
    "defender_failed_actions50 = defender_failed_actions\n",
    "defender_actions_taken50 = defender_actions_taken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fe3638c-3e49-4ddc-a733-036077fbaa59",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.hist(attacker10_rewards_per_episode, alpha=0.5, label='10 Episodes')\n",
    "plt.hist(attacker50_rewards_per_episode, alpha=0.5, label='50 Episodes')\n",
    "plt.legend()\n",
    "plt.title('Attacker Rewards Distribution')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.hist(defender10_rewards_per_episode, alpha=0.5, label='10 Episodes')\n",
    "plt.hist(defender50_rewards_per_episode, alpha=0.5, label='50 Episodes')\n",
    "plt.legend()\n",
    "plt.title('Defender Rewards Distribution')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62cca571-b52c-4941-8060-2b1755d4aaa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#attacker rewards comparison\n",
    "t_stat, p_value = stats.ttest_ind(attacker10_rewards_per_episode, attacker50_rewards_per_episode, equal_var=False)\n",
    "print(f'Attacker Rewards: t-statistic={t_stat:.2f}, p-value={p_value:.4f}')\n",
    "\n",
    "#defender rewards comparison\n",
    "t_stat, p_value = stats.ttest_ind(defender10_rewards_per_episode, defender50_rewards_per_episode, equal_var=False)\n",
    "print(f'Defender Rewards: t-statistic={t_stat:.2f}, p-value={p_value:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9831074-db59-413b-bdcd-85bb75a0ccc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculate percentage of successful actions for each simulation set\n",
    "def percentage_successful(actions_taken, successful_actions):\n",
    "    return [successful / total if total != 0 else 0 for successful, total in zip(successful_actions, actions_taken)]\n",
    "\n",
    "percentage_successful_10 = percentage_successful(defender_actions_taken10, defender_successful_actions10)\n",
    "percentage_successful_50 = percentage_successful(defender_actions_taken50, defender_successful_actions50)\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(12, 6))\n",
    "episodes_10 = np.arange(1, len(percentage_successful_10) + 1)\n",
    "episodes_50 = np.arange(1, len(percentage_successful_50) + 1)\n",
    "plt.plot(episodes_10, percentage_successful_10, label='10 Episodes @ 10 Iterations', marker='o')\n",
    "plt.plot(episodes_50, percentage_successful_50, label='50 Episodes @ 200 Iterations', marker='x')\n",
    "\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Percentage of Successful Actions')\n",
    "plt.title('Percentage of Successful Defender Actions')\n",
    "plt.legend(frameon=True,facecolor='white')\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf5eeb3e-0646-4be7-9d2a-e58ea99d26b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#mann-whitney U Test\n",
    "u_stat, p_value = stats.mannwhitneyu(percentage_successful_10, percentage_successful_50, alternative='less')\n",
    "\n",
    "print(f'Mann-Whitney U Test: U-Statistic={u_stat:.2f}, p-value={p_value:.4f}')\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python [conda env:cybersim] *",
   "language": "python",
   "name": "conda-env-cybersim-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
