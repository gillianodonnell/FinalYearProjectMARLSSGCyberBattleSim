{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c225d693-7e76-4010-a219-b92ec8b474f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import copy\n",
    "import logging\n",
    "import networkx\n",
    "from networkx import convert_matrix\n",
    "from typing import NamedTuple, Optional, Tuple, List, Dict, TypeVar, TypedDict, cast\n",
    "\n",
    "import numpy\n",
    "import gym\n",
    "from gym import spaces\n",
    "from gym.utils import seeding\n",
    "\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "from cyberbattle._env.shared_cyberbattle_env import EnvironmentBounds, AttackerGoal, DefenderGoal, DefenderConstraint\n",
    "from cyberbattle._env.defender import DefenderAgent,ScanAndReimageCompromisedMachines\n",
    "from cyberbattle.simulation.model import PortName, PrivilegeLevel\n",
    "from cyberbattle.simulation import commandcontrol, model, actions\n",
    "from cyberbattle._env.discriminatedunion import DiscriminatedUnion\n",
    "from cyberbattle.agents.baseline import agent_wrapper as w\n",
    "import cyberbattle.agents.baseline.learner as learner\n",
    "from cyberbattle.agents.baseline.learner import Learner\n",
    "import cyberbattle.agents.baseline.agent_tabularqlearning as a\n",
    "import cyberbattle.agents.baseline.agent_ddql as ddqla\n",
    "import cyberbattle.agents.baseline.agent_dql as dqla\n",
    "from cyberbattle.defender_agents.trainedDefender import *\n",
    "import random\n",
    "random.seed(120394016)\n",
    "%matplotlib inline\n",
    "import cyberbattle.agents.baseline.agent_tabularqlearning as a\n",
    "import cyberbattle.agents.baseline.learner as learner\n",
    "from cyberbattle.agents.baseline.agent_wrapper import Verbosity\n",
    "#ql\n",
    "iteration_count = 10\n",
    "training_episode_count = 10\n",
    "eval_episode_count = 20\n",
    "gamma_sweep = [\n",
    "    0.015,  # about right\n",
    "]\n",
    "import gym\n",
    "\n",
    "gym_env = gym.make('CyberBattleChain-v0',\n",
    "    size=12,\n",
    "    attacker_goal=AttackerGoal(\n",
    "        own_atleast=0,\n",
    "        own_atleast_percent=1.0\n",
    "    ),\n",
    "    defender_constraint=DefenderConstraint(\n",
    "        maintain_sla=0.80\n",
    "    )\n",
    ")\n",
    "\n",
    "defender_name = \"DefenderName\"\n",
    "defender_agent = DefenderWrapper(gym_env,defender_name)\n",
    "from cyberbattle.agents.baseline.agent_wrapper import ActionTrackingStateAugmentation, StateAugmentation\n",
    "from cyberbattle.agents.baseline.plotting import PlotTraining, plot_averaged_cummulative_rewards\n",
    "import progressbar\n",
    "import math\n",
    "from cyberbattle.agents.baseline import learner\n",
    "import sys\n",
    "from cyberbattle.simulation.model import FirewallRule\n",
    "ep = w.EnvironmentBounds.of_identifiers(\n",
    "    maximum_total_credentials=22,\n",
    "    maximum_node_count=22,\n",
    "    identifiers=gym_env.identifiers\n",
    ")\n",
    "#cyberbattlechain_defender.render_as_fig()\n",
    "import cyberbattle.agents.baseline.agent_tabularqlearning as a\n",
    "import cyberbattle.agents.baseline.learner as learner\n",
    "from cyberbattle.agents.baseline.agent_wrapper import Verbosity\n",
    "epsilon = 0.66666666666666\n",
    "gamma= 0.06925\n",
    "learning_rate= 0.34444444444444444\n",
    "epsilon_exponential_decay=3198.830407105263\n",
    "epsilon_minimum=0.05708811937931034\n",
    "episode_count=10\n",
    "iteration_count=10\n",
    "epsilon_multdecay=0.7777777777777778\n",
    "statespace = w.HashEncoding(ep, [\n",
    "            # Feature_discovered_node_count(),\n",
    "            # Feature_discovered_credential_count(),\n",
    "            w.Feature_discovered_ports_sliding(ep),\n",
    "            w.Feature_discovered_nodeproperties_sliding(ep),\n",
    "            w.Feature_discovered_notowned_node_count(ep, 3)\n",
    "        ], 5000)  # should not be too small, pick something big to avoid collision\n",
    "\n",
    "actionspace = w.RavelEncoding(ep, [\n",
    "            w.Feature_active_node_properties(ep)])\n",
    "print(statespace)\n",
    "print(actionspace)\n",
    "q_matrix = DefenderQMatrix(statespace, actionspace,gym_env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d93e0354-ba5d-4413-af0c-ab208f180ce8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_training_with_hyperparameters(gym_env, environment_properties, statespace, actionspace, \n",
    "                                      epsilon, gamma, learning_rate, epsilon_exponential_decay, epsilon_minimum, episode_count, iteration_count, epsilon_multdecay):\n",
    "    q_matrix = DefenderQMatrix(statespace, actionspace, gym_env)\n",
    "    learner = DefenderQLearner(gym_env, defender_agent, environment_properties, q_matrix,\n",
    "                               epsilon=epsilon, gamma=gamma, learning_rate=learning_rate)\n",
    "    \n",
    "    trained_defender = epsilon_greedy_defender_training(\n",
    "        cyberbattle_gym_env=gym_env,\n",
    "        environment_properties=environment_properties,\n",
    "        learner=learner,\n",
    "        episode_count=episode_count,\n",
    "        iteration_count=iteration_count,  \n",
    "        epsilon=epsilon,\n",
    "        epsilon_exponential_decay=epsilon_exponential_decay,\n",
    "        epsilon_minimum=epsilon_minimum,\n",
    "        epsilon_multdecay=epsilon_multdecay,\n",
    "        verbosity=logging.INFO,\n",
    "        render=True,\n",
    "        plot_episodes_length=True,\n",
    "        title=\"Defender Q-Learning Training\"\n",
    "    )\n",
    "    total_reward = sum([sum(rewards) for rewards in trained_defender['all_episodes_rewards']])\n",
    "    return total_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "467ff8a8-7823-45ad-b38a-2f6d6e7e8dc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "plt.style.use('seaborn-darkgrid')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9813eaaa-74e4-4fcc-b96e-a518fe84f449",
   "metadata": {},
   "source": [
    "# Baseline"
   ]
  },
  {
   "cell_type": "raw",
   "id": "3c1819ec-aec9-4ed7-9fe4-e64a6fe59545",
   "metadata": {},
   "source": [
    "import cyberbattle.agents.baseline.agent_tabularqlearning as a\n",
    "import cyberbattle.agents.baseline.learner as learner\n",
    "from cyberbattle.agents.baseline.agent_wrapper import Verbosity\n",
    "epsilon = 0.66666666666666\n",
    "gamma= 0.06925\n",
    "learning_rate= 0.34444444444444444\n",
    "epsilon_exponential_decay=3198.830407105263\n",
    "epsilon_minimum=0.05708811937931034\n",
    "episode_count=50\n",
    "iteration_count=200\n",
    "epsilon_multdecay=0.7777777777777778\n",
    "statespace = w.HashEncoding(ep, [\n",
    "            # Feature_discovered_node_count(),\n",
    "            # Feature_discovered_credential_count(),\n",
    "            w.Feature_discovered_ports_sliding(ep),\n",
    "            w.Feature_discovered_nodeproperties_sliding(ep),\n",
    "            w.Feature_discovered_notowned_node_count(ep, 3)\n",
    "        ], 5000)  # should not be too small, pick something big to avoid collision\n",
    "\n",
    "actionspace = w.RavelEncoding(ep, [\n",
    "            w.Feature_active_node_properties(ep)])\n",
    "print(statespace)\n",
    "print(actionspace)\n",
    "q_matrix = DefenderQMatrix(statespace, actionspace,gym_env)\n",
    "learner2 = DefenderQLearner(gym_env, defender_agent, ep, q_matrix, epsilon=0.9, gamma=0.015, learning_rate=.07)\n",
    "random.seed(120394016)\n",
    "#%matplotlib inline\n",
    "\n",
    "trained_defender = epsilon_greedy_defender_training(\n",
    "        cyberbattle_gym_env=gym_env,\n",
    "        environment_properties=ep,\n",
    "        learner=learner2,\n",
    "        episode_count=episode_count,  # Set the number of episodes for the training\n",
    "        iteration_count=iteration_count,  # Set the number of iterations per episode\n",
    "        epsilon=epsilon,\n",
    "        epsilon_exponential_decay=epsilon_exponential_decay,\n",
    "        epsilon_minimum=epsilon_minimum,\n",
    "        epsilon_multdecay=epsilon_multdecay,\n",
    "        verbosity=logging.INFO,\n",
    "        render=True,\n",
    "        plot_episodes_length=True,\n",
    "        title=\"Defender Q-Learning Training\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e060342a-60c4-4ccd-ba1e-2e3badde4027",
   "metadata": {},
   "source": [
    "# Increased"
   ]
  },
  {
   "cell_type": "raw",
   "id": "87aa23a8-ab53-48ac-af3d-5083cc3394c1",
   "metadata": {},
   "source": [
    "def calculate_reward(self, action_result):\n",
    "      if action_result['status'] == 'success':\n",
    "          if action_result['action'] == 'reimage_node':\n",
    "              return 50  \n",
    "          else:\n",
    "              return 25 \n",
    "      elif action_result['status'] == 'failed':\n",
    "          #print('failed')\n",
    "          #print(action_result)\n",
    "          if action_result['action'] == 'reimage_node':\n",
    "              #print('reimaged')\n",
    "              return 0\n",
    "          else:\n",
    "              #print('not reimaged')\n",
    "              return -15  #Penalty for failed actions\n",
    "      elif action_result['status'] == 'unnecessary':\n",
    "          return -5  #Smaller penalty for unnecessary actions\n",
    "      return 0  #Default reward for other cases"
   ]
  },
  {
   "cell_type": "raw",
   "id": "bc9ae0fb-f558-4713-a1cb-112c3e7cf0f9",
   "metadata": {},
   "source": [
    "new:     def calculate_reward(self, action_result):\n",
    "      if action_result['status'] == 'success':\n",
    "          if action_result['action'] == 'reimage_node':\n",
    "              return 5000  #Higher reward for re-imaging a compromised node\n",
    "          else:\n",
    "              return 4000  #Standard reward for other successful actions\n",
    "      elif action_result['status'] == 'failed':\n",
    "          #print('failed')\n",
    "          #print(action_result)\n",
    "          if action_result['action'] == 'reimage_node':\n",
    "              #print('reimaged')\n",
    "              return -4000\n",
    "          else:\n",
    "              #print('not reimaged')\n",
    "              return -5000  #Penalty for failed actions\n",
    "      elif action_result['status'] == 'unnecessary':\n",
    "          return -1000  #Smaller penalty for unnecessary actions\n",
    "      return 0  #Default reward for other cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52a7d5a0-19f9-450e-920f-8f5f846a5b27",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cyberbattle.agents.baseline.agent_tabularqlearning as a\n",
    "import cyberbattle.agents.baseline.learner as learner\n",
    "from cyberbattle.agents.baseline.agent_wrapper import Verbosity\n",
    "epsilon = 0.66666666666666\n",
    "gamma= 0.06925\n",
    "learning_rate= 0.34444444444444444\n",
    "epsilon_exponential_decay=3198.830407105263\n",
    "epsilon_minimum=0.05708811937931034\n",
    "episode_count=50\n",
    "iteration_count=200\n",
    "epsilon_multdecay=0.7777777777777778\n",
    "statespace = w.HashEncoding(ep, [\n",
    "            # Feature_discovered_node_count(),\n",
    "            # Feature_discovered_credential_count(),\n",
    "            w.Feature_discovered_ports_sliding(ep),\n",
    "            w.Feature_discovered_nodeproperties_sliding(ep),\n",
    "            w.Feature_discovered_notowned_node_count(ep, 3)\n",
    "        ], 5000) \n",
    "\n",
    "actionspace = w.RavelEncoding(ep, [\n",
    "            w.Feature_active_node_properties(ep)])\n",
    "print(statespace)\n",
    "print(actionspace)\n",
    "q_matrix = DefenderQMatrix(statespace, actionspace,gym_env)\n",
    "learner2 = DefenderQLearner(gym_env, defender_agent, ep, q_matrix, epsilon=0.9, gamma=0.015, learning_rate=.07)\n",
    "random.seed(120394016)\n",
    "#%matplotlib inline\n",
    "\n",
    "trained_defender = epsilon_greedy_defender_training(\n",
    "        cyberbattle_gym_env=gym_env,\n",
    "        environment_properties=ep,\n",
    "        learner=learner2,\n",
    "        episode_count=episode_count, \n",
    "        iteration_count=iteration_count,\n",
    "        epsilon=epsilon,\n",
    "        epsilon_exponential_decay=epsilon_exponential_decay,\n",
    "        epsilon_minimum=epsilon_minimum,\n",
    "        epsilon_multdecay=epsilon_multdecay,\n",
    "        verbosity=logging.INFO,\n",
    "        render=True,\n",
    "        plot_episodes_length=True,\n",
    "        title=\"Defender Q-Learning Training\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab027f95-f81f-452b-a39e-338f0eda10e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class SimulationTracker:\n",
    "    def __init__(self):\n",
    "        self.attacker_rewards = []\n",
    "        self.defender_rewards = []\n",
    "        self.actions_taken = {'attacker': [], 'defender': []}\n",
    "        self.exploration_actions = {'attacker': 0, 'defender': 0}\n",
    "        self.exploitation_actions = {'attacker': 0, 'defender': 0}\n",
    "        self.episode_lengths = []\n",
    "        self.reward_variability = []\n",
    "        self.state_visitation_frequencies = {}\n",
    "        self.temporal_differences = []\n",
    "        self.infected_nodes = []\n",
    "\n",
    "    def log_episode(self, attacker_reward, defender_reward, episode_length, episode_rewards):\n",
    "        self.attacker_rewards.append(attacker_reward)\n",
    "        self.defender_rewards.append(defender_reward)\n",
    "        self.episode_lengths.append(episode_length)\n",
    "        self.reward_variability.append(np.var(episode_rewards))\n",
    "        #self.temporal_differences.append(temporal_difference)\n",
    "\n",
    "    def log_action(self, role, action, exploration):\n",
    "        action_str = str(action)  \n",
    "        self.actions_taken[role].append(action_str)\n",
    "        if exploration:\n",
    "            self.exploration_actions[role] += 1\n",
    "        else:\n",
    "            self.exploitation_actions[role] += 1\n",
    "\n",
    "\n",
    "    def log_state_visitation(self, state):\n",
    "        if state not in self.state_visitation_frequencies:\n",
    "            self.state_visitation_frequencies[state] = 1\n",
    "        else:\n",
    "            self.state_visitation_frequencies[state] += 1\n",
    "\n",
    "    def log_infected_nodes(self, nodes_count):\n",
    "        self.infected_nodes.append(nodes_count)\n",
    "\n",
    "    def print_summary(self):\n",
    "        # Summary statistics\n",
    "        print(\"Simulation Summary:\")\n",
    "        print(f\"Average Attacker Reward: {np.mean(self.attacker_rewards)}\")\n",
    "        print(f\"Average Defender Reward: {np.mean(self.defender_rewards)}\")\n",
    "        print(f\"Average Episode Length: {np.mean(self.episode_lengths)} steps\")\n",
    "        print(f\"Average Reward Variability: {np.mean(self.reward_variability)}\")\n",
    "        print(f\"Average Temporal Difference: {np.mean(self.temporal_differences)}\")\n",
    "        print(f\"Average Infected Nodes per Episode: {np.mean(self.infected_nodes)}\")\n",
    "\n",
    "        #exploration vs. exploitation\n",
    "        print(\"\\nExploration vs. Exploitation:\")\n",
    "        for role in ['attacker', 'defender']:\n",
    "            total_actions = self.exploration_actions[role] + self.exploitation_actions[role]\n",
    "            if total_actions > 0:\n",
    "                print(f\"{role.capitalize()} Exploration: {self.exploration_actions[role] / total_actions:.2f}\")\n",
    "                print(f\"{role.capitalize()} Exploitation: {self.exploitation_actions[role] / total_actions:.2f}\")\n",
    "\n",
    "        #action Distributions\n",
    "        self._print_action_distributions()\n",
    "\n",
    "        self._plot_rewards()\n",
    "\n",
    "    def _print_action_distributions(self):\n",
    "        print(\"\\nAction Distributions:\")\n",
    "        for role, actions in self.actions_taken.items():\n",
    "            # actions are already converted to strings by log_action\n",
    "            unique_actions, counts = np.unique(actions, return_counts=True)\n",
    "            print(f\"{role.capitalize()} Actions:\")\n",
    "            for action, count in zip(unique_actions, counts):\n",
    "                print(f\"Action {action}: {count} times\")\n",
    "\n",
    "\n",
    "    def _plot_rewards(self):\n",
    "        plt.figure(figsize=(10, 5))\n",
    "        plt.plot(self.attacker_rewards, label='Attacker Rewards')\n",
    "        plt.plot(self.defender_rewards, label='Defender Rewards')\n",
    "        plt.xlabel('Episode')\n",
    "        plt.ylabel('Reward')\n",
    "        plt.title('Rewards per Episode for Attacker and Defender')\n",
    "        plt.legend()\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d82afa89-eb81-42fe-8dcf-a973f3742907",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import copy\n",
    "import logging\n",
    "import networkx\n",
    "from networkx import convert_matrix\n",
    "from typing import NamedTuple, Optional, Tuple, List, Dict, TypeVar, TypedDict, cast\n",
    "\n",
    "import numpy\n",
    "import gym\n",
    "from gym import spaces\n",
    "from gym.utils import seeding\n",
    "\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "from cyberbattle._env.shared_cyberbattle_env import EnvironmentBounds, AttackerGoal, DefenderGoal, DefenderConstraint\n",
    "from cyberbattle._env.defender import DefenderAgent,ScanAndReimageCompromisedMachines\n",
    "from cyberbattle.simulation.model import PortName, PrivilegeLevel\n",
    "from cyberbattle.simulation import commandcontrol, model, actions\n",
    "from cyberbattle._env.discriminatedunion import DiscriminatedUnion\n",
    "from cyberbattle.agents.baseline import agent_wrapper as w\n",
    "import cyberbattle.agents.baseline.learner as learner\n",
    "from cyberbattle.agents.baseline.learner import Learner\n",
    "import cyberbattle.agents.baseline.agent_tabularqlearning as a\n",
    "import cyberbattle.agents.baseline.agent_ddql as ddqla\n",
    "import cyberbattle.agents.baseline.agent_dql as dqla\n",
    "from cyberbattle.defender_agents.trainedDefender import *\n",
    "import random\n",
    "random.seed(120394016)\n",
    "%matplotlib inline\n",
    "import cyberbattle.agents.baseline.agent_tabularqlearning as a\n",
    "import cyberbattle.agents.baseline.learner as learner\n",
    "from cyberbattle.agents.baseline.agent_wrapper import Verbosity\n",
    "#ql\n",
    "iteration_count = 10\n",
    "training_episode_count = 10\n",
    "eval_episode_count = 20\n",
    "gamma_sweep = [\n",
    "    0.015,  # about right\n",
    "]\n",
    "import gym\n",
    "\n",
    "gym_env = gym.make('CyberBattleChain-v0',\n",
    "    size=12,\n",
    "    attacker_goal=AttackerGoal(\n",
    "        own_atleast=0,\n",
    "        own_atleast_percent=1.0\n",
    "    ),\n",
    "    defender_constraint=DefenderConstraint(\n",
    "        maintain_sla=0.80\n",
    "    )\n",
    ")\n",
    "\n",
    "defender_name = \"DefenderName\"\n",
    "defender_agent = DefenderWrapper(gym_env,defender_name)\n",
    "from cyberbattle.agents.baseline.agent_wrapper import ActionTrackingStateAugmentation, StateAugmentation\n",
    "from cyberbattle.agents.baseline.plotting import PlotTraining, plot_averaged_cummulative_rewards\n",
    "import progressbar\n",
    "import math\n",
    "from cyberbattle.agents.baseline import learner\n",
    "import sys\n",
    "from cyberbattle.simulation.model import FirewallRule\n",
    "ep = w.EnvironmentBounds.of_identifiers(\n",
    "    maximum_total_credentials=22,\n",
    "    maximum_node_count=22,\n",
    "    identifiers=gym_env.identifiers\n",
    ")\n",
    "#cyberbattlechain_defender.render_as_fig()\n",
    "import cyberbattle.agents.baseline.agent_tabularqlearning as a\n",
    "import cyberbattle.agents.baseline.learner as learner\n",
    "from cyberbattle.agents.baseline.agent_wrapper import Verbosity\n",
    "epsilon = 0.66666666666666\n",
    "gamma= 0.06925\n",
    "learning_rate= 0.34444444444444444\n",
    "epsilon_exponential_decay=3198.830407105263\n",
    "epsilon_minimum=0.05708811937931034\n",
    "episode_count=10\n",
    "iteration_count=10\n",
    "epsilon_multdecay=0.7777777777777778\n",
    "statespace = w.HashEncoding(ep, [\n",
    "            # Feature_discovered_node_count(),\n",
    "            # Feature_discovered_credential_count(),\n",
    "            w.Feature_discovered_ports_sliding(ep),\n",
    "            w.Feature_discovered_nodeproperties_sliding(ep),\n",
    "            w.Feature_discovered_notowned_node_count(ep, 3)\n",
    "        ], 5000)  # should not be too small, pick something big to avoid collision\n",
    "\n",
    "actionspace = w.RavelEncoding(ep, [\n",
    "            w.Feature_active_node_properties(ep)])\n",
    "print(statespace)\n",
    "print(actionspace)\n",
    "q_matrix = DefenderQMatrix(statespace, actionspace,gym_env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17cb5acb-61cc-4956-8499-9b8672b2e3d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(120394016)\n",
    "# Parameter ranges\n",
    "epsilon = 0.9629629629629622\n",
    "#gamma=np.linspace(0.05,0.09,10)\n",
    "gamma= 0.05888888888888889\n",
    "#learning_rate=np.linspace(0.3,.35,10)\n",
    "learning_rate=0.3222222222222222\n",
    "#epsilon_decay=np.linspace(2333.33333,5000,20)\n",
    "epsilon_exponential_decay=5000\n",
    "#epsilon_minimum=np.linspace(0.01,.07777777,10)\n",
    "epsilon_minimum=0.0513409942068965\n",
    "episode_count=50\n",
    "iteration_count=200\n",
    "#epsilon_multdecay=np.linspace(0.3,0.875,20)\n",
    "epsilon_multdecay=0.6631578947368421\n",
    "\n",
    "trained_attacker = learner.epsilon_greedy_search(\n",
    "        gym_env,\n",
    "        ep,\n",
    "        a.QTabularLearner(ep, gamma=0.05888888888888889, learning_rate=0.3222222222222222, exploit_percentile=100),\n",
    "        episode_count=episode_count,\n",
    "        iteration_count=iteration_count,\n",
    "        epsilon=epsilon,\n",
    "        render=True,\n",
    "        epsilon_multdecay=epsilon_multdecay,  # 0.999,\n",
    "        epsilon_exponential_decay=epsilon_exponential_decay,\n",
    "        epsilon_minimum=epsilon_minimum,\n",
    "        verbosity=Verbosity.Quiet,\n",
    "        title=\"Q-learning\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62a4f526-c02c-450e-83f8-25e5d7800a09",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "\n",
    "gym_env = gym.make('CyberBattleChain-v0',\n",
    "    size=12,\n",
    "    attacker_goal=AttackerGoal(\n",
    "        own_atleast=0,\n",
    "        own_atleast_percent=1.0\n",
    "    ),\n",
    "    defender_constraint=DefenderConstraint(\n",
    "        maintain_sla=0.80\n",
    "    )\n",
    ")\n",
    "\n",
    "defender_name = \"DefenderName\" \n",
    "defender_agent = DefenderWrapper(gym_env,defender_name)\n",
    "\n",
    "gym_environment = gym.make('CyberBattleChain-v0',\n",
    "    size=12,\n",
    "    attacker_goal=AttackerGoal(\n",
    "        own_atleast=0,\n",
    "        own_atleast_percent=1.0\n",
    "    ),\n",
    "    defender_constraint=DefenderConstraint(\n",
    "        maintain_sla=0.80\n",
    "    ),\n",
    "    defender_agent=defender_agent\n",
    ")\n",
    "\n",
    "#this was made due to issues where the attacker q learner needed to use a wrapper environment\n",
    "from cyberbattle.agents.baseline.agent_wrapper import AgentWrapper, StateAugmentation\n",
    "def wrap_environment(env):\n",
    "    initial_observation = env.reset()  \n",
    "    state = StateAugmentation(initial_observation)  \n",
    "    return AgentWrapper(env, state)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82e0ba71-5c7e-40e2-b775-f17a42873786",
   "metadata": {},
   "outputs": [],
   "source": [
    "attacker_learner = trained_attacker['learner']\n",
    "defender_learner = trained_defender['learner']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aad7da8d-49e2-4a93-8626-432abdbbb547",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "number_of_episodes = 50\n",
    "max_steps_per_episode = 200\n",
    "attacker_epsilon = 0.1\n",
    "defender_epsilon = 0.1\n",
    "\n",
    "attacker_rewards_per_episode = []\n",
    "defender_rewards_per_episode = []\n",
    "win_rates = {'attacker': [], 'defender': []}\n",
    "actions_taken_count = {'attacker': [], 'defender': []}\n",
    "exploration_vs_exploitation = {'exploration': [], 'exploitation': []}\n",
    "episode_lengths = []\n",
    "state_visitation_frequencies = {}  \n",
    "reward_variability = []\n",
    "defender_successful_actions = []\n",
    "defender_failed_actions = []\n",
    "defender_actions_taken = []\n",
    "tracker = SimulationTracker() #instantiate simulation tracker\n",
    "wrapped_env = wrap_environment(gym_environment)\n",
    "win_counts = {'attacker': 0, 'defender': 0}\n",
    "\n",
    "random.seed(120394016)\n",
    "for episode in range(number_of_episodes):\n",
    "    observation = wrapped_env.reset()\n",
    "    attacker_total_reward = 0\n",
    "    defender_total_reward = 0\n",
    "    steps_this_episode=0\n",
    "    episode_rewards = [] \n",
    "    for step in range(max_steps_per_episode):\n",
    "        steps_this_episode +=1\n",
    "        # Attacker's Turn\n",
    "        exploration = np.random.random() < attacker_epsilon\n",
    "        if exploration:\n",
    "            _, attacker_action, _ = attacker_learner.explore(wrapped_env)\n",
    "        else:\n",
    "            _, attacker_action, _ = attacker_learner.exploit(wrapped_env, observation)\n",
    "        #print(attacker_action)\n",
    "        if attacker_action is not None:\n",
    "            observation, attacker_reward, done, _ = gym_environment.step(attacker_action)\n",
    "            attacker_total_reward += attacker_reward\n",
    "            print(_)\n",
    "            actions_taken_count['attacker'].append(attacker_action)\n",
    "            episode_rewards.append(attacker_reward)  \n",
    "            tracker.log_action('attacker', attacker_action, exploration)\n",
    "            #actions_taken_count['attacker'][attacker_action] = actions_taken_count['attacker'].get(attacker_action, 0) + 1\n",
    "\n",
    "\n",
    "        # Defender's Turn\n",
    "        exploration = np.random.random() < defender_epsilon\n",
    "        if exploration:\n",
    "            _, defender_action, _ = defender_learner.explore(wrapped_env)\n",
    "        else:\n",
    "            _, defender_action, _ = defender_learner.exploit(wrapped_env, observation)\n",
    "\n",
    "        if defender_action is not None:\n",
    "            observation, defender_reward, done, _ = gym_environment.step(defender_action)\n",
    "            #action= _['defender_info']['action']\n",
    "            defender_total_reward += defender_reward\n",
    "            actions_taken_count['defender'].append(defender_action)  \n",
    "            episode_rewards.append(defender_reward)  \n",
    "            tracker.log_action('defender', defender_action, exploration)\n",
    "            #actions_taken_count['defender'][defender_action] = actions_taken_count['defender'].get(defender_action, 0) + 1\n",
    "        #logging\n",
    "        #after each episode\n",
    "        tracker.log_episode(attacker_total_reward, defender_total_reward, steps_this_episode, episode_rewards)\n",
    "        tracker.print_summary()\n",
    "        tracker._plot_rewards()\n",
    "\n",
    "        if done:\n",
    "            break\n",
    "            \n",
    "    defender_info = _['defender_info']  \n",
    "    if defender_info:  \n",
    "        defender_actions_taken.append(defender_info['performance_metrics']['actions_taken'])\n",
    "        defender_successful_actions.append(defender_info['performance_metrics']['successful_actions'])\n",
    "        defender_failed_actions.append(defender_info['performance_metrics']['failed_actions'])\n",
    "\n",
    "    attacker_rewards_per_episode.append(attacker_total_reward)\n",
    "    defender_rewards_per_episode.append(defender_total_reward)\n",
    "    episode_lengths.append(steps_this_episode)\n",
    "    reward_variability.append(np.var(episode_rewards))\n",
    "    print(f\"Episode {episode + 1}: Attacker Reward = {attacker_total_reward}, Defender Reward = {defender_total_reward}\")\n",
    "\n",
    "wrapped_env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fb44228-f573-4231-ba6b-0eb0f5e03302",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "\n",
    "def moving_average(data, window_size):\n",
    "    return np.convolve(data, np.ones(window_size)/window_size, mode='valid')\n",
    "\n",
    "number_of_episodes = len(attacker_rewards_per_episode) \n",
    "\n",
    "window_size = 3\n",
    "attacker_ma = moving_average(attacker_rewards_per_episode, window_size)\n",
    "defender_ma = moving_average(defender_rewards_per_episode, window_size)\n",
    "\n",
    "# Calculate the mean and 95% CI for both attacker and defender rewards\n",
    "attacker_mean = np.mean(attacker_rewards_per_episode)\n",
    "defender_mean = np.mean(defender_rewards_per_episode)\n",
    "attacker_ci = 1.96 * stats.sem(attacker_rewards_per_episode)\n",
    "defender_ci = 1.96 * stats.sem(defender_rewards_per_episode)\n",
    "\n",
    "plt.figure(figsize=(14, 7))\n",
    "\n",
    "#original rewards\n",
    "plt.plot(attacker_rewards_per_episode, 'o-', color='skyblue', alpha=0.5, label='Attacker Rewards')\n",
    "plt.plot(defender_rewards_per_episode, 'x-', color='lightcoral', alpha=0.5, label='Defender Rewards')\n",
    "\n",
    "#moving averages\n",
    "plt.plot(range(window_size-1, len(attacker_ma)+window_size-1), attacker_ma, color='navy', label='Attacker MA')\n",
    "plt.plot(range(window_size-1, len(defender_ma)+window_size-1), defender_ma, color='darkred', label='Defender MA')\n",
    "\n",
    "#mean lines\n",
    "plt.axhline(attacker_mean, color='blue', linestyle='--', label=f'Attacker Mean: {attacker_mean:.2f}')\n",
    "plt.axhline(defender_mean, color='red', linestyle='--', label=f'Defender Mean: {defender_mean:.2f}')\n",
    "\n",
    "#Confidence interval bands around the means\n",
    "#plt.fill_betweenx([0, max(max(attacker_rewards_per_episode), max(defender_rewards_per_episode))], attacker_mean - attacker_ci, attacker_mean + attacker_ci, color='blue', alpha=0.1)\n",
    "#plt.fill_betweenx([0, max(max(attacker_rewards_per_episode), max(defender_rewards_per_episode))], defender_mean - defender_ci, defender_mean + defender_ci, color='red', alpha=0.1)\n",
    "\n",
    "plt.title('Attacker and Defender Rewards per Episode', fontsize=16)\n",
    "plt.xlabel('Episode', fontsize=14)\n",
    "plt.ylabel('Rewards', fontsize=14)\n",
    "plt.legend(loc='upper right', fontsize=12)\n",
    "plt.grid(True, linestyle='--', linewidth=0.5, alpha=0.7)\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f60cf042-e94d-4001-9485-7e83b5772053",
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculate means\n",
    "attacker_mean = np.mean(attacker_rewards_per_episode)\n",
    "defender_mean = np.mean(defender_rewards_per_episode)\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(attacker_rewards_per_episode, label='Attacker Rewards', color='blue', marker='o')\n",
    "plt.plot(defender_rewards_per_episode, label='Defender Rewards', color='red', marker='x')\n",
    "\n",
    "plt.axhline(y=attacker_mean, color='skyblue', linestyle='--', label=f'Attacker Mean: {attacker_mean:.2f}')\n",
    "plt.axhline(y=defender_mean, color='darkred', linestyle='--', label=f'Defender Mean: {defender_mean:.2f}')\n",
    "\n",
    "plt.title('Rewards Per Episode for Attacker and Defender')\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Total Reward')\n",
    "plt.legend()\n",
    "plt.grid(alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d09deadc-cde5-4ae7-ba2b-50d177abe5c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "win_rates['attacker'] = np.cumsum(np.array(attacker_rewards_per_episode) > np.array(defender_rewards_per_episode)) / np.arange(1, number_of_episodes + 1)\n",
    "win_rates['defender'] = 1 - win_rates['attacker']\n",
    "\n",
    "attacker_win_rate=win_rates['attacker'] \n",
    "defender_win_rate=win_rates['defender']\n",
    "\n",
    "#calculate means\n",
    "attacker_win_rate_mean = np.mean(attacker_win_rate)\n",
    "defender_win_rate_mean = np.mean(defender_win_rate)\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(attacker_win_rate, label='Attacker Win Rate', color='blue')\n",
    "plt.plot(defender_win_rate, label='Defender Win Rate', color='red')\n",
    "\n",
    "plt.axhline(y=attacker_win_rate_mean, color='blue', linestyle='--', label=f'Attacker Mean: {attacker_win_rate_mean:.2f}')\n",
    "plt.axhline(y=defender_win_rate_mean, color='red', linestyle='--', label=f'Defender Mean: {defender_win_rate_mean:.2f}')\n",
    "\n",
    "plt.title('Win Rates Over Episodes')\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Win Rate')\n",
    "plt.legend()\n",
    "plt.grid(alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(win_rates['attacker'], label='Attacker Win Rate', color='blue')\n",
    "plt.plot(win_rates['defender'], label='Defender Win Rate', color='red')\n",
    "plt.title('Win Rates Over Episodes')\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Win Rate')\n",
    "plt.legend()\n",
    "plt.grid(alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3070810c-26b5-4f91-8238-34aeed280f41",
   "metadata": {},
   "outputs": [],
   "source": [
    "defender_win_rate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acd2c64f-59d4-4db3-9858-536cdf161c74",
   "metadata": {},
   "source": [
    "#Episode Lengths Over Time:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b35c3d8a-1b79-47ae-b0be-907540ad0f9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(14, 7))\n",
    "plt.plot(episode_lengths, label='Episode Lengths')\n",
    "plt.title('Episode Lengths Over Time')\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Length (Number of Steps)')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a84582f-3181-4ed2-abad-fbec27858a1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import scipy.stats as stats\n",
    "\n",
    "def moving_average(data, window_size):\n",
    "    \"\"\"Calculate the moving average using a sliding window approach.\"\"\"\n",
    "    return np.convolve(data, np.ones(window_size)/window_size, mode='valid')\n",
    "\n",
    "def calculate_confidence_interval(data, confidence=0.95):\n",
    "    \"\"\"Calculate the confidence interval for the given data.\"\"\"\n",
    "    n = len(data)\n",
    "    mean = np.mean(data)\n",
    "    sem = stats.sem(data)  #standard error of the mean\n",
    "    h = sem * stats.t.ppf((1 + confidence) / 2., n-1)  #margin of error\n",
    "    return mean - h, mean + h\n",
    "\n",
    "window_size = 5  \n",
    "ma_successful = moving_average(defender_successful_actions, window_size)\n",
    "ma_failed = moving_average(defender_failed_actions, window_size)\n",
    "\n",
    "ci_lower_successful, ci_upper_successful = calculate_confidence_interval(ma_successful)\n",
    "ci_lower_failed, ci_upper_failed = calculate_confidence_interval(ma_failed)\n",
    "\n",
    "plt.figure(figsize=(14, 7))\n",
    "plt.style.use('seaborn-darkgrid')\n",
    "\n",
    "plt.fill_between(range(window_size-1, len(ma_successful)+window_size-1), ci_lower_successful, ci_upper_successful, color='blue', alpha=0.1, label='Confidence Interval (Successful)')\n",
    "plt.fill_between(range(window_size-1, len(ma_failed)+window_size-1), ci_lower_failed, ci_upper_failed, color='red', alpha=0.1, label='Confidence Interval (Failed)')\n",
    "plt.plot(range(window_size-1, len(ma_successful)+window_size-1), ma_successful, label='Moving Average (Successful)', color='blue', linewidth=2)\n",
    "plt.plot(range(window_size-1, len(ma_failed)+window_size-1), ma_failed, label='Moving Average (Failed)', color='red', linewidth=2)\n",
    "\n",
    "mean_successful = np.mean(defender_successful_actions)\n",
    "mean_failed = np.mean(defender_failed_actions)\n",
    "plt.axhline(y=mean_successful, color='blue', linestyle='--', linewidth=2, label=f'Mean Successful Actions: {mean_successful:.2f}')\n",
    "plt.axhline(y=mean_failed, color='red', linestyle='--', linewidth=2, label=f'Mean Failed Actions: {mean_failed:.2f}')\n",
    "\n",
    "plt.title('Defender Action Success vs. Failure Rates with Moving Averages', fontsize=16)\n",
    "plt.xlabel('Episode', fontsize=14)\n",
    "plt.ylabel('Number of Actions', fontsize=14)\n",
    "plt.legend(loc='upper right', fontsize=12, bbox_to_anchor=(1.25, 1))\n",
    "plt.xticks(fontsize=12)\n",
    "plt.yticks(fontsize=12)\n",
    "plt.grid(True, which='major', linestyle='--', linewidth=0.5, alpha=0.7)\n",
    "plt.minorticks_on()\n",
    "plt.grid(True, which='minor', linestyle=':', linewidth=0.5, alpha=0.5)\n",
    "\n",
    "plt.tight_layout(rect=[0, 0, 0.75, 1]) \n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9d63cd9-fbe7-4a83-8e6c-4e34363e5ddc",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(defender_successful_actions)\n",
    "print(defender_actions_taken)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc0d459c-e907-4593-a0a8-9e538a2781b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "defender_win_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3cee68e-18cc-481c-9f57-dbcfccc84edc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#comparing defender win rates\n",
    "baseline_reward_structure=[1.        , 0.5       , 0.33333333, 0.25      , 0.4       ,\n",
    "       0.5       , 0.57142857, 0.5       , 0.55555556, 0.6       ,\n",
    "       0.63636364, 0.66666667, 0.69230769, 0.64285714, 0.66666667,\n",
    "       0.6875    , 0.64705882, 0.61111111, 0.63157895, 0.6       ,\n",
    "       0.57142857, 0.54545455, 0.52173913, 0.5       , 0.52      ,\n",
    "       0.53846154, 0.55555556, 0.57142857, 0.55172414, 0.53333333,\n",
    "       0.51612903, 0.53125   , 0.54545455, 0.55882353, 0.54285714,\n",
    "       0.52777778, 0.51351351, 0.5       , 0.48717949, 0.5       ,\n",
    "       0.51219512, 0.5       , 0.48837209, 0.47727273, 0.48888889,\n",
    "       0.47826087, 0.4893617 , 0.5       , 0.51020408, 0.52      ]\n",
    "\n",
    "modified_reward_structure=[1.        , 1.        , 0.66666667, 0.5       , 0.6       ,\n",
    "       0.66666667, 0.71428571, 0.625     , 0.66666667, 0.7       ,\n",
    "       0.72727273, 0.75      , 0.76923077, 0.71428571, 0.73333333,\n",
    "       0.75      , 0.70588235, 0.66666667, 0.68421053, 0.65      ,\n",
    "       0.61904762, 0.63636364, 0.60869565, 0.58333333, 0.6       ,\n",
    "       0.61538462, 0.62962963, 0.64285714, 0.62068966, 0.6       ,\n",
    "       0.58064516, 0.59375   , 0.60606061, 0.61764706, 0.6       ,\n",
    "       0.58333333, 0.56756757, 0.55263158, 0.53846154, 0.55      ,\n",
    "       0.56097561, 0.54761905, 0.53488372, 0.52272727, 0.53333333,\n",
    "       0.52173913, 0.53191489, 0.54166667, 0.55102041, 0.56      ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3763f1d2-33c7-4406-9184-0e2d7392b205",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "\n",
    "defender_win_rate_new = np.array([1., 1., 0.66666667, 0.5, 0.6, 0.66666667, 0.71428571, 0.625, 0.66666667, 0.7, 0.72727273, 0.75, 0.76923077, 0.71428571, 0.73333333, 0.75, 0.70588235, 0.66666667, 0.68421053, 0.65, 0.61904762, 0.63636364, 0.60869565, 0.58333333, 0.6, 0.61538462, 0.62962963, 0.64285714, 0.62068966, 0.6, 0.58064516, 0.59375, 0.60606061, 0.61764706, 0.6, 0.58333333, 0.56756757, 0.55263158, 0.53846154, 0.55, 0.56097561, 0.54761905, 0.53488372, 0.52272727, 0.53333333, 0.52173913, 0.53191489, 0.54166667, 0.55102041, 0.56])\n",
    "defender_win_rate_old = np.array([1., 0.5, 0.33333333, 0.25, 0.4, 0.5, 0.57142857, 0.5, 0.55555556, 0.6, 0.63636364, 0.66666667, 0.69230769, 0.64285714, 0.66666667, 0.6875, 0.64705882, 0.61111111, 0.63157895, 0.6, 0.57142857, 0.54545455, 0.52173913, 0.5, 0.52, 0.53846154, 0.55555556, 0.57142857, 0.55172414, 0.53333333, 0.51612903, 0.53125, 0.54545455, 0.55882353, 0.54285714, 0.52777778, 0.51351351, 0.5, 0.48717949, 0.5, 0.51219512, 0.5, 0.48837209, 0.47727273, 0.48888889, 0.47826087, 0.4893617, 0.5, 0.51020408, 0.52])\n",
    "\n",
    "episodes = np.arange(1, 51)\n",
    "\n",
    "#calculate statistical significance\n",
    "t_stat, p_value = stats.ttest_ind(defender_win_rate_new, defender_win_rate_old)\n",
    "\n",
    "plt.figure(figsize=(12, 7))\n",
    "plt.plot(episodes, defender_win_rate_new, label='New Reward Structure', color='green', marker='o')\n",
    "plt.plot(episodes, defender_win_rate_old, label='Old Reward Structure', color='red', marker='x')\n",
    "plt.title('Defender Win Rate: New vs. Old Reward Structure', fontsize=16)\n",
    "plt.xlabel('Episode', fontsize=14)\n",
    "plt.ylabel('Win Rate', fontsize=14)\n",
    "plt.legend(loc='best', fontsize=12)\n",
    "plt.grid(True, which='both', linestyle='--', alpha=0.7)\n",
    "\n",
    "#highlighting statistical significance\n",
    "plt.figtext(0.5, 0.01, f'p-value = {p_value:.2e}', ha='center', fontsize=12, bbox={\"facecolor\":\"orange\", \"alpha\":0.5, \"pad\":5})\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f869001-af3c-4709-8d08-bafd851217de",
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculate the means\n",
    "mean_new = np.mean(defender_win_rate_new)\n",
    "mean_old = np.mean(defender_win_rate_old)\n",
    "\n",
    "#calculate statistical significance\n",
    "t_stat, p_value = stats.ttest_ind(defender_win_rate_new, defender_win_rate_old)\n",
    "\n",
    "plt.figure(figsize=(12, 7))\n",
    "plt.plot(episodes, defender_win_rate_new, label='New Reward Structure', color='green', marker='o')\n",
    "plt.plot(episodes, defender_win_rate_old, label='Old Reward Structure', color='red', marker='x')\n",
    "\n",
    "plt.axhline(y=mean_new, color='darkgreen', linestyle='--', linewidth=1.5, label=f'Mean New (μ={mean_new:.2f})')\n",
    "plt.axhline(y=mean_old, color='darkred', linestyle='--', linewidth=1.5, label=f'Mean Old (μ={mean_old:.2f})')\n",
    "\n",
    "plt.title('Defender Win Rate: New vs. Old Reward Structure', fontsize=16)\n",
    "plt.xlabel('Episode', fontsize=14)\n",
    "plt.ylabel('Win Rate', fontsize=14)\n",
    "plt.legend(loc='best', fontsize=12)\n",
    "plt.grid(True, which='both', linestyle='--', alpha=0.7)\n",
    "\n",
    "plt.text(x=0.5, y=0.01, s=f'p-value: {p_value:.2e}', horizontalalignment='center', fontsize=12)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bff8305-5524-4084-8e33-4e21d60727d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "\n",
    "defender_win_rate_new = np.array([1., 1., 0.66666667, 0.5, 0.6, 0.66666667, 0.71428571, 0.625, 0.66666667, 0.7, 0.72727273, 0.75, 0.76923077, 0.71428571, 0.73333333, 0.75, 0.70588235, 0.66666667, 0.68421053, 0.65, 0.61904762, 0.63636364, 0.60869565, 0.58333333, 0.6, 0.61538462, 0.62962963, 0.64285714, 0.62068966, 0.6, 0.58064516, 0.59375, 0.60606061, 0.61764706, 0.6, 0.58333333, 0.56756757, 0.55263158, 0.53846154, 0.55, 0.56097561, 0.54761905, 0.53488372, 0.52272727, 0.53333333, 0.52173913, 0.53191489, 0.54166667, 0.55102041, 0.56])\n",
    "defender_win_rate_old = np.array([1., 0.5, 0.33333333, 0.25, 0.4, 0.5, 0.57142857, 0.5, 0.55555556, 0.6, 0.63636364, 0.66666667, 0.69230769, 0.64285714, 0.66666667, 0.6875, 0.64705882, 0.61111111, 0.63157895, 0.6, 0.57142857, 0.54545455, 0.52173913, 0.5, 0.52, 0.53846154, 0.55555556, 0.57142857, 0.55172414, 0.53333333, 0.51612903, 0.53125, 0.54545455, 0.55882353, 0.54285714, 0.52777778, 0.51351351, 0.5, 0.48717949, 0.5, 0.51219512, 0.5, 0.48837209, 0.47727273, 0.48888889, 0.47826087, 0.4893617, 0.5, 0.51020408, 0.52])\n",
    "\n",
    "episodes = np.arange(1, 51)\n",
    "\n",
    "mean_new = np.mean(defender_win_rate_new)\n",
    "mean_old = np.mean(defender_win_rate_old)\n",
    "\n",
    "#calculate statistical significance\n",
    "t_stat, p_value = stats.ttest_ind(defender_win_rate_new, defender_win_rate_old)\n",
    "\n",
    "plt.figure(figsize=(12, 7))\n",
    "plt.plot(episodes, defender_win_rate_new, label='New Reward Structure', color='green', marker='o')\n",
    "plt.plot(episodes, defender_win_rate_old, label='Old Reward Structure', color='red', marker='x')\n",
    "\n",
    "#mean lines\n",
    "plt.axhline(y=mean_new, color='darkgreen', linestyle='--', linewidth=1.5, label=f'New Mean: {mean_new:.2f}')\n",
    "plt.axhline(y=mean_old, color='darkred', linestyle='--', linewidth=1.5, label=f'Old Mean: {mean_old:.2f}')\n",
    "\n",
    "plt.title('Defender Win Rate: New vs. Old Reward Structure', fontsize=16)\n",
    "plt.xlabel('Episode', fontsize=14)\n",
    "plt.ylabel('Win Rate', fontsize=14)\n",
    "plt.legend(loc='best', fontsize=12)\n",
    "plt.grid(True, which='both', linestyle='--', alpha=0.7)\n",
    "\n",
    "plt.figtext(0.5, 0.01, f'p-value = {p_value:.2e}', ha='center', fontsize=12, bbox={\"facecolor\":\"orange\", \"alpha\":0.5, \"pad\":5})\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a23e72d1-83e8-4405-9240-f9ce1df0b261",
   "metadata": {},
   "source": [
    "This visualization compares the defender's win rate over 50 episodes between a new reward structure and an old one. The green line represents the win rate under the new reward structure, while the red line shows the win rate with the old structure. Each point on the graph corresponds to a specific episode, showing how the win rate evolves over time for both structures.\n",
    "\n",
    "The statistical significance of the difference between these two win rates is indicated by the p-value, which is displayed at the bottom of the plot. A p-value measures the probability of observing the data—or something more extreme—if the null hypothesis is true. In this context, the null hypothesis states that there's no difference in the win rates provided by the new and old reward structures.\n",
    "\n",
    "A small p-value (typically ≤ 0.05) would suggest that there's a statistically significant difference between the win rates under the two reward structures, implying that the changes in the reward structure have a meaningful impact on the defender's performance. Conversely, a large p-value suggests that any difference in win rates could be due to random chance, and the new reward structure might not significantly improve the defender's performance.\n",
    "\n",
    "Given that this visualization includes a p-value, the p-value is small, it indicates that the new reward structure significantly impacts the defender's win rate compared to the old structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c9f42cd-ce10-493d-9816-5b0e9a7f6517",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Provided win rate arrays\n",
    "defender_win_rate_new = np.array([1., 1., 0.66666667, 0.5, 0.6, 0.66666667, 0.71428571, 0.625, 0.66666667, 0.7, 0.72727273, 0.75, 0.76923077, 0.71428571, 0.73333333, 0.75, 0.70588235, 0.66666667, 0.68421053, 0.65, 0.61904762, 0.63636364, 0.60869565, 0.58333333, 0.6, 0.61538462, 0.62962963, 0.64285714, 0.62068966, 0.6, 0.58064516, 0.59375, 0.60606061, 0.61764706, 0.6, 0.58333333, 0.56756757, 0.55263158, 0.53846154, 0.55, 0.56097561, 0.54761905, 0.53488372, 0.52272727, 0.53333333, 0.52173913, 0.53191489, 0.54166667, 0.55102041, 0.56])\n",
    "defender_win_rate_old = np.array([1., 0.5, 0.33333333, 0.25, 0.4, 0.5, 0.57142857, 0.5, 0.55555556, 0.6, 0.63636364, 0.66666667, 0.69230769, 0.64285714, 0.66666667, 0.6875, 0.64705882, 0.61111111, 0.63157895, 0.6, 0.57142857, 0.54545455, 0.52173913, 0.5, 0.52, 0.53846154, 0.55555556, 0.57142857, 0.55172414, 0.53333333, 0.51612903, 0.53125, 0.54545455, 0.55882353, 0.54285714, 0.52777778, 0.51351351, 0.5, 0.48717949, 0.5, 0.51219512, 0.5, 0.48837209, 0.47727273, 0.48888889, 0.47826087, 0.4893617, 0.5, 0.51020408, 0.52])\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(14, 7))\n",
    "episodes = np.arange(1, 51)  \n",
    "plt.plot(episodes, defender_win_rate_new, label='New Reward Structure', color='green', marker='o')\n",
    "plt.plot(episodes, defender_win_rate_old, label='Old Reward Structure', color='red', marker='x')\n",
    "\n",
    "# Mean lines\n",
    "mean_new = np.mean(defender_win_rate_new)\n",
    "mean_old = np.mean(defender_win_rate_old)\n",
    "plt.axhline(y=mean_new, color='green', linestyle='--', linewidth=2, label=f'Mean New: {mean_new:.2f}')\n",
    "plt.axhline(y=mean_old, color='red', linestyle='--', linewidth=2, label=f'Mean Old: {mean_old:.2f}')\n",
    "\n",
    "# Annotations for mean values\n",
    "plt.annotate(f'Mean: {mean_new:.2f}', xy=(50, mean_new), xytext=(51, mean_new),\n",
    "             arrowprops=dict(facecolor='green', shrink=0.05), color='green')\n",
    "#plt.annotate(f'Mean: {mean_old:.2f}', xy)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:cybersim] *",
   "language": "python",
   "name": "conda-env-cybersim-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
